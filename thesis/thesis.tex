\documentclass{uvamscse}
\usepackage{color}

\input{program-listings}
\newcommand{\cmd}[1]{\texttt{$\backslash$#1}}

\title{Geographically-aware scaling for real-time persistent websocket applications.}
% \coverpic[100pt]{figures/terminal.png}
\subtitle{Master's Project in Software Engineering}
\date{Summer 2015}

\author{Lukasz Harezlak}
\authemail{lukasz.harezlak@gmail.com}
\host{Instamrkt, \url{https://instamrkt.com}}

\abstract{
  This section summarises the content of the thesis for potential readers who do not have time to read it whole,
  or for those undecided whether to read it at all. Sum up the following aspects:

  \begin{itemize}
    \item relevance and motivation for the research
    \item research question(s) and a brief description of the research method
    \item results, contributions and conclusions
  \end{itemize}

Kent Beck~\cite{JohnsonBBCGW93} proposes to have four sentences in a good abstract:

  \begin{enumerate}
    \item The first states the problem.
    \item The second states why the problem is a problem.
    \item The third is the startling sentence.
    \item The fourth states the implication of the startling sentence.
  \end{enumerate}
}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Let the juicy stuff start.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
Software scalability.
Cloud scalability.
Why is it important
Importance of measuring in the clouds and testing optimal architectures.

\section{Initial Study}
What we found out researching.

\section{Problem Statement}
what's the best architecture that answers that best:
how to deliver data to geographically distributed users with minimal, consistent and manageable latency
how to react
scales up and down in response to demand changes (WEBSOCKETS - DISCONNECT SESSIONS? WHEN?  This provides some additional challenges when it comes to websockets, since in order to stop an instance one needs to make sure it does not serve any sessions. [MMPROJ9])
according to the models based on the set of metrics described in detail later \ref{Selected metrics}
Also, there is an inherent limitation for scaling a websocket application - a number of TCP ports (and file descriptors available to a ws/wss connections) on a server instance. Hardware limitations are different in an http-based application.
\subsection{Research Questions}
\begin{enumerate}
  \item Does architecture with geographically aware scaling case can produce better results than the baseline architecture?
  \item What is the preferred architecture decomposition for a system with given characteristics - stateless, deployed in the cloud, dynamically scaled up and down, with persistent connections, clients distributed globally and uncacheable data generated in real time?
\end{enumerate}
\subsection{Solution Outline}
How our solution works in short.
\subsection{Research Method}
Software engineering is a relatively difficult field to investigate in terms of lack of clarity on how to do it. Most of the problems are of design, rather than pure scientific, nature. West Churchman coined a specific term for these kind of modern problems - wicked (since they are resistant to resolutions) [MMPROJ1]. Eastbrook et al claim it is often difficult to identify the true underlying nature of the research problem and thus the best method to research it [MMPROJ2]. In their work, they name and compare five most classes of research methods to select from: controlled experiments, case studies, survey research, ethnographies, action research. They help to select the method by first establishing the type of research question being asked - existence question (“Does X exist”?), description and classification question (“What is X like?”), and descriptive-comparative questions(“How does X offer from Y?”). The questions of this research are of the last type. The authors suggest pinpointing up-front what will be accepted as a valid answer to the research question [MMPROJ2]. The detailed description of each of the methods helps me to settle for the controlled experiment. It’s well-suited for testing a hypothesis where manipulating independent variables has an effect of dependent ones, which is exactly the case of me research. The manipulated variable is architecture decomposition, and the measured ones are determined by scalability measurement models described below.
\subsubsection{Research Difficulty}
The experiments will be performed in a shared cloud environment, which is inherently unpredictable and changing. Designing and executing a test yielding statistically relevant results where all the necessary variables are controlled (within reasonable boundaries) will be a challenge.
The focus of the project is on scaling a websocket application. This is a relatively new technology, thus finding proper scientific coverage is not trivial.
Some of the necessary development and data collection might be in relatively low-level technology.
The geo-location with reasonable accuracy of the clients might prove difficult too.
Overall, it’s a complex project requiring knowledge of both hardware (protocol), software (architecture) and consisting of multiple disciplines.
even linking order in the compiler can change the performance!!! [one of CRIMES]
\subsection{Hypothesis}
The correct geographical decomposition of application stack can lead to vastly improved performance in comparison with baseline architecure(TODO:S????).

\section{Contributions}
Case study on AWS.
Architecture decomposition analysis.
Deliverable piece of code - metrics framework and auto-scaling scripts.

\section{Related Work}
The big paper on measuring scalability in the clouds.
custom routing between regions. A lot on page 14/21 MMPROJ.
The big paper on deploying different scalability structures [MMPROJ9].
Software engineers need to design for the cloud, not only deploy in the cloud. This is even more important when using multiple data centers situated in different legislative domains; constructed with different hardware, network, and software components; and prone to different environmental risks.
Both Route 53 and ELB do not consider applications’ regulatory requirements when selecting a data center site. Moreover, they do not consider the cost and degree of utilization of the employed resources within a data center.
because they want custom metrics routing they
Selecting the cloud:
As a first step, the user authenticates to one of the entry points. At this point, the entry point has the user’s identity and geographical location (extracted from the IP address). As a second step, the entry point broadcasts the user’s identifier to the admission controllers of all data centers. We call this step matchmaking broadcast.
As a first step, the user authenticates to one of the entry points. At this point, the entry point has the user’s identity and geographical location (extracted from the IP address). As a second step, the entry point broadcasts the user’s identifier to the admission controllers of all data centers. We call this step matchmaking broadcast.
The admission controllers respond to the entry point whether the user’s data are present and if they are allowed (in terms of legislation and regulations) to serve the user. In the response, they also include information about costs within the data center.
Based on the admission controllers’ responses, the entry point selects the data center to serve the user and redirects him or her to the load balancer deployed within it. The entry point filters all clouds that have the user’s data and are eligible to serve him or her. If there is more than one such cloud, the entry point selects the most suitable with respect to network latency and pricing. If no cloud meets the data location and legislative requirements, the user is denied service.
After a data center is selected, the user is served by the AS and DB servers within the chosen cloud as prescribed by the standard three-tier architecture.
they do:
sticky load balancing
it is not beneficial to terminate a running VM ahead of its next billing time. It is better to keep it running until its billing time in order to reuse it if resources are needed again.
their objectives - cost minimization and latency minimization
they use front VMs and each cloud runs an ELB an Admission Controller
latency approximation
GeoLite db (ip->geo coordinates)
compute latency using PingER (To approximate the latency between a user and a cloud, we select the three pairs of PingER hosts that are closest to the user and the cloud, respectively, and define the latency as a weighted sum of the three latencies between the hosts in these three pairs)
CloudSim
Small mentions of the others.

\section{Outline}
Here we outline the structure of the thesis. A short paragraph on what happens in each chapter.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{References TO MOVE TO THESIS.BIB}

[Master's Project Plan]

[MMPROJ1] Churchman, C. West, "Wicked Problems", Management Science 14 (4) (December 1967).
[MMPROJ2] S.Easterbrook, J.Singer, M.A. Storey, D. Damian, Selecting Empirical Methods for Software Engineering Research, Java Report 3 (7) (1998) 51–56.
[MMPROJ9] N. Grozev, R. Buyya, Multi-Cloud Provisioning and Load Distribution for Three-Tier Applications, ACM Transactions on Autonomous and Adaptive Systems (TAAS), Vol.9 Iss.3, Article No. 13 (October 2014)
[MMPROJ10] T. Leighton, Improving performance of the internet, Communications of the ACM - Inspiring Women in Computing, Vol. 52, Iss. 2, p. 44-51 (February 2009)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}

\section{Scalability}

Scalability seems to be a notion that everyone intuitively grasps, but has difficulties when it comes to clear explanations. In my literature study I came across a few definitions, which might help with that, of which three can be found below:

\begin{quote}
~\cite{Williams04} Scalability is a measure of an application system’s ability to, without modification, cost-effectively provide increased throughput, reduced response time and/or support more users when hardware resources are added.
\end{quote}

\begin{quote}
\cite{WeinstockOnSystem2006} Scalability is an ability of a system to handle increased workload (without adding resources).
\end{quote}\label{x}
In light of this definition we would talk about a scalability failure if one of the following occured:
\begin{itemize}
  \item Address space was exceeded,
  \item Memory was overloaded,
  \item Available network bandwith was exceeded,
  \item etc.
\end{itemize}

\begin{quote}
\cite{WeinstockOnSystem2006} Scalability is an ability of a system to handle increased workload by repeatedly applying a cost-effective strategy for extending a system’s capacity.
\end{quote}
According to this definition, one could determine system scalability failure if a given resource got overloaded or exhausted an adding capacity to this resource would not result in a proportional ability to handle additional demand, e.g.:
\begin{itemize}
  \item an additional processor will not contribute to meeting the higher demand if handing of that processor entails an overhead).
  \item a newly added server instance might not contribute to handling a higher user demand if slows down the routing process.
\end{itemize}

\subsection{Scalability Cost}
Scalability is generally desired in the software systems, yet, as all architectural software decisions come at a cost \cite{GerHeiBench}, one must be aware of the trade-offs usually associated with it. Weinstock and Goodenough \cite{WeinstockOnSystem2006} point these out:
\begin{itemize}
  \item performance and scalability (non-scalable system will often demonstrate degrading performance with increasing demand, but scalable systems require performance sacrifice on lower usage levels),
  \item cost and scalability (designing a system to be scalable up-front entails additional costs),
  \item operability and scalability (it is difficult for humans to operate large systems),
  \item usability and scalability (it may be possible to increase servable demand with limiting system's service scope - e.g. removing personalization and displaying generic, cacheable data),
  \item data consistency and scalability (higher scalability can be achieved if system allows for data inconsistencies).
\end{itemize}

\subsection{Need for Scalability}

As the globalization and internetization progresses, more and more systems are expected to be capable of serving millions of globally distributed users. A single server instance often cannot live up to that task and thus application needs to be divided and distributed in multiple smaller chunks. This division happens on different application layers, and different parts of the system have to communicate and synchronize with each other.

In case of many software systems (the system under test \ref{The System Under Test} included), user traffic, and with it the need for system services and resources, varies significantly. A need to be able to scale up and down dynamically in response to traffic arises from this.

Huge parts of the internet are shifting towards real-time. This trend is giving rise to new technologies for exchanging messages between clients and servers in the client-server architecture. Traditionally, client would send a request to a server and receive a response. This is hugely inefficient when there is a need for continuous bidirectional exchange of messages. As an improvement, new mechanisms for server-client communication have been introduced recently \ref{Client-Server Communication Improvements} to enhance the process and reduce overhead. One of them - websockets - are a huge next step on this path, but introduce new challenges. One of them is scalability of applications which make use of this technology.

\subsection{Existing Approaches}
There exist multiple scalability vectors for web applications. Different decompositions of application stack can be applied to achieve the goal of scalability. A few traditional approaches of tackling an issue like that exist already \cite{Akamai}:
\begin{description}
  \item[Scaling up.]
  Increasing volume of system allocated resources. As internet is unreliable and so called "middle-mile bottlenecks" exist,
  a web application end-user latency and throughput experience is not fully deterministic. Traffic levels fluctuate tremendously, so the need to provision for peak traffic levels means that expensive infrastructure will sit underutilized most of the time. It is easy to implement, but costly, even extremely when you start pushing at current hardware limits.\cite{Qvef}
  \item[Scaling out.]
  Scaling out horizontally - increasing the number of units of resources comprising the system. Cost of hardware can be reduced dramatically this way. In a web application, one can deploy multiple instances of servers. Different types of balancing can be applied to distribuet traffic among them: application layer balancing, business load balancing, and anticipating load. The overhead of parsing requests in the application layer is high thus limiting scalability compared to load balancing in the transport layer. Client state needs to be stored in a layer shared between the webservers.\cite{Qvef}
  \item[Content Delivery Networks.]
  These only handle static assets. Communication in our system \ref{The System Under Test} happens mainly over websockets, which are not supported by CDNs. CDNs can be divided into Big Data Center CDNs and Highly Distributed CDNs, which put application data within end-user ISPs.\cite{Akamai}
  \item[Peer to peer networks.]
  An architecture different from client - server, where users communicate with each other directly. It handles adding and removing nodes to and from the network dynamically very well.
\end{description}

\subsection{Cloud Scalability}
The two researchers present us with an interesting thought - software engineers need to design for the cloud, not only to deploy in it. To facilitate that process, they propose an adaptive dynamic provisioning and autonomous workload redirection algorithms. [MMPROJ9]

A lot of tools and providers, most popular - google, azure, aws. Described in details in \ref{Cloud architecture setup}.

Moreover, they do not consider the cost and degree of utilization of the employed resources within a data center. [MMPROJ9]

multiple availability zones very important in a shared cloud environment - often because another customer is getting hit by a DDoS.

\subsection{Data-layer Scalability}
Data layer is often a performance bottleneck because of requirements for transactional access and atomicity - it is hard to scale out because of this [MMPROJ9].

The technology stack I am working with in this scope consists of mysql as a persistent storage and redis as a key-value cache. Described in details \ref{Technology Stack}.

Many solutions and strategies for dealing with database scalability have emerged, including nosql and newsql databases, data replication, caching and database sharding [MMPROJ8]. In Amazon’s environment, Grozev and Buyya suggest using Elasticache service [MMPROJ9]. We cannot use this, also described in \ref{Cloud architecture setup}.

Cooper et al touch on that subject in their work [MMPROJ7]. They claim that in scaling out one should aim for elasticity and high availability. These are hard to achieve using traditional database systems. They emphasize that there are protocols that help achieve strong transaction: two-phase commit and paxos.
They also give an overview of different database systems, including PNUTS, BigTable, HBase, Cassandra, Sharded MySQL, Azure, CouchDB, and SimpleDB. Mysql they use does not support elastic growth and dynamic data repartitioning -  Sharded MySQL is inherently inelastic.
In their work [MMPROJ7], we can find enumeration of classic data-related scalability tradeoffs, such as latency and durability. They talk about different replication techniques - synchronous and asynchronous and different data partitioning solutions.

Mysql White paper [MMPROJ19] gives us a good overview of how scalability works in Mysql’s case. Authors suggest identify which characteristic the application falls into - lots of write operations, real-time user experience, 24 x 7 user experience or agility and ease-of-use. My application falls into the second tier.
They claim to support (among others) auto-sharding for write-scalability, active / active geographic replication and online scaling and schema upgrades.  Geographic replications offers distribution of clusters across remote data centers, which they claim helps reduce latency (important in our case). Authors show how mysql is optimized for real-timeness: data structures are optimized for in-memory access, persistence run in background processes, all indexed columns are stored in memory. They claim that mysql clustered is very well-suited for on-line, on-demand scaling, which provides a contrast to what cooper et al have found  (careful here, son) [MMPROJ7].

Liu in his Eventual Consistency [MMPROJ20] writes about BASE property - basically available, soft state, eventually consistent. It’s a complement of ACID.  Author claims we lack precise metrics to measure its aspects and that’s why every implementation implements eventual consistency differently.
He writes about different implementations. Most importantly for my research, he explains mysql cluster implementation - that it performs much like an ACID database but with the performance benefits of a cluster and can be configured at multiple topologies, not only the basic master-slave. Consistency differs per configuration.

Ruflin et al in Social-Data Storage-Systems [21] evaluate different storage system types: rdbms,  key-value, column store, document store and graph databases. Two first are of my interest, especially since they evaluate technologies I use - redis and mysql. The authors mention unconventional usages of Mysql, as a key value store e.g. by Twitter. Reddit also uses it, with a single table [SOURCE]

They show how mysql can be scaled horizontally by both sharding and data replication. They also indicate that the more structured the RDBMS data, the harder it is to scale horizontally. Mysql is optimized for writes, since only one record in one table is touched, whereas reads can prove expensive if they contain joints, especially spreading across multiple cluster nodes. Facebook and Twitter solved it by putting a cache on top of mysql [21].

Redis is said to be of limited scalability, because of the fact that for good performance the whole data set should fit into memory.

Further improvements to mysql: Pakkonen and Pakkala show how MySQL performance can be increased [22], e.g. by disabling automatic committing or binary logging (binlog) output. They used the already mentioned YCSB benchmarking tool [7]. They also present the results of the tests they performed - switching off auto committing changes the latency from (1.4s to 130 ms) on their data sample, which is a huge improvement.

\subsection{Websocket Scalability}

A nice introduction into push-base communication over the internet can be found in Agarwal’s work - Toward a Push-Scalable Global Internet [MMPROJ11]. The key message in this paper is that push message delivery on the World Wide Web is not scalable for servers, intermediate network elements, and battery-operated mobile device clients. And yet, most of modern day websites have highly dynamic content updated multiple times a minute.
Author here proposes a content-based, machine-learning optimized solution for closing and opening connections when needed. The result he achieves is that the number of hours of active always-on connections can be cut by half while still achieving real-time message delivery for up to 90 percent of all messages.

The author performs the following reasoning in his study. Most of internet communications happens over HTTP Running over TCP. Real-time message delivery requires an always-on connection from the server to the client. HTTP proxies have limited memory and tcp ports, are shared among multiple users. Servers need to be provisioned in order to maintain active tcp connections from large populations of user clients. These all provide challenges that need to be dealt with in scalable applications.

my own research has shown - difficult, one needs to tweak kernels, cloud scalability built for http

previous: ajax, long polling, short polling, websocket [MMPROJ11]

M. Franklin and S. Zdonik provide an insight into the history of push-based technologies. Their paper from 1998 [MMPROJ12] classifies communication mechanisms into aperiodic pull, periodic pull, aperiodic push and periodic push.

Cassetti and Luz [MMPROJ15] claim that overhead introduced by the websocket protocol and WebSocket API is rather small as compared to other communication methods. Furthermore, you can achieve superior bandwidth and performance when using websockets.

\subsection{Geographical Distribution}

The topic is nicely introduced by Tom Leighton in his article Improving performance of the internet [10]. He provides a few arguments why it is important to keep data as close to end users as possible. Apart from obvious latency benefits, by doing this one reduces the chances of suffering from a big “middle mile” provider outage.
He introduces a scale to reason about internet locality:

Scale name
Range
Latency range
local
< 100 miles
1ms
regional
500 - 1000 miles
15ms
cross-continent
~ 3000 miles
50ms
multi-continent
~ 6000 miles
100ms

The longer data must travel through the middle mile, the more it is subject to congestion, packet loss, and poor performance. This is why the company he works for, Akamai, locates servers close to smaller network, end users (on a scale that I cannot reproduce in this research - requires control over infrastructure, in this case it's Amazon's). He claims that big websites have at least two geographically dispersed mirror locations to improve performance, reliability and scalability [MMPROJ10].
In his work, Leighton includes a set of guidelines to follow when thinking about geographical scalability:
reduce transport layer overhead (this is achieved partly in my case by using websockets),
prefetch embedded content,
assemble pages at the edge,
offload computations to the edge,
ensure significant redundancy in all systems to facilitate failover,
software logic to provide message reliability,
distributed control or coordination.

Ed Howorka in his interesting paper “Colocation beats the speed of light” [MMPROJ13] focuses on the best placement of servers for traders trading on multiple exchanges. His paper demonstrates that traders gain nothing by positioning their computer at the midpoint between two financial exchanges. He claims that  every algorithm on a central machine talking to surrounding servers (users in my case) can be replaced by colocated servers (located next to, in my case, users). What is more, he implies that server colocation provides a better solution for high-speed applications (as opposed to using a big, centralized server located in the middle). His formal proof in the article (why isn't this a case for us).

\subsection{Measuring Scalability} \label{Measuring Scalability}
Even in 'locked down' cloud computing environments such as EC2 where the developer has little control over network topology or hardware platform, understanding the performance bottlenecks imposed by the offered infrastructure is valuable [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.163.7403 ,http://www.cs.berkeley.edu/~fox/Cloudstone-Sep08-DRAFT.pdf]

What we have from literature study. ONLY THE GENERAL STUFF - WE HAVE A FULL CHAPTER \ref{Scalability Measurements} ON THIS.

\section{Benchmarking}

[MMPROJ7] important to have random distributions(uniform, zipfian, latest, multinomial)

[MMPROJ2] https://homes.cs.washington.edu/~bornholt/post/performance-evaluation.html
computer science experiments tend to be among the most non-deterministic and prone to omitted-variable bias
Computer systems have many moving parts in both hardware and software, researchers are unlikely to have a complete understanding of every part in those systems, and controlling each of those components is often next to impossible.
linking order can significantly bias the results of a benchmark
the size of your UNIX environment variables biases performance - we made sure this are consistent across tests

Most researchers I’ve met tend to rebuild their infrastructure over and over again, writing new scripts to parse, tabulate and plot their results for every experiment. This approach clearly doesn’t scale.
This is where we can come in and contribute to the scientific and open source communities. Plan to release the deployment and whatever is possible.


[MMPROJ5] http://www.sosy-lab.org/~dbeyer/Publications/2015-SPIN.Benchmarking\_and\_Resource\_Measurement.pdf
benchmarking good and well established for very specific tasks ( https://www.spec.org/ 2 http://www.tpc.org/ 3 http://nlrp.ipd.kit.edu/), our case more complex (as describd below)
non-deterministic aspects (networks, multi-threading etc.)
Results are reproducible if it is guaranteed that the same results can be obtained later again (assuming a deterministic tool) by re-running the benchmarks on a machine with the same hardware and the same software versions. Reproducibility of experimental results requires reliable measurement. We call a measurement reliable, if the measurement method ensures accuracy (small systematic and random measurement error, i.e., no bias or “volatile” effects, resp.) and sufficient precision

Benchmarking a distributed tool (3) is much more complex and out of scope. [3rd paper from http://www.sosy-lab.org/~dbeyer/Publications/2015-SPIN.Benchmarking\_and\_Resource\_Measurement.pdf]
extremely difficult to get all variables under control in IT - hardware, all the layers of software, non deterministic stuff like networks, etc. Don't even get me started on clouds where you basically give up control over hardware to the cloud host.

\subsection{Benchmarking Crimes}

\cite{GerHeiBench}
Selective benchmarking
  not covering the full evaluation space
  not evaluating potential performance degradation
    Progressive criterion - performance actually does improve significantly in area of interest
    Conservative criterion - performance does not significantly degrade elsewhere
    both need to be demonstrated - we wrote our testing scenarios to cover the full area of interest
    “Reality is that techniques that improve performance generally require some degree of extra work: extra bookkeeping, caching, etc. These things always have a cost, and it is dishonest to pretend otherwise. This is really at the heart of systems: it's all about picking the right trade-offs. A new technique will therefore almost always introduce some overheads, and you need to demonstrate that they are acceptable.” - how do we fit in here?

benchmark subsetting without strong justification - WARNING SIGNS: “we picked a representative subset”, “typical results are shown” (reads as cherry picked to fit our results), “I want to understand why you picked the particular subset [of benchmarks].” If your selection contains many obscure or outdated devices, or is heavily biased towards X and Y drivers, then I suspect that you have something to hide.
Selective data set hiding deficiencies. Luckily we have statistical tools to detect that (chess data skeptic podcast)

Same dataset for calibration and validation
Calibrate the system first (using calibration workload). Use evaluation workload to show how accurate the model is. Both workloads need to be totally disjoint. - TODO: DOEST THIS APPLY TO OUR CASE?

No indication of significance of data.
Raw averages, without any indication of variance. At least standard deviations must be quoted. If in doubt use student's t-test to check significance.

Benchmarking of simplified simulated system - WE'RE DOING THE REAL DEAL.

Inappropriate and misleading benchmarks, Unfair benchmarking of competitors - We care about our system's best setup so no incentive to lie.

Relative numbers only, Reads as I am covering that the results are really bad or irrelevant. - happy to provide absolutes but sometimes the unit will be made up so numbers only make sense in context

No proper baseline - Often the state-of-the-art solution, we couldn't find data to prove that what we are comparing against is bad. occam's razor, simple solution proves best - only known limitation is the number of sockets.

Arithmetic mean for averaging across benchmark scores
The proper way to average (i.e. arrive at a single figure of merit) is to use the geometric mean of the normalised scores [http://dl.acm.org/citation.cfm?id=5673]

[MMPROJ3] http://dl.acm.org/citation.cfm?id=5673
Using the arithmetic mean to summarize normalized benchmark results leads to mistaken conclusions that can be avoided by using the preferred method: the geometric mean.

\section{Websocket Protocol}

\subsection{Client-Server Communication Improvements}\label{Client-Server Communication Improvements}
Before the rise of the websocket protocol, different techniques were used to improve communication between server to client. Among them, the following can be listed (some of them serving different purposes):
ajax - a request / response model; was an improvement since the page didn’t have to be refreshed anymore to get new data,
short polling - using a timer to regulate sending requests to server, similar to refreshing the page, useful when data doesn’t change too often,
long polling - now considered to be a workaround of preventing creating connections for each request, keeps a connection artificially alive for some time, clients have to reconnect periodically,
webRTC - a peer to peer connection
server-side events - only allow for a server-initiated communication.

websocket draft - RFC6455

[http://stackoverflow.com/questions/4852702/do-html-websockets-maintain-an-open-connection-for-each-client-does-this-scale/25340220\#25340220]
Each TCP connection in itself consumes very little in terms server resources. Often setting up the connection can be expensive but maintaining an idle connection it is almost free. The first limitation that is usually encountered is the maximum number of file descriptors (sockets consume file descriptors) that can be open simultaneously. This often defaults to 1024 but can easily be configured higher.
http connections more expensive:
- Each HTTP connection carries a lot of baggage that isn't used most of the time: cookies, content type, conetent length, user-agent, server id, date, last-modified, etc. Once a WebSockets connection is established, only the data required by the application needs to be sent back and forth.
- TYPICALLY: HTTP servers are configured to log the start and completion of every HTTP request taking up disk and CPU time. It will become standard to log the start and completion of WebSockets data, but while the WebSockets connection doing duplex transfer there won't be any additional logging overhead (except by the application/service if it is designed to do so)
- interactive applications that use AJAX either continuously poll or use some sort of long-poll mechanism. WebSockets is a much cleaner (and lower resource) way of doing a more event'd model where the server and client notify each other when they have something to report over the existing connection.
obviously don't support cdn

As for decreased latency via WS vs. HTTP, it's true since there's no more parsing of HTTP headers beyond the initial WS handshake. Plus, as more and more packets are successfully sent, the TCP congestion window widens, effectively reducing the RTT.

Think of it this way: what is cheaper, keeping an open connection, or opening a new connection for every request (with the negotiation overhead of doing so, remember it's TCP.)

http://serverfault.com/questions/48717/practical-maximum-open-file-descriptors-ulimit-n-for-a-high-volume-system
the number of client connections that a server can support has nothing to do with ports in this scenario, since the server is [typically] only listening for WS/WSS connections on one single port. I think what the other commenters meant to refer to were file descriptors. You can set the maximum number of file descriptors quite high, but then you have to watch out for socket buffer sizes adding up for each open TCP/IP socket.  MAKE SURE THIS IS CONSISTENT THROUGHOUT THE PAPER.
If the file descriptors are tcp sockets, etc, then you risk using up a large amount of memory for the socket buffers and other kernel objects; this memory is not going to be swappable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The System Under Test}\label{The System Under Test}

real time prediction parmet
parimutuel pools
directed contracts
sort of a stock market for real-time prediction on anything, e.g. live sport

\section{General Purpose of The System}
first applicatoin - live sports
low manageable latencies important

\subsection{Sample Use Case}
A good example e.g. can be predicting the outcomes of certain drives in the football match as they happen. A sample case: Manchester United - Liverpool live game, live data coming from UK servers, introduced into the system through a UK node, most users (who also generate data that needs to be distributed) in China (10k), India(10k), Australia(3k). Where should websocket servers which distribute messages spun up for minimal latencies for all clients? Is it faster to spin up db replicas on that nodes too?

\subsection{Users of the System}
in stadium
on couch
dekstop
tablet
mobile
Users of the platform are mostly on mobile networks (that often drop), so reconnecting them quick to the right (providing lowest latencies) instance is important.
lumpy demand - it comes in 2-hour-long spikes and then can go quiet for days

\section{Basic Architecture}

Diagram

\section{Technology Stack} \label{Technology Stack}
Python + redis + mysql + redis (cluster) all versions
cloud stack described here: \ref{Cloud architecture setup}

\subsection{Programming languages}

python

\subsection{Database Technologies}

\subsubsection{Mysql Cluster}
write scalable, important for us, multi master, automatic / manual sharding
distributed, shared-nothing data that provides very high, scalable performance for applications that largely use primary key access and have high concurrency, but it incurs a cost in network access when accessing data between a MySQL Server and tables distributed across Data Nodes
In terms of CAP, MySQL Cluster will sacrifice availability to maintain consistency in the event of an unsurvivable network partition.
at least 3 machines (otherwise split brain problem)
data nodes (ndbd) - need memory, single-threaded
sql nodes (mysqld) - need cpu, multithreaded
Important:
Will MySQL Cluster “out of the box” run an application faster than my existing MySQL database?
The reality is probably not without some optimizations to the application and the database. However, by following the guidelines and best practices in the Guide to Optimizing Performance of the MySQL Cluster Database1 , significant performance gains can be realized, especially as you start to scale write operations while maintaining very high levels of availability and low latency.
[http://openquery.com.au/files/mysql-cluster-intro-use.pdf]


\section{System Scaleup Delay}
Delays: up to 2 minutes to get metrics, up to ????? minutes for instance to be accessible to clients + ???? for DNS changes to propagate (TTL set to 60 seconds but TODO VERITYF)

\section{Unique Aspects of The System}
critical data flows over websockets
Users receiving data shared locally should receive it at the same time as data shared globally (with as low a latency as possible). This creates a need for globally consistent and manageable latency between end user and the system.
Connected users and sources of data are geographically changing.
Users geographical center of mass is changing for each peak of demand.
Extremely lumpy demand (peaks lasting around 2 hours). This creates a need for being able to quickly scale up and scale down.
New data is generated every few seconds by the users so caching the content and distributing geographically is difficult.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiment Outline}

\begin{enumerate}
  \item Baseline architecture on a local network.
  \item Baseline architecture deployed in the cloud.
  \item Improved architecture in the cloud.
\end{enumerate}

All of them measured with the same set of metrics with a prepared framework for gathering them.

Details below.

\section{Goal of the experiment}

The goal of the project is to design a scalability framework for a real-time persistent websocket distributed application (the system). The core researched topic will be whether a systems awareness of clients geographical distribution can improve the system performance according to selected metrics, in comparison with traditional approaches.

The goal of the project is to see if the proposed architecture decomposition can perform better (quantitatively, according to the selected metric model) than the baseline architecture in serving geographically dispersed clients. Approaches used to scale simple http applications cannot always be translated to websocket applications since the communication protocol differs. Websockets put a different kind of strain on the server machines since these need to keep the connection opened on a port for a prolonged period of time rather than simply open, server and close (as is the case with http). Along the way, an answer needs to be found what level of decoupling provides best performance on each layer of a stateless persistent system. One of the properties of a system of that kind is that key value stores come under heavy load since this is where the state resides. A good solution for distributing (sharding / replicating) these also needs to be found. Same goes for persistent storage.
TODO: REFERENCE CURRENT APPROACHES HERE.

\section{Load Testing Framework}\label{Load Testing Framework}
all the analyzed tools, list also in lab notes from may 11 [FOR EACH WHY WASN'T SATISFACTORY]
jmeter, thor (low extensibility which we needed), autobahn (max available connections but no further control),
gatling-websocket, http://www.opensourcetesting.org/performance.php
tsung - looked promising, highly scalable erlang, but hard to understand what was going on, community was not active enough, wsbench

\subsection{Kernel tuning}
all websocket updates
http://serverfault.com/questions/48717/practical-maximum-open-file-descriptors-ulimit-n-for-a-high-volume-system
Also, and very important, you may need to check if your application has a memory/file descriptor leak. Use lsof to see all it has open to see if they are valid or not. Don't try to change your system to work around applications bugs.

look for \_what can limit concurrent sockets?\_ in lab notes

“The number of connections wstest can open on a server is limited by the number of ephemeral ports on the machine on the outgoing interface / IP. Something like 64k at most. If you need to test the server with more connections, currently you will need to run multiple instances of wstest (on different machines).” from: http://autobahn.ws/testsuite/usage.html\#mode-massconnect

Understanding tcp sockets:
sockets are left in TIME\_WAIT on the server when the client suite is killed which makes sense
when client exits gracefully no sockets left in TIME\_WAIT / CLOSE\_WAIT by the server
on\_close() from tornado only kicks in on client close
.onclose in js only kicks in when server initiates through self.close()
when server terminates through self.close() sockets ends in TIME\_WAIT for a minute (also makes sense according to docs)
describe how you can influence that (reusing web sockets + delay time )

\section{Baseline architecture on a local network}
Architecture diagrams.
Technical details - local server capabilities.
Load testing.

\section{Cloud architecture setup} \label{Cloud architecture setup}
\subsection{Route53}
  dns mapped to inscance ips / load balancer dns names
  lbr, geo, weighed rr
  their internal heuristics take network conditions from past weeks [VERIFY], rather amibiguous
  For each end user’s location, Route 53 will return the most specific Geo DNS record that includes that location. In other words, for a given end user’s location, Route 53 will first return a state record; if no state record is found, Route 53 will return a country record; if no country record is found, Route 53 will return a continent record; and finally, if no continent record is found, Route 53 will return the global record.

  health checks - can be used custom-defined, but for us we can use load balancing health checks (will only route if healthy incstances behind it) [http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html]

  http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Tutorials.html
\subsection{Autoscaling}
  instances behind each groups. automatically scalable, connected to external redis and mysql instances.
  only within one region, multiple availability zones
  http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/GettingStartedTutorial.html
  http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/how-as-works.html\#arch-AutoScalingMultiAZ
  Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group. Auto Scaling does this by attempting to launch new instances in the Availability Zone with the fewest instances.
  alarm - object that watches over a single metric (e.g. avg cpu use of ec2 instances in auto scaling group over specified time period) (OK, ALARM, INSSUFICIENT\_DATA), can trigger scaling up / down policy on an autoscaling group
  policy variants (change ExactCapacity, ChangeInCapacity, PercentChangeInCapacity)
  recommendation one policy for scaling out, another policy for scaling in
  http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/as-scale-based-on-demand.html
  http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/AlarmThatSendsEmail.html
  http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/US\_AlarmAtThresholdEC2.html
  \subsubsection{Alarms}
  avg / min / max / sum / sample count (for whole group)
  of cpu / disk io / network io
  is >= / > / < / <= than x %
  for at least X consecutive periods of 5 min / 15 min / 1 hour / 6 hours
  \subsubsection{Scaling Policies}
  execute policy when alarm
  add / remove / set to   X   instances / % of group
  and then wait Y (cooldown period)

  \subsubsection{Launch configurations}

\section{Load balancing}
  elb uses dns name to server a pool of lb instances in the backend (you might exceed one instance connection limit), that's why they are not provided a static ip
  routing: request count-based for http(s), for others (tcp which is a workaround for websockets) they use tcp [http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/how-elb-works.html\#request-routing]
  The client uses DNS round robin to determine which IP address to use to send the request to the load balancer. No control over routing without load balancing groups.
  For example, if you have ten instances in Availability Zone us-west-2a and two instances in us-west-2b, the traffic is equally distributed between the two Availability Zones. As a result, the two instances in us-west-2b serve the same amount of traffic as the ten instances in us-west-2a. Instead, you should distribute your instances so that you have six instances in each Availability Zone.
  To distribute traffic evenly across all back-end instances, regardless of the Availability Zone, enable cross-zone load balancing on your load balancer. However, we still recommend that you maintain approximately equivalent numbers of instances in each Availability Zone for higher fault tolerance.
  [http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/how-elb-works.html\#request-routing]
  only ~64k connections possible per ELB instance TODO: VERIFY THAT
  stickiness only works for HTTP/HTTPS protocols
  Once you have a testing tool in place, you will need to define the growth in the load. We recommend that you increase the load at a rate of no more than 50 percent every five minutes. B.oth step patterns and linear patterns for load generation should work well with Elastic Load Balancing. If you are going to use a random load generator, then it is important that you set the ceiling for the spikes so that they do not go above the load that Elastic Load Balancing will handle until it scales (see Pre-Warming the ELB).
  [https://aws.amazon.com/articles/1636185810492479\#pre-warming]

\section{Data Layer}

Technologies we used described in \ref{Technology Stack}. We used Mysql and redis as a caching solution.

\subsection{Amazon Relational Database Service}
Does not support mysql cluster as of July 2015. Support coming but not released yet.
Offers mysql read replicas. With a single master. That obviously scales only reads
You can create a MySQL Read Replica in a different region than the source DB instance to improve your disaster recovery capabilities, scale read operations into a region closer to end users, or make it easier to migrate from a data center in one region to a data center in another region.

\subsection{Elasticache}
doesn't support crossregion (you cannot connect to an elasticache cluster from outisde of the region)

The above explain why, for a scaled out solution we had to roll with our custom setup. We were suggested using technologies like DynamoDB and Kinesis, but out of scope.

\subsection{Baseline architecture deployed in the cloud}

\subsection{Improved architecture deployed in the cloud}

\section{Experiment deliverables}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Scalability Measurements} \label{Scalability Measurements}
The previous description \ref{Measuring Scalability} concerned general stuff.
Here we describe the metrics we settled for.

Most of what is necessary for the purpose of this research has been covered by Pushkala Pattabiraman et al [MMPROJ3].
They realized that cloud computing and its measurement provides a new set of challenges when it comes to measuring performance testing, as opposed to measuring performance of  traditional software systems. They list some key points for measuring cloud applications, among them we can find: validating and ensuring the elasticity of scalability and evaluating utility service billings and pricing models. The latter is also important in my case since cost is one of the driving factors in assessing the scalability of my system. A question they raise regarding this is: How to use a transparent approach to monitor and evaluate the correctness of the utility bill based on a posted price model during system performance evaluation and scalability measurement?.
The authors divide the performance indicators into three groups:
computing resource (CPU, disk, memory, networks) - they can be helpful in establishing baseline architecture in my case,
workload indicators (connected users, throughput and latency),
performance indicators - processing speed, system reliability and scalability based on the given QoS standards.
For each they propose formal models with pluggable values and graphic representations (BELOW).
On top of that, the research contains a case study performed in the Amazon EC2 environment [MMPROJ3].
Cloud limitations need to be taken into account. One needs to be aware of hidden costs (e.g. autoscaling service is free on EC2, but it requires cloudwatch, which is not). The authors also advise to pay attention to inconsistencies in performance and scalability data [MMPROJ3].

many others:
There is much more work related to the general scalability of distributed systems. Srinivas and Janakiram in their work [MMPROJ5] mention a metric evaluating scalability as a product of throughput and response time (or any value function) divided by the cost factor. They propose another model considering scalability as a function of synchronization, consistency, availability, workload and faultload. It aims on identifying bottlenecks and hence improving the scalability. The authors also emphasize the fact of interconnectedness of synchronization, consistency and availability.
Jogalekar and Woodside [6] propose a strategy-based scalability metric based on cost effectiveness (a function of system's throughput and it’s quality of service). It separates evaluation of throughput or quantity of work from QoS (which, according to the authors, can be any suitable expression).[MMPROJ6]

PASA[MMPROJ17]

\section{Selected metrics}
latency (messaging + handhsaking) (our suite)
sustainable concurrent websocket connectoins (our suite)
infrastructure cost (per unit of time, supporting the same number of users) (manual calculation)
dropped connections (our suite)
cpu usage (cloudwatch)
memory usage (our custom server metrics, pidstat)
network in network out (cloudwatch)
iops reads + writes (bots number and byte values, cloudwatch)

to calculate manually:
percentage of available ports used?
how long to run on a x euros


availability (SLAs???????)
architecture reaction speed to changes in demand (scaling up and scaling down) (????)

CRAM METRICS
TODO: review

\subsection{EC2 Available metrics}
collectible every minute (in detailed mode), by default every 5 minute, paid additionaly - delay up to 2 minutes
the current EC2 cloud technology does not provide any CloudWatch API to monitor the allocation and utilization of memory for EC2 instances (we do it on our own)
limits exist (10 metric, 10 alarms, 1,000,000 million requests, 1000 SNS email notifications per month for free
no limits on custom metrics
up to 5gb of incoming data logs for free
up to 5gb of data archiving for free) but we don't expect to hit them.
small delay (up to 2 minutes, that's what we experienced - delays in autoscaling)

alarms can be configured based on this -  http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch.html
data not aggregated across regions [lab2]

\subsubsection{Instance Specific}
[Lab2 - http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/cloudwatch\_concepts.html]
custom possible
available statistics for each: min, max, sum, avg, sampleCount (count of data points but research that)
cpu utilization
network in / out (in bytes)
disk read / disk write (number of ops + bytes)
status checks

\subsubsection{Cloud (group) Specific}
Collected from Elastic Load Balancer for all instances connected.

\subsubsection{Custom Metric Collection}
pidstat
latencies by our custom client suite
for network we looked ad ntop, nethogs, nload, vnstat, wireshark but amazon provides this data.

\section{Selected model}
[MMPROJ3] With CRUMs, SPMs etc.
four different types of needs:
resource utilization and allocation measurement
performance measurement under allocated computing resources
scalability measurement in different cloud infrastructures
cost driven evaluation based on a pre-defined price model

additional values we want to capture to proposed in the model:
cost
time to scale up / down
What we will use to compare architectures:
SCM (allocated resources) or SEC (used)
(1/cost) as an additional metric to SEC? or amount of s you can run on a 100bucks

Results of all setups get plugged into SCM / SEC and then ESS.

\section{Baseline, Local network}
Lab notes from 11 of May go here:

Basic metrics
\begin{itemize}
  \item cpu, memory, disk usage (pidstat / CloudWatch)
  \item network i/o (wireshark, list others analyzed)
  \item latency, throughput, concurrent connections, messages dropped?
\end{itemize}

\subsection{Load Testing}
http://stackoverflow.com/questions/3871886/ssl-and-load-balancing

elb uses dns name to server a pool of lb instances in the backend (you might exceed one instance connection limit)
in order for the full range of ELB machine IP addresses to be utilized, “make sure each [client] refreshes their DNS resolution results every few minutes.”
One test client equals one public IP address. ELB machines seem to route all traffic from a single IP address to the same back-end instance, so if you run more than one test client process behind a single public IP address, ELB regards these as a single client.
Use 12 test clients for every availability zone you have enabled on the ELB. These test clients do not need to be in different availability zones – they do not even need to be in EC2 (although it is quite attractive to use EC2 instances for test clients). If you have configured your ELB to balance among two availability zones then you should use 24 test clients.
Each test client should gradually ramp up its load over the course of a few hours. Each client can begin at (for example) one connection per second, and increase its rate of connections-per-second every X minutes until the load reaches your desired target after a few hours. - WE CANNOT DO THAT

Tools described here \cite{Load Testing Framework}

\subsubsection{Users Distribution}
The authors suggest choosing randomly when generating load as to which operation to perform, on what data size etc. They suggest using different random distributions: uniform, zipfian, latest, multinomial[MMPROJ7]. Another distribution is suggested by Grozev and Buyya [MMPROJ9] - Poisson distribution with a constant mean.

\section{Baseline, Deployed in the cloud}

\section{Improved, Deployed in the cloud}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiment results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}

\section{Statistical Significance}
http://technology.stitchfix.com/blog/2015/05/26/significant-sample/

\section{Threats to validity}
ec2 whacky, control over hardware released to amazon (can be dedicated machines)
shared environment, someone else might get ddosed or sth - you have no control over that
network is inherently non-deterministic, load tests needed to actually test availability

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Further work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BELOW WE HAVE ORIGINAL TEMPLATE STUFF THAT NEEDS TO BE REMOVED
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Everything below that needs to be removed (except for bib)}
\chapter{Front Matter}

The first thing is to connect the class by saying:

\begin{snippet}
\begin{verbatim}
\documentclass{uvamscse}
\end{verbatim}
\end{snippet}

\section{Title}

Specify the title of the thesis with \cmd{title} and \cmd{subtitle} commands:

\begin{snippet}
\begin{verbatim}
\title{MetaThesis}
\subtitle{A Thesis Template Leading by Example}
\end{verbatim}
\end{snippet}

Any thesis can survive without a \cmd{subtitle}, but the \cmd{title} is mandatory.

\section{Author}

Introduce yourself with \cmd{author} and \cmd{authemail}:

\begin{snippet}
\begin{verbatim}
\author{Vadim Zaytsev}
\authemail{vadim@grammarware.net}
\end{verbatim}
\end{snippet}

Again, \cmd{authemail} is not mandatory. If you need anything fancier, just put it inside \cmd{author}.

\begin{snippet}
\begin{verbatim}
\author{Vadim Zaytsev\footnote{Yes, that one.}}
\end{verbatim}
\end{snippet}

The footnote would be printed on the bottom of the title page, and will be
referred to by a symbol, not by a number as any footnotes within the main
document body.

\section{Date}

By default, the date inserted in your PDF is the day of the build, e.g., ``March 25, 2014''. If you want it to be formatted differently or be more vague or outright fake, use \cmd{date}:

\begin{snippet}
\begin{verbatim}
\date{Spring 2014}
\end{verbatim}
\end{snippet}

The argument is just a string, the format is unrestricted:

\begin{snippet}
\begin{verbatim}
\date{Tomorrow. Honestly.}
\end{verbatim}
\end{snippet}

\section{Host}

If your hosting organisation is not the UvA, specify it with \cmd{host}. The
logo on the bottom of the title page will still be the UvA one, because this
is the organisation guaranteeing your degree.

\begin{snippet}
\begin{verbatim}
\host{Grammarware, Inc., \url{http://grammarware.github.io}}
\end{verbatim}
\end{snippet}

NB: footnotes will not work, unless you know how to \cmd{protect} them.

\section{Cover picture}

If the first page of your thesis looks too blunt, add a picture to it:

\begin{snippet}
\begin{verbatim}
\coverpic{figures/terminal.png}
\end{verbatim}
\end{snippet}

You can even specify the picture's width as an optional argument:

\begin{snippet}
\begin{verbatim}
\coverpic[100pt]{figures/terminal.png}
\end{verbatim}
\end{snippet}

How these three options look, you can see from \autoref{fig:titles}.

\begin{figure}[t]
  \fbox{\includegraphics[width=.25\textwidth]{figures/title1.pdf}}
  \hfill
  \fbox{\includegraphics[width=.25\textwidth]{figures/title2.pdf}}
  \hfill
  \fbox{\includegraphics[width=.25\textwidth]{figures/title3.pdf}}
  \caption{A hypothetical thesis title page without a cover picture (on the left), with an overly large one (in the centre) and with a tiny pic (on the right).}
  \label{fig:titles}
\end{figure}

\section{Abstract}

A thesis is fine without an abstract, if you do not feel like writing it and
your supervisor does not feel like enforcing it. If you do want an abstract,
make it with the \cmd{abstract} command:

\begin{snippet}
\begin{verbatim}
\abstract{This is not a thesis.}
\end{verbatim}
\end{snippet}

The abstract is just like any other section of your thesis, so you can use any
\LaTeX\ tricks there. If you think that the name ``abstract'' is too abstract
for your abstract, you can still use \cmd{abstract} without being too
abstract:

\begin{snippet}
\begin{verbatim}
\abstract[Confession]{I am a cenosillicaphobiac.}
\end{verbatim}
\end{snippet}

Kent Beck~\cite{JohnsonBBCGW93} proposes to have four sentences in a good abstract:

\begin{enumerate}
  \item The first states the problem.
  \item The second states why the problem is a problem.
  \item The third is the startling sentence.
  \item The fourth states the implication of the startling sentence.
\end{enumerate}

In practice, each of these ``sentences'' can be longer than an actual
sentence, but it is in general a good rule of thumb to condense the summary of
your thesis into these four tiny messages. Do not write too much, make it
tweetable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Core Chapters}

The structure of your thesis is up to you and your supervisor. Whatever you
do, do not consider the guidelines below as dogmas.

\section{Classic structure}

\begin{description}
  \item[Problem statement and motivation.]
  You describe in detail what problem the research is addressing, and what is
the motivation to address this problem. There is a concise and objective
statement of the research questions, hypotheses and goals. It is made clear
why these questions and goals are important and relevant to the world outside
the university (assuming it exists). You can already split the main research
question into subquestions in this chapter. This section also describes an
analysis of the problem: where does it occur and how, how often, and what are
the consequences? An important part is also to scope the research: what
aspects are included and what aspects are deliberately left out, and why?
  \item[Research method.]
  Here you describe the methods used to answer the research questions. A good
structure of this section often follows the subquestions by providing a method
for each. The research method needs a thorough motivation grounded in theory
in order to be acceptable. As a part of the method, you can introduce a number
of hypotheses --- these will be tested by the research, using the methods
described here. An important part of this section is validation. How will you
evaluate and validate the outcomes of the research?
  \item[Background and context.]
  This chapter contains all the information needed to put the thesis into
context. It is common to use a revised version of your literature survey for
this purpose. It is important to explicitly refer from your text to sources
you have used, they will be listed in your bibliography. For example, you can
write ``A small number of programming languages account for most language
use~\cite{MeyerovichR2013}'', where the following entry would be included in
your bibliography:
\begin{quote}
\cite{MeyerovichR2013} Leo A. Meyerovich and Ariel S. Rabkin. Empirical Analysis of Programming Language Adoption. In \emph{Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages and Applications}, OOPSLA, pages 1--18. ACM, 2013. \doi{10.1145/2509136.2509515}.
\end{quote}
Have a look at \autoref{sec:biblio} to learn more about citation.
  \item[Research.]
  This chapter reports on the execution of the research method as described in
an earlier chapter. If the research has been divided into phases, they are
introduced, reported on and concluded individually. If needed, this chapter
could be split up to balance out the sizes of all chapters.
  \item[Results.]
  This chapter presents and clarifies the results obtained during the
  research. The focus should be on the factual results, not the interpretation
  or discussion. Tables and graphics should be used to increase the clarity of
  the results where applicable.
  \item[Analysis and conclusions.]
  This chapter contains the analysis and interpretation of the results. The
  research questions are answered as best as possible with the results that
  were obtained. The analysis also discussed parts of the questions that were
  left unanswered.

  An important topic is the validity of the results. What methods of
  validation were used? Could the results be generalised to other cases? What
  threats to validity can be identified? There is room here to discuss the
  results of related scientific literature here as well. How do the results
  obtained here relate to other work, and what consequences are there? Did
  your approach work better or worse? Did you learn anything new compared to
  the already existing body of knowledge? Finally, what could you say in
  hindsight on the research approach by followed? What could have done better?
  What lessons have been learned? What could other researchers use from your
  experience? A separate section should be devoted to ``future work'', i.e.,
  possible extension points of your work that you have identified. Even other
  researchers should be able to use those as a starting point.
\end{description}

\section{Reporting on replications}

Here are the guidelines to report on replicated studies~\cite{Carver10}:

\begin{description}
  \item[Information about the original study]~\\
    \begin{description}
    \item[Research question(s)] that were the basis for the design
    \item[Participants,] their number and any other relevant characteristics
    \item[Design] as a graphical or textual description of the experimental design
    \item[Artefacts,] the description of them and/or links to the artefacts used
    \item[Context variables] as any important details that affected the design of the study or interpretation of the
results
    \item[Summary of the results] in a brief overview of the major findings
    \end{description}
  %
  \item[Information about the replication]~\\
    \begin{description}
    \item[Motivation for conducting the replication] as a
description of why the replication was conducted:
to validate the results, to broaden the results by
changing the participant pool or the artifacts.
    \item[Level of interaction with original experimenters.]
The level of interaction between the original experimenters and the
replicators should be reported. This interaction could range from none (i.e.
simply read the  paper) to them being the same people. There is quite a lot of
discussion of the level of interaction allowed for the replication to be
``successful'', but this level should be reported even without  addressing
the controversy.
    \item[Changes to the original experiment.] Any changes made to the
design, participants, artifacts, procedures, data collected and/or analysis
techniques should be  discussed along with the motivation for the change.
    \end{description}
  \item[Comparison of results to original]~\\
    \begin{description}
    \item[Consistent results,] when replication results supported
results from the original study, and
    \item[Differences in results,] when results from the replication
did not coincide with the results from the original study.
Authors should also discuss how changes made to the
experimental design (see above) may have caused
these differences.
    \end{description}
    \item[Drawing conclusions across studies]
\end{description}

NB: this section contains portions of text repeated directly from Carver~\cite{Carver10} and
only slightly massaged. Do not do this for your thesis, write your own thoughts down.

\section{\LaTeX\ details}

\subsection{Environments}

A \LaTeX\ environment is something with opening and closing tags, which look
like \cmd{begin}\{\texttt{name}\} and \cmd{end}\{\texttt{name}\}. Some useful
environments to know:

\begin{center}
\begin{tabular}{ll}
  \texttt{itemize}      & bullet lists\\
  \texttt{enumerate}    & numbered lists\\
  \texttt{description}  & definition lists\\
  \hline
  \texttt{center}       & centered line elements\\
  \texttt{flushright}   & right aligned lines\\
  \texttt{flushleft}    & left aligned lines\\
  \hline
  \texttt{tabular}      & table\\
  \texttt{longtable}    & multi-page table (needs the \texttt{longtable} package)\\
  \texttt{sideways}     & rotates some text\\
  \texttt{quote}        & block quote\\
  \texttt{verbatim}     & unformatted text\\
  \texttt{minipage}     & compound box with elements inside\\
  \texttt{boxedminipage}& compound box with elements inside and a border around it\\
  \hline
  \texttt{table}        & floating table (needs to have \texttt{tabular} nested inside)\\
  \texttt{figure}       & floating figure\\
  \texttt{sourcecode}   & floating listing\\
  \hline
  \texttt{equation}     & mathematical equation\\
  \texttt{lstlisting}   & pretty-printed syntax highligted listing\\
  \texttt{multline}     & mathematical equation spanning over multiple lines\\
  \texttt{eqnarray}     & system of mathematical equations\\
  \texttt{gather}       & bundled mathematical equations\\
  \texttt{align}        & bundled and aligned mathematical equations\\
  \texttt{array}        & matrix\\
  \texttt{CD}           & commutative diagrams\\
\end{tabular}
\end{center}

\section{Listings}

\begin{sourcecode}
\begin{lstlisting}[language=prolog]
define(Ps1,G1,G2)
 :-
    usedNs(G1,Uses),
    ps2n(Ps1,N),
    require(
      member(N,Uses),
      'Nonterminal ~q must not be fresh.',
      [N]),
    new(Ps1,N,G1,G2),
    !.
\end{lstlisting}
\caption{Code in Prolog}
\end{sourcecode}

\begin{sourcecode}
\begin{lstlisting}[language=sdf]
module Syntax

imports Numbers
imports basic/Whitespace

exports
  sorts
    Program Function Expr Ops Name Newline

  context-free syntax
    Function+                          -> Program
    Name Name+ "=" Expr Newline+       -> Function
    Expr Ops Expr                      -> Expr      {left,prefer,cons(binary)}
    Name Expr+                         -> Expr      {avoid,cons(apply)}
    "if" Expr "then" Expr "else" Expr  -> Expr      {cons(ifThenElse)}
    "(" Expr ")"                       -> Expr      {bracket}
    Name                               -> Expr      {cons(argument)}
    Int                                -> Expr      {cons(literal)}
    "-"                                -> Ops       {cons(minus)}
    "+"                                -> Ops       {cons(plus)}
    "=="                               -> Ops       {cons(equal)}
\end{lstlisting}
\caption{Code in SDF}
\end{sourcecode}

\begin{sourcecode}
\begin{lstlisting}[language=Java]
import types.*;
import org.antlr.runtime.*;

public class TestEvaluator
    public static void main(String[] args) throws Exception {

        // Parse file to program
        ANTLRFileStream input = new ANTLRFileStream(args[0]);
        FLLexer lexer = new FLLexer(input);
        CommonTokenStream tokens = new CommonTokenStream(lexer);
        FLParser parser = new FLParser(tokens);
        Program program = parser.program();

        // Parse sample expression
        input = new ANTLRFileStream(args[1]);
        lexer = new FLLexer(input);
        tokens = new CommonTokenStream(lexer);
        parser = new FLParser(tokens);
        Expr expr = parser.expr();

        // Evaluate program
        Evaluator eval = new Evaluator(program);
        int expected = Integer.parseInt(args[2]);
\end{lstlisting}
\caption{Code in Java}
\end{sourcecode}

\begin{sourcecode}
\begin{lstlisting}[style=mono,language=Python]
#!/usr/local/bin/python
# wiki: BGF
import os
import sys
import slpsns
import elementtree.ElementTree as ET

# root::nonterminal* production*
class Grammar:
  def __init__(self):
    self.roots = []
    self.prods = []
  def parse(self,fname):
    self.roots = []
    self.prods = []
    self.xml = ET.parse(fname)
    for e in self.xml.findall('root'):
      self.roots.append(e.text)
    for e in self.xml.findall(slpsns.bgf_('production')):
      prod = Production()
      prod.parse(e)
      self.prods.append(prod)
\end{lstlisting}
\caption{Code in Python}
\end{sourcecode}

\chapter{Literature}\label{sec:biblio}

\textsc{Bib}TeX\ is a JSON-like format for bibliographic entries. Encode each
source once as a \textsc{Bib}\TeX\ entry, give it a name and refer to it from
any place in your thesis. The bibliography at the end of the thesis will be
compiled automatically from those entries that are referenced at least once,
it will also be automatically sorted and fancyfied (URLs, DOIs, etc).

DOI is a digital object identifier, it is uniquely and immutably assigned to
any paper published in a well-established journal or conference proceedings
and can be used to refer to it. When used in a browser, it resolves to a
publisher's website where paper can be obtained. Including DOIs in citations
is considered good practice and lets the readers of your thesis get to the
text of the paper in one click. Books do not have DOIs, only ISBNs; some
workshop proceedings and most unofficial publications do not have DOIs. If you
want to get a DOI assigned to your work such as a piece of code, upload it to
\href{http://www.figshare.com}{FigShare}.

Keys in key-value pairs within each \textsc{Bib}\TeX\ entry are never quoted,
values usually are, but can also be included within curly brackets or left as
is, which works fine for numbers (e.g., years). If you want to preserve the
value from any adjustments (e.g., no recapitalisation in titles), use curlies
\emph{and} quotes. Separate authors and editors by ``and'', which will
automatically be mapped to commas or left as ``and''s as necessary.

\section{Books}

\cite{GruneJacobs} is just as good as the Dragon Book, but newer and has an
awesome extended bibliography available for free.

\begin{snippet}
\begin{verbatim}
@book{GruneJacobs,
  author    = "D. Grune and C. J. H. Jacobs",
  title     = "{Parsing Techniques: A Practical Guide}",
  series    = "Monographs in Computer Science",
  edition   = 2,
  publisher = "Springer",
  url       = "http://www.cs.vu.nl/~dick/PT2Ed.html",
  year      = 2008,
}
\end{verbatim}
\end{snippet}

\section{Journal papers}

Not all TOSEM papers are hard to read~\cite{GrammarwareAgenda}.

\begin{snippet}
\begin{verbatim}
@article{GrammarwareAgenda,
  author      = "Paul Klint and Ralf L{\"a}mmel and Chris Verhoef",
  title       = "{Toward an Engineering Discipline for Grammarware}",
  journal     = "ACM Transactions on Software Engineering Methodology (TOSEM)",
  volume      = 14,
  number      = 3,
  year        = 2005,
  pages       = "331--380",
}
\end{verbatim}
\end{snippet}

\section{Conference papers}

There is no limit to how many grammars can be used in one paper, but the
current record stands at 569~\cite{Micropatterns2013}.

\begin{snippet}
\begin{verbatim}
@inproceedings{Micropatterns2013,
  author = "Vadim Zaytsev",
  title = "{Micropatterns in Grammars}",
  booktitle = "{Proceedings of the Sixth International Conference on Software Language Engineering
                (SLE 2013)}",
  year = 2013,
  editor = "Martin Erwig and Richard F. Paige and Eric Van Wyk",
  volume = "8225",
  series = "LNCS",
  pages = "117--136",
  address = "Switzerland",
  month = oct,
  publisher = "Springer International Publishing",
  doi = "10.1007/978-3-319-02654-1_7",
}
\end{verbatim}
\end{snippet}

\section{Theses}

The seventh PhD student of Paul Klint was Jan Rekers~\cite{Rekers92}.

\begin{snippet}
\begin{verbatim}
@phdthesis{Rekers92,
 author   = "J. Rekers",
 title    = "{Parser Generation for Interactive Environments}",
 school   = "University of Amsterdam",
 year     = 1992,
 url      = "http://homepages.cwi.nl/~paulk/dissertations/Rekers.pdf",
}
\end{verbatim}
\end{snippet}

There is also \texttt{mastersthesis} type with exactly the same structure for
referring to Master's theses.

\section{Technical reports}

The original seminal work introducing two-level grammars was never published
in any book or conference, but there is a technical report explaining
it~\cite{Wijngaarden65}. SMC, or \emph{Stichting Matematisch Centrum}, was the
old name of CWI fifty years ago.

\begin{snippet}
\begin{verbatim}
@techreport{Wijngaarden65,
        author      = "Adriaan van Wijngaarden",
        title       = "{Orthogonal Design and Description of a Formal Language}",
        month       = oct,
        year        = 1965,
        institution = "SMC",
        type        = "{MR 76}",
        url         = "http://www.fh-jena.de/~kleine/history/languages/VanWijngaarden-MR76.pdf",
}
\end{verbatim}
\end{snippet}

\section{Wikipedia}

You do not refer to Wikipedia from academic writing, it works the other way around.

\section{Anything else}

You can refer to pretty much anything (websites, blog posts, software) through
\texttt{misc} type of entry~\cite{ANTLR}:

\begin{snippet}
\begin{verbatim}
@misc{ANTLR,
 author       = "Terence Parr",
 title        = "{ANTLR---ANother Tool for Language Recognition}",
 howpublished = "Software",
 url          = "http://antlr.org",
 year         = "2008"
}
\end{verbatim}
\end{snippet}

{%\tiny
\bibliographystyle{alphaurl}
\bibliography{thesis}
}

\end{document}
