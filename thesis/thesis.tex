\documentclass{uvamscse}

\usepackage{color}
\usepackage{url}
\usepackage{amsmath}


\usepackage{epigraph}
\setlength\epigraphwidth{8cm}
\setlength\epigraphrule{0pt}

\usepackage{etoolbox}
\makeatletter
\patchcmd{\epigraph}{\@epitext{#1}}{\itshape\@epitext{#1}}{}{}
\makeatother

\input{program-listings}
\newcommand{\cmd}[1]{\texttt{$\backslash$#1}}

\usepackage{graphicx}
\graphicspath{ {figures/} }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title page.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Geographically-aware scaling for real-time persistent websocket applications.}
\coverpic[100pt]{figures/logo.png}
\subtitle{Master's Project in Software Engineering}
\date{Summer 2015}

\author{Lukasz Harezlak}
\authemail{lukasz.harezlak@gmail.com}
\host{Instamrkt, \url{https://instamrkt.com}}

\abstract{
  This section summarises the content of the thesis for potential readers who do not have time to read it whole,
  or for those undecided whether to read it at all. Sum up the following aspects:

  \begin{itemize}
    \item relevance and motivation for the research
    \item research question(s) and a brief description of the research method
    \item results, contributions and conclusions
  \end{itemize}

Kent Beck~\cite{JohnsonBBCGW93} proposes to have four sentences in a good abstract:

  \begin{enumerate}
    \item The first states the problem.
    \item The second states why the problem is a problem.
    \item The third is the startling sentence.
    \item The fourth states the implication of the startling sentence.
  \end{enumerate}
}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Let the juicy stuff start.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

\epigraph{"Does this solution scale?"}{--- Every business person ever}

Everyone these days, probably more than ever, wants to build solutions that scale. Together with the globalization of our lifes and businesses grows the need for global, distributed, scalable software systems.

To address this needs, more and more providers offer Cloud Computing solutions \ref{Cloud Scalability}, giving rise to the new architecture model called \textit{Infrastructure as a Service (IaaS)}. This makes building scalable software systems easier, since the system engineers do not have to design and maintain their proprietary server infrastructure anymore. This also allows to build systems that are more efficient in terms of resource usage - the existing \textit{IaaS} solutions offer scaling up and down capabilities in response to changes in demand for system's services.

As more and more systems are deployed to the cloud, a need for measuring their properties becomes more and more ubiquitous.

In order to reason about scalablity-related properties of the system, one needs to define this notion considered by many as vague. For reasoning about cloud scalability, I find the following definition most relevant:

\begin{quote}
\cite{WeinstockOnSystem2006} Scalability is an ability of a system to handle increased workload by repeatedly applying a cost-effective strategy for extending a system’s capacity.
\end{quote}

Furthermore, to have a scientific discussion about scalability, quantifiable models and metrics are necessary. Selection of an appropriate scalabality measurement model was a big part of my initial study.

\section{Initial Study}

Initial study pointed me to the relevant scalability vectors discussed by researchers and engineers regarding cloud computing. Each of them has a dedicated section in the Background Chapter \ref{Scalability}:

\begin{itemize}
  \item General Cloud Scalability (\ref{Cloud Scalability})
  \item Data Layer Scalability (\ref{Data Layer Scalability})
  \item WebSocket Scalability (\ref{Websocket Scalability})
\end{itemize}

The study yielded multipile scalability measurement models. I analyzed the work of Jogalekar and Woodside \cite{EvaScaDS}, Srinivas and Janakiram \cite{SriJan}, Cooper et al. \cite{Ycsb} and multiple others.

Some of the proposed models are note applicable since they were designed for different types of scalability - not for a cloud-based distributed system, but for instance for multiprocessor or multithreading scalabilities.

The models I selected to use for measurements in this research were proposed by Pattabhiraman et al. \cite{Models}. They were designed specifically for the task I am performing - measuring properties of a distributed system deployed in a cloud environment. Their models are also customizable - one can plug in metrics one deems relevant, which allowed us to select metrics that are representative of the business goal of the System Under Test \ref{The System Under Test}.

Selection and application of literature-based models is described in full in Scalability Measurements chapter \ref{Scalability Measurements}.

\section{Problem Statement}\label{Problem Statement}

The problem that I study in this research concerns application stack decomposition of the System Under Test \ref{The System Under Test}. It can be generalized to scalability of systems with similar characteristics:

\begin{itemize}
  \item utilizing real-time, uncacheable data,
  \item utilizing user-generated data,
  \item broadcast to user base distributed globally,
  \item broadcast using the WebSocket protocol \ref{Websocket Protocol},
  \item deployed in the cloud,
  \item dynamically scalable (up and down).
\end{itemize}

This regards the size and geographical placement of WebSocket server instances, database and cache server instances (with data distribution), routing between them and iternal layer communication.

I propose and compare three architectures \ref{Compared Architectures} to find which satisfies scalability criteria best.

More specifically, the host company is in need of scaling their system. The initial research showed little indication of which architecture deomposition will provide lowest client-experienced latencies, best allocated resources utilization and lowest operating cost. On top of that, they do not posses any tools or frameworks that would allow them to measure their system internally and externally. Neither are they in possession of a framework to generate the load test on which the scalability of the system could be measured.

\subsection{Research Questions}
\begin{enumerate}
  \item What's the architecture decomposition stack that:
    \begin{enumerate}
      \item guarantees data delivery to geographically distributed users with minimal, consistent and manageable latency?
      \item scales up and down most effectively in response to demand changes?\footnote{This proved to be untestable as the researched progressed, because of the nature of DNS protocol, described in detail \ref{Excluded Metrics}.}
      \item guarantees lowest cost and best degree of utilization of the employed resources within a data center?
    \end{enumerate}
    The answer to these question is seeked among the 3 tested architecture \ref{Compared Architectures}.
  \item Does architecture with geographical awareness of its users' distribution provide better Quality of Service than the baseline architecture?
\end{enumerate}

Answers to these questions are evaluated using the models and a set of metrics described in detail in a dedicated chapter \ref{Scalability Measurements}.

\subsection{Solution Outline}

In order to answer the Research Questions, a solution consisting of the following steps was developed:
\begin{enumerate}
  \item \texttt{Preparing the load test suite}. It is used to simulate geographical distribution of users and load they generate as well as to collect latency-related metrics of WebSocket handshakes and message delivery. Described in details in a Load Testing Framework section \ref{Load Testing Framework}.
  \item \texttt{Preparing the load test suite}. It is used to collect data from the application servers. This consists of collecting Amazon's CloudWatch\footnote{\url{http://aws.amazon.com/cloudwatch/}} metrics and our custom metrics (e.g. process memory usage). Metric collection process is described in details in a dedicated section \ref{AWS Available metrics}.
  \item \texttt{Feeding the collected metric into scalability measurement models}.
  \item \texttt{Automatic scalability management}. This is performed using Amazon's CloudWatch Alarms\footnote{\url{http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/AlarmThatSendsEmail.html}} triggering Auto Scaling Groups \footnote{\url{http://aws.amazon.com/autoscaling/}} capacity changes. In order to include our custom metrics in orchestrating the scalability process, a module based on Amazon's Python library (Boto\footnote{\url{http://aws.amazon.com/sdk-for-python/}}) was built.
\end{enumerate}

\subsection{Research Method}
Software Engineering is a relatively difficult field to investigate. Most of the problems are of design (rather than pure scientific) nature. West Churchman coined a specific term for these kind of problems \cite{Wckd}. He calls them \texttt{wicked}, since they are resistant to resolutions.

Easterbrook et al. \cite{EastBr} claim it is often difficult to identify the true underlying nature of the research problem and thus the best method to research it. In their work, they name and compare five most common classes of research methods to select from:
\begin{itemize}
  \item controlled experiments,
  \item case studies,
  \item survey research,
  \item ethnographies,
  \item action research.
\end{itemize}
They help to select the method by first establishing the type of research question being asked:
\begin{itemize}
  \item existence question - \textit{Does X exist?},
  \item description and classification question - \textit{What is X like?},
  \item descriptive-comparative question - \textit{How does X differ from Y?}.
\end{itemize}
The questions of this research are of the last type. The authors \cite{EastBr} suggest pinpointing up-front what will be accepted as a valid answer to the research question. I answer this question in a separate section \ref{Expected Answers}.

The detailed description of each of the methods helped me to settle for the \texttt{controlled experiment} as a research method for this research. It is well-suited for testing a hypothesis where manipulating independent variables has an effect of dependent ones, which is exactly the case of my research. The manipulated variable is architecture decomposition, and the measured ones are determined by scalability measurement models described in Scalabality Measurements \ref{Scalability Measurements} chapter.

\subsection{Research Difficulty}
The experiments is performed in a shared cloud environment, which is inherently unpredictable and changing. Non-deterministic network conditions play a significant role here. Aside from that, one has to acknowledge the fact that control over hardware in a cloud environment is by definition given up by the researcher.

Designing and executing a test, yielding statistically relevant results, where all the necessary variables are controlled (within reasonable boundaries) is a huge challenge. Bornholt \cite{BornBen} mentions that even the linking order in the process of generating binaries can have an effect of the performance of the benchmarked system!

The focus of the project is on scaling a websocket application. This is a relatively new technology, thus finding proper scientific coverage is not trivial.

Some of the necessary development and data collection tools are relatively low-level and require thorough understanding of UNIX operating systems, on which the servers are deployed. The same is applicable to tuning the operating system for optimal TCP-related performance \ref{Kernel tuning}.

Simluating the geo-location of clients with reasonable accuracy also proves problematic.

Overall, it’s a complex project requiring knowledge of multiple aspects of both hardware (TCP, WebSocket protocol, networks, operating system), software (architecture, cloud computing, data replication/sharding).

\subsection{Hypothesis}
The correct geographical decomposition of the stack of The System Under Test \ref{The System Under Test} can lead to improved performance in comparison with the Baseline Architecure \ref{Baseline Architecture}.

\section{Contributions}
The research will result in the following contributions:

\begin{enumerate}
  \item Architecture decomposition analysis for system's meeting the criteria described in the Problem Statement section \ref{Problem Statement}.
  \item Case Study on Amazon Web Services\footnote{\url{http://aws.amazon.com/}} and usability of their cloud stack for applications using WebSocket protocol.
  \item Open sourced pieces of code for:
  \begin{itemize}
    \item metrics collection (both from the load test suite and the servers),
    \item auto-scaling execution,
    \item constructing scalability models and populating them with collected metrics,
    \item remote execution of load tests on a fleet of client-simulating servers.
  \end{itemize}
  These are described in details in Experiment Deliverables section \ref{Experiment deliverables}.
\end{enumerate}

\section{Related Work}\label{Related Work}

There is two key papers this research is based on. First of them is \textit{Multi-Cloud Provisioning and Load Distribution for Three-Tier Applications} by Grozev and Buyya \cite{GroBuy}. The second one is \textit{SaaS performance and scalability evaluation in clouds} by Pattabhiraman et al \cite{Models}.

\subsubsection{Multi-Cloud Provisioning and Load Distribution for Three-Tier Applications}

In this work, the authors tackle the problem of deploying applications across multiple clouds while satisfying sytem's nonfunctional requirements (in their understanding, application deployed in Amazon's \textit{us-east-1} region and \textit{eu-west-1} region would be deployed to multiple clouds). They propose algorithms for resource provising and load distribution. They focus on resource utilization and cost and legal regulations regarding serving users and servers geographical location and include them in their algorithms.

In order to satisfy the requirements, the authors had to develop their own routing stack. Both Amazon's services involved in the routing process -- Route 53 \footnote{\url{http://aws.amazon.com/route53/}} and Elastic Load Balancing\footnote{\url{http://aws.amazon.com/elasticloadbalancing/}} -- do not consider application's regulatory requirements when selecting a data center site. Moreover, they do not consider the cost and degree of utilization of the employed resources within a data center.

This is how they perform cloud selection to serve a user:
\begin{enumerate}
  \item As a first step, the user authenticates with one of their entry point servers. At this point, the entry point has the user’s identity and geographical location (extracted from the IP address).
  \item As a second step, the entry point broadcasts the user’s identifier to the admission controllers of all data centers. They call this step \textit{matchmaking broadcast}.
  \item The admission controllers respond to the entry point whether the user’s data are present and if they are allowed (in terms of legislation and regulations) to serve the user. In the response, they also include information about costs within the data center.
  \item Based on the admission controllers’ responses, the entry point selects the data center to serve the user and redirects him or her to the load balancer deployed within it. The entry point filters all clouds that have the user’s data and are eligible to serve him or her. If there is more than one such cloud, the entry point selects the most suitable with respect to network latency and pricing. If no cloud meets the data location and legislative requirements, the user is denied service.
  \item After a data center is selected, the user is served by the Application Server and Database servers within the chosen cloud.
\end{enumerate}

Below I list most relevant takeaways from their work.

\begin{description}
  \item[Sticky Load Balancing] - it is a technique of load balancing that routes same users to the same backend server instaces. They utilize it in their work - I cannot in this research. Amazon's stack does not offer sticky load balancing for protocols other than HTTP.
  \item[Cloud Server Instance Termination] - it is not beneficial to terminate a running VM ahead of its next billing time. It is better to keep it running until its billing time in order to reuse it if resources are needed again.
  \item[Cost Minimization] - this is not an automated feature cloud providers offer. Authors advice on how to include it in a cloud controlling stack. It is an important business goal in my researh.
  \item[Latency Minimization and Approximation] - authors, just as myself, treat latency as one of the key objectives in the process of optimizing their cloud stack. They approximate latencies and users geographic location using tools PingER\footnote{\url{http://www-iepm.slac.stanford.edu/pinger/}} and GeoLite\footnote{\url{http://dev.maxmind.com/geoip/legacy/geolite/}}. PingER is used to approximate latencies between user and a cloud and GeoLite to map users IP addresses to geographical coordinates. In my research, I do not need to use tools like that since Amazon's stack offers this in sufficient capacity.
\end{description}

Why is this work relevant? The authors are attempting to solve the same problem - how to distribute and route users between multiple clouds given arbitrary conditions. They work with a similar technology stack as I do.

\subsubsection{SaaS performance and scalability evaluation in clouds}

Pattabhiraman et al realized that cloud computing and its measurement provides a new set of challenges when it comes to measuring performance, as opposed to measuring performance of traditional software systems. They list key points for measuring cloud applications - validating and ensuring the elasticity of scalability and evaluating utility service billings and pricing models. Both are important in my case. They raise an interesting question regarding the latter:

\begin{quote}
 How to use a transparent approach to monitor and evaluate the correctness of the utility bill based on a posted price model during system performance evaluation and scalability measurement?
\end{quote}

The authors divide the performance indicators into three groups and propose formal quantifiable meters for each of them:
\begin{enumerate}
  \item computing resource indicators (CPU, disk, memory, networks, etc.),
  \begin{itemize}
    \item Computing Resource Allocation Meter (CRAM),
    \item Computing Resource Utilization Meter (CRUM),
  \end{itemize}
  \item workload indicators (connected users, throughput and latency, etc.),
  \begin{itemize}
    \item System Load Meter (SLM),
  \end{itemize}
  \item performance indicators (processing speed, system reliability, availability)
  \begin{itemize}
    \item System Performance Meter (SPM).
  \end{itemize}
\end{enumerate}
They also propose model which allow to combine the above: System Capacity Meter (SCM), System Effective Capacity Meter (SEC), Effective System Scalability (ESS), and Effective Scalable Range (ESR). Values that are fed into these meters are customizable. The meters are described in detail in the Scalability Measurements chapter \ref{Scalability Measurements}.

Authors perform a case study with Amazon's AWS stack. The case study consists of performed tests and measurements taken on scaling up and scaling down their application. It provides some warnings for me to consider:
\begin{itemize}
  \item Cloud limitations need to be taken into account. For instance, memory usage data is not collectible from their server instances using their services.
  \item One needs to be aware of hidden costs that cloud providers excel at introducing.
  \item One needs pay attention to inconsistencies in performance and scalability data, especially data collected using third-party tools and.
\end{itemize}

More related work is described in the following chapter - Background \ref{Background}.

\section{Document Outline}
In this section I outline the structure of the thesis - all chapters with short summaries.

\begin{enumerate}
  \item \texttt{Background} - contains a short summary of the literature study. Describes researched work in the relevant fields \ref{Background}.
  \item \texttt{Benchmarking} - contains a description and guidelines regarding the benchmarking process, which constitutes a big part of this research. Lists typical benchmarking mistakes and what I do to avoid them \ref{Benchmarking}.
  \item \texttt{The System Under Test} - contains a description and guidelines regarding the benchmarking process, which constitutes a big part of this research. Lists typical benchmarking mistakes and what I do to avoid them \ref{The System Under Test}.
  \item \texttt{Experiment Outline and Design} - contains a description of the conducted experiment, its goals and deliverables. Full description of the compared architectures and cloud stack on which the research is performed is also included in this chapter \ref{Experiment Outline and Design}.
  \item \texttt{Scalability Measurements} - contains a description of literature-based scalability metrics and models. Lists the models and metrics selected for measurements in this research, as well as all metrics collectible from the AWS cloud environment \ref{Scalability Measurements}
  \item \texttt{Experiment results} - contains the data collected in the experiments \ref{Experiment results}.
  \item \texttt{Evaluation} - contains evaluation of the experiment and analysis of the results \ref{Evaluation}.
  \item \texttt{Conclusions} - contains a list of conclusions derived from the analysis of the experiment results \ref{Conclusions}.
  \item \texttt{Further Work} - contains a description of relevant, testabl variables I did not have enough time to cover in this research. \ref{Further Work}
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}\label{Background}

\section{Scalability}\label{Scalability}

Scalability seems to be a notion that everyone intuitively grasps, but has difficulties when it comes to clear explanations. In my literature study I came across a few definitions, which might help with that, of which three can be found below:

\begin{quote}
~\cite{Williams04} Scalability is a measure of an application system’s ability to, without modification, cost-effectively provide increased throughput, reduced response time and/or support more users when hardware resources are added.
\end{quote}

\begin{quote}
\cite{WeinstockOnSystem2006} Scalability is an ability of a system to handle increased workload (without adding resources).
\end{quote}\label{x}
In light of this definition we would talk about a scalability failure if one of the following occured:
\begin{itemize}
  \item Address space was exceeded,
  \item Memory was overloaded,
  \item Available network bandwith was exceeded,
  \item etc.
\end{itemize}

\begin{quote}
\cite{WeinstockOnSystem2006} Scalability is an ability of a system to handle increased workload by repeatedly applying a cost-effective strategy for extending a system’s capacity.
\end{quote}
According to this definition, one could determine system scalability failure if a given resource got overloaded or exhausted an adding capacity to this resource would not result in a proportional ability to handle additional demand, e.g.:
\begin{itemize}
  \item an additional processor will not contribute to meeting the higher demand if handing of that processor entails an overhead).
  \item a newly added server instance might not contribute to handling a higher user demand if slows down the routing process.
\end{itemize}

\subsection{Scalability Trade-offs}
Scalability is generally desired in the software systems, yet, as all architectural software decisions come at a cost \cite{GerHeiBench}; one must be aware of the trade-offs usually associated with it. Weinstock and Goodenough \cite{WeinstockOnSystem2006} and Tsai et al. \cite{WeiTek} point these out:
\begin{itemize}
  \item performance and scalability (non-scalable system will often demonstrate degrading performance with increasing demand, but scalable systems require performance sacrifice on lower usage levels),
  \item cost and scalability (designing a system to be scalable up-front entails additional costs),
  \item operability and scalability (it is difficult for humans to operate large systems),
  \item usability and scalability (it may be possible to increase servable demand with limiting system's service scope - e.g. removing personalization and displaying generic, cacheable data),
  \item data consistency and scalability (higher scalability can be achieved if system allows for data inconsistencies).
  \item levels of scalability mechanisms and automated migration (automated migration has to be aware of every level of scalability mechanism; the more of them the more complex the migration),
  \item workload support and database access (different workloads need support for different database access mechanisms, such as row-oriented storages, colum-oriented storages or hybrid approaches).
\end{itemize}

\subsection{Need for Scalability}

As the globalization and internetization progresses, more and more systems are expected to be capable of serving millions of globally distributed users. A single server instance often cannot live up to that task and thus application needs to be divided and distributed in multiple smaller chunks. This division happens on different application layers, and different parts of the system have to communicate and synchronize with each other.

In case of many software systems (the system under test \ref{The System Under Test} included), user traffic, and with it the need for system services and resources, varies significantly. A need to be able to scale up and down dynamically in response to traffic arises from this.

Huge parts of the internet are shifting towards real-time. This trend is giving rise to new technologies for exchanging messages between clients and servers in the client-server architecture. Traditionally, client would send a request to a server and receive a response. This is hugely inefficient when there is a need for continuous bidirectional exchange of messages. As an improvement, new mechanisms for server-client communication have been introduced recently \ref{Client-Server Communication Improvements} to enhance the process and reduce overhead. One of them - websockets - are a huge next step on this path, but introduce new challenges. One of them is scalability of applications which make use of this technology.

\subsection{Existing Approaches}
Scalability has been studied in parallel computing, distributed computing, databases, and service-oriented architectures (SOA) \cite{WeiTek}. Scalable design principles for application servers include: divide-and-conquer, asynchrony, encapsulation, concurrency, parsimony, system partitioning, service-based layered architecture, pooling and multiplexing, queuing and background processes, data synchronization, distributed session tracking, and intelligent load distribution.

There exist multiple scalability vectors for web applications. Different decompositions of application stack can be applied to achieve the goal of scalability. A few traditional approaches of tackling an issue like that exist already \cite{Akamai}:
\begin{description}
  \item[Scaling up.]
  Increasing volume of system allocated resources (a stronger machine, or with a better configuration, including more computing resource, more memory, higher disk bandwidth and larger disk space \cite{WeiTek}). As internet is unreliable and so called "middle-mile bottlenecks" exist, a web application end-user latency and throughput experience is not fully deterministic. Traffic levels fluctuate tremendously, so the need to provision for peak traffic levels means that expensive infrastructure will sit underutilized most of the time. It is easy to implement, but costly, even extremely when you start pushing at current hardware limits.\cite{Qvef} Because the resources of a single machine cannot be increased infinitely, and increase of the cost is not proportional to the increase of the resources, scale-up is not feasible in an Internet scale \cite{WeiTek}.
  \item[Scaling out.]
  Scaling out horizontally - increasing the number of units of resources comprising the system. Cost of hardware can be reduced dramatically this way. In a web application, one can deploy multiple instances of servers. Different types of balancing can be applied to distribuet traffic among them: application layer balancing, business load balancing, and anticipating load. The overhead of parsing requests in the application layer is high thus limiting scalability compared to load balancing in the transport layer. Client state needs to be stored in a layer shared between the webservers.\cite{Qvef}  Scale-out is usually adopted in a cloud \cite{WeiTek}.
  \item[Content Delivery Networks.]
  These only handle static assets. Communication in our system \ref{The System Under Test} happens mainly over websockets, which are not supported by CDNs. CDNs can be divided into Big Data Center CDNs and Highly Distributed CDNs, which put application data within end-user ISPs.\cite{Akamai}
  \item[Peer to peer networks.]
  An architecture different from client - server, where users communicate with each other directly. It handles adding and removing nodes to and from the network dynamically very well.
\end{description}

\subsection{Scalability Factors}
There exist multiple scalability factors one has to be aware of when designing and implementing scalability of a software system\cite{WeiTek}.

\subsubsection{Levels of Scalability Mechanisms}
Most applications deployed in the cloud adopt a tiered application - typically the storage, application and presentation tiers. Cross-tier scaling is not very common because of the high cost of duplicating the whole application stack. Hybrid approach is more popular - the whole stack is duplicated but single tier scaling within the stack is leveraged.

Scalability mechanisms for each tier might be different, since each of them has its own intrinsic properties, objectives and constraints.

\subsubsection{Automated Migration}
For many systems, automatic migration of data is critical for scalability. This migration can be performed online (the application continous operating throughout the whole process) or offline (the application is taken down for maintenance).

\subsubsection{Workload Support}
Different workloads require different scalability mechanisms. Online Analytical Processing (OLAP) applications have to be prepared to scale for high volume of read operations, whereas Online Transaction Processing (OLTP) applications have to be prepared to scale for a lot of write operations. Mixed workloads (as is the case for The System Under Test \ref{The System Under Test}) require an architecture ensuring no biases towards either type of operations.

\subsubsection{Recovery and Fault-tolerance}
A scalable system needs to detect failures of nodes and scale down to exclude the failed node from operation. Similarily, when the failed node comes back to the system, it should scale up including the recovered node. In a fully scalable system, this process should be autonomous.

\subsubsection{Software Architecture}
In a scalable system, architecture should be devided into decoupled modules which can be scaled independently. Authors \cite{WeiTek} use Service-Oriented Architecture as a model example of an architecture that many scalable systems adopt.

\subsubsection{Database Access}
Accessing data is often time consuming. This is why data layer is likely to be a bottleneck of a SaaS application. Indirect access (acessing the data layer through an exposed API) allows data tier and other tiers to scale independently using their own preferred mechanisms.

\subsection{Cloud Scalability}\label{Cloud Scalability}
Cloud computing has become virtually ubiquitous. All biggest internet services are either deployed to a cloud, or run their proprietary cloud systems. A lot of companies running proprietary clouds also make them available for hosting to the external clients. Netflix\footnote{\url{https://aws.amazon.com/solutions/case-studies/netflix/}} and Spotify\footnote{\url{https://aws.amazon.com/solutions/case-studies/spotify/}} (both deployed to Amazon's AWS) are examples of the first approach, with Microsoft(Azure)\footnote{\url{https://azure.microsoft.com/}}, Google(Google Cloud Platform)\footnote{\url{https://cloud.google.com/}} and Amazon(Amazon Web Services)\footnote{\url{https://aws.amazon.com/}} being the example of the second. These services are typically billed on a resource utility basis.

General cloud capabilities and the cloud stack I am working with for the scope of this project is discribed in a dedicated chapter \ref{Cloud architecture setup}.

One of the biggest advantages of deploying one's architecture to the cloud that aforementioned companies offer is the ability to dynamically scale up and in response to application traffic changes.

As Grozev and Buyya \cite{GroBuy} put it, to fully facilitate cloud capabilities, software engineers need to design for the cloud, not only to deploy to it.

\subsection{Data Layer Scalability}\label{Data Layer Scalability}

Data layer scalability is an important part of system scalability. In a distributed system, multiple system agents share the data. There exist multiple strategies for operating on a shared, distributed data layer.

Data layer is often a performance bottleneck because of requirements for transactional access and atomicity - it is hard to scale out when system uses a relational data store \cite{GroBuy}.

\begin{description}
  \item[CAP Theorem]
  put forward by Eric Brewer \cite{Cap} states that it is impossible for a distributed system to provide all of the following guarantees:
  \begin{itemize}
    \item consistency,
    \item availability,
    \item partition tolerance.
  \end{itemize}
  Therefore, a trade-off needs to be made, depending on stakeholder priorities, which of the three to give up.
  \item[ACID]
   stands for Atomicity, Consistency, Isolation, Durability and is a set of properties guaranteeing reliable processing of database transactions. It has been a guideline in designing multiple database systems. The term was originaly coined by Haerder and Reuter in 1983 \cite{ACID}.
  \item[BASE]
   stands for Basically Available, Soft State, Eventually Consistent \cite{EveCon}. It’s a complement of ACID. Author claims we lack precise metrics to measure BASE aspects and that’s why every system implements eventual consistency differently.

  Most importantly for this research, the author \cite{EveCon} explains \textit{MySQL Cluster} - it performs much like an ACID database but with the performance benefits of a cluster. Aside from that, \textit{MySQL Replication} can be put to use. It can be configured at multiple topologies, not only the basic master-slave; consistency differs per configuration.
\end{description}

Many solutions and strategies for dealing with database scalability have emerged, including \textit{NoSQL} and \textit{NewSQL} databases, data replication and sharding \cite{Amza}.

Cooper et al touch on that subject in their work \cite{Ycsb}. They claim that in scaling out the database layer one should aim for elasticity (dynamically adding capacity to a running system) and high availability. These are hard to achieve using traditional database systems. They show that new protocols are being developed to address that issue, such as that: \textit{two-phase commit protocol} (provides atomicity for distributed transactions) and \textit{paxos}.


The authors \cite{Ycsb} also give an overview of different database systems, including \textit{PNUTS}, \textit{BigTable}, \textit{HBase}, \textit{Cassandra}, \textit{Sharded MySQL}, \textit{Azure}, \textit{CouchDB}, and \textit{SimpleDB}.


In their work \cite{Ycsb}, we can find enumeration of classic data-related scalability trade-offs:
\begin{itemize}
    \item read performance and write performance,
    \item latency and durability,
    \item synchronous and asynchronous replication,
    \item data partitioning (column and row-based storage).
  \end{itemize}


The technology stack I have been working with in the scope of this project consists of \textit{MySQL} as persistent storage and \textit{Redis} as a key-value cache. It is described in details in a dedicated chapter \ref{Technology Stack}.

\subsubsection{MySQL Scalability}

Mysql White paper \cite{MySQL} gives us a good overview of how scalability works in case of MySQL Cluster. Authors suggest starting the design of the scaling process by identifying which characteristic the application posseses: lots of write operations, real-time user experience, 24x7 user experience or agility and ease-of-use. The System Under Test \ref{The System Under Test} falls into the first and second categories.

They claim to support (among others) auto-sharding for write-scalability, active / active geographic replication and online scaling and schema upgrades.  Geographic replication offers distribution of clusters across remote data centers, which they claim helps reduce latency (which is extremely important case of The System Under Test \ref{The System Under Test}).

Authors show how MySQL Cluster is optimized for real-timeness:
\begin{itemize}
  \item data structures are optimized for in-memory access,
  \item persistence of updates runs in the background,
  \item all indexed columns are stored in memory.
\end{itemize}
According to the white paper \cite{MySQL}, MySQL Cluster is very well-suited for on-line, on-demand scalin.

MySQL can also be used to scale in unconventional ways. Ruflin et al, in Social-Data Storage-Systems \cite{SoDaSS}, mention that Twitter uses MySQL as a key value store.

They show how MySQL can be scaled horizontally by both sharding and data replication. They also indicate that the more structured the RDBMS data, the harder it is to scale horizontally. MySQL is optimized for writes, since only one record in one table is touched, whereas reads can prove expensive if they contain joints, especially spreading across multiple cluster nodes. Facebook and Twitter solved it by putting a cache on top of MySQL \cite{SoDaSS}.

\subsubsection{Redis Scalability}
Single Redis installation is said to be of limited scalability, because of the fact that for good performance the whole data set should fit into memory \cite{SoDaSS}. Redis offers cluster\footnote{\url{http://redis.io/topics/cluster-spec}} and replication\footnote{\url{http://redis.io/topics/replication}} solutions.

In a Redis Cluster, data is automatically sharded (not replicated) among multiple nodes. One has control over sharding; it is possible to ensure that certain data points end up on the same (or on a given) node. Redis Cluster does not guarantee strong consistency - under certain conditions the Cluster can lose writes that were acknowledged to the client. Support for synchronous writes exists, through the command \textit{WAIT}, which highly (but not entirely) decreases the likelihood of lost writes. Nevertheless, using it is discouraged unless absolutely necessary.

Redis can also be scaled for reads using a simple replication in a single master - multiple slaves topology.

\subsection{Websocket Scalability}\label{Websocket Scalability}

An introduction into push-base communication over the internet can be found in Agarwal’s work - Toward a Push-Scalable Global Internet \cite{PushScale}. The key message in the article is that push message delivery on the World Wide Web is not scalable for servers, intermediate network elements, and battery-operated mobile device clients. And yet, most of modern day websites have highly dynamic content updated even up to multiple times a minute (very much so for The System Under Test \ref{The System Under Test}).

Most of internet communications happens over HTTP Running over TCP. Real-time message delivery requires an always-on connection from the server to the client. HTTP proxies have limited memory and TCP ports, are shared among multiple users. Servers need to be provisioned in order to maintain active TCP connections from large populations of user  clients. These all provide challenges that need to be dealt with in scalable applications \cite{PushScale}.

A number of TCP ports available (and file descriptors available to a server process) on a server instance is an inherent scalability limitation of an application using the WebSocket protocol. Hardware limitations are different in an HTTP-based communication.

On a more positive note, Cassetti and Luz \cite{WebsApi} claim that overhead introduced by the websocket protocol and websocket API is rather small as compared to other communication methods. Furthermore, one data intensive applications can achieve superior bandwidth and performance when using websockets.

My own research has proven preparing websocket-based communication scalability to be difficult and tedious. Most of the cloud scalability stack that is available and that I have been working with was designed and built to support http-based -- and not websocket-based -- communication. On top of that, one needs to tweak the host operating system kernel in multiple ways to increase the number of concurrent websocket connections the system can maintain. Details of the tuning process are described later \ref{Kernel tuning}.

The Websocket Protocol is described in details in a dedicated section \ref{Websocket Protocol}.

\subsection{Measuring Scalability}
Measuring scalabliity proves a challenge, since there is no single physical quantity or unit which the community has accepted as a scalability measure.

Measuring and researching in a cloud environment is difficult, because, by definition, the researcher has no control over the hardware that his system is running on. Clouds are also inherently non-deterministic. Nevertheless, as Sobel et al. describe in their work \cite{Sobel}, even in cloud computing environments, where researchers have little control over network topology or hardware platform, understanding the performance bottlenecks and scalability limitations imposed by the offered infrastructure is valuable.

As this is a crucial topic to my research, this topic is explored in detail in a dedicated chapter \ref{Scalability Measurements}.

\section{Geographical Distribution}

With the rise of global internet services, single applications have to server users who are distributed around the whole globe. With the distribution of the users, comes distribution of the data. Multiple solutions exist that, through data distribution, aim at improving the \textit{Quality of Service}.

The topic is nicely introduced by Tom Leighton in his article \textit{Improving performance of the internet} \cite{Akamai}. He provides a few arguments why it is important to keep data as close to end users as possible. Apart from obvious latency benefits, by doing this, one reduces the chances of suffering from a big \textit{middle mile} provider outage.
He introduces a scale to reason about internet locality.

\begin{table}[h]
\begin{center}
\begin{tabular}{llll}
  \texttt{Scale Name}       & \texttt{Range}        & \texttt{Latency Range}  & \texttt{Typical Packet Loss}\\
  \hline
  local                     & less than 100 miles   & 1ms                     & 0.6\% \\
  regional                  & 500 - 1000 miles      & 15ms                    & 0.7\% \\
  cross-continent           & 3000 miles            & 50ms                    & 1.0\% \\
  multi-continent           & 6000 miles            & 100ms                   & 1.4\% \\
\end{tabular}
\end{center}
\caption{Internet Locality}
\label{table:internetLocality}
\end{table}
As the table \ref{table:internetLocality} shows, the longer data must travel through the middle mile, the more it is subject to congestion, packet loss, and poor performance \cite{Akamai}. This is why companies try to locate servers close to end users (on a scale that cannot be reproduced in this research - requires finer control over infrastructure). The lowest granular scale I am able to work with within this research is \textit{regional}. Leighton writes that big websites have at least two geographically dispersed mirror locations to improve performance, reliability and scalability.

In his work, he includes a set of guidelines to consider when performing geographical scalability:
\begin{itemize}
  \item reduce transport layer overhead (I am achieving this with usage of the websocket protocol),
  \item prefetch embedded content,
  \item assemble pages at the edge (even on an end client machine),
  \item offload computations to the edge,
  \item ensure significant redundancy in all systems to facilitate failover,
  \item use software logic to provide message reliability.
\end{itemize}

Ed Howorka in his interesting paper \textit{Colocation beats the speed of light} \cite{EdHoColoc}  about trading system geographical distributions focuses on the best placement of servers for traders trading on multiple exchanges. His paper demonstrates that traders gain nothing by positioning their computer at the midpoint between two financial exchanges. He claims that  every algorithm on a central machine talking to surrounding servers (users in case of the System Under Test \ref{The System Under Test}) can be replaced by colocated servers (located in geographical proximity to users). Furthermore, he implies and sets out to prove that server colocation provides a better solution for high-speed applications (as opposed to using a big, centralized server located in the middle).


\section{WebSocket Protocol}\label{Websocket Protocol}

WebSockets is an independent, TCP-based communication protocol. The protocol was standardized by the Internet Engineering Task Force (IEFT) in 2011\footnote{\url{https://tools.ietf.org/html/rfc6455}} and has been gaining popularity ever since. All major web browsers and mobile operating system support it network\footnote{\url{http://caniuse.com/\#feat=websockets}}.

\subsection{Technical details}

Every WebSocket communication has to start with an opening handshake. WebSocket us a protocol related to HTTP, in a sense that its handshake is interpreted by HTTP servers as an \textit{Upgrade request}. Websocket also operates on the same ports as HTTP (80 and 443), so as not to get the communication blocked by all the internet's intermediaries configured e.g. to allow exclusively HTTP traffic.

A sample handshake included in the client's request is depicted below. It is a simple \texttt{GET} request. A set of random bytes - \texttt{Sec-WebSocket-Key} - needs to be included.

\begin{sourcecode}
\begin{lstlisting}[style=mono]
GET HTTP/1.1
Upgrade: websocket
Connection: Upgrade
Host: echo.websocket.org
Origin: http://www.websocket.org
Sec-WebSocket-Key: i9ri`AfOgSsKwUlmLjIkGA==
Sec-WebSocket-Version: 13
Sec-WebSocket-Protocol: chat
\end{lstlisting}
\caption{Websocket Upgrade Client Request}
\end{sourcecode}

The server taes the \texttt{Sec-WebSocket-Key}, appends a globally unique identifier (GUID) string, computes a \texttt{SHA1} hash from it and \texttt{Base64}-encodes it. The computed value is sent in the response header confirming the upgrade as \texttt{Sec-WebSocket-Accept}:

\begin{sourcecode}
\begin{lstlisting}[style=mono]
HTTP/1.1 101 Web Socket Protocol Handshake
Upgrade: WebSocket
Connection: Upgrade
Sec-WebSocket-Accept: Qz9Mp4/YtIjPccdpbvFEm17G8bs=
Sec-WebSocket-Protocol: chat
Access-Control-Allow-Origin: http://www.websocket.org
\end{lstlisting}
\caption{Websocket Upgrade Server Response}
\end{sourcecode}

That leads to opening a websocket (\textit{ws://}) communication channel between the client and the server. Secure websocket connections (\textit{wss://}) are also possible.

\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{ws}
\caption{WebSocket Protocol}
\end{figure}

Typically mentioned benefits of using WebSocket protocol include reduced overhead in comparison with HTTP communication. After the initial handshake, only application-specific data travels between client server, as opposed to HTTP where every request/response contains headers, cookies, content type, content length, user-agent, server id, date, last-modified etc.

WebSocket protocol has an another advantage over HTTP on a TCP protocol level (on which both are based) - TCP connection only needs to be opened once per WebSocket communication, whereas for HTTP it is opened for every request (which introduces overhead).

What is more, HTTP servers are often configured to persist in a log form the start and completion of every HTTP request. This entails additional CPU cycles and costly I/O operations. Unless an application is configured to log its proprietary data, this additional logging cost is much smaller in case of WebSockets (since there is only one initial request).

Configuring a server for high-scalability WebSocket communication is no easy task, as I have learned the hard way during this research. The whole process is decribed in details in a dedicated section \ref{Kernel tuning}.

\subsection{Usage and Origins}\label{Client-Server Communication Improvements}

Multiplayer online games and real-time applications with a lot of user generated content are types of application that benefit most from popularization of the WebSocket protocol.

One of the biggest advantages of websockets is that the server can easily send unsolicited messages to the client, rather than simply respond to client's requests (as is the case with HTTP protocol). Before websockets, other solutions were being developed to enable bi-directional client-server communication, yet none of them has gained such popularity and is considered as elegant.

Before websockets, the applications mentioned above had to resort to other, less efficient ways to facilitate bi-directional client-server communication.

% M. Franklin and S. Zdonik provide an insight into the history of push-based technologies. Their paper from 1998 [MMPROJ12] classifies communication mechanisms into aperiodic pull, periodic pull, aperiodic push and periodic push.
Agarwal presents us with an overview of protocols previously used for the same kind of communication \cite{PushScale}:
\begin{description}
  \item[AJAX]
  - asynchronous JavaScript and XML, a request/response model; when originally introduced, it was a huge improvement, since the page didn’t have to be refreshed anymore to get new data. Still used widely, but for a slightly different purpose than the WebSocket Protocol.
  \item[Long Polling]
  - consists of the client sending a single request and the server waits until it can provide the response (or times out), which does not create much traffic but uses a lot of server resources. Connections are artificially kept open and clients need to reconnect periodically.
  \item[Short Polling]
  - also called AJAX-based timer, consists of the client repeatedly (timer is used to regulate that) sending the same request to the server (polling), which creates a lot of useless traffic.
  \item[webRTC]
  - Web Real-Time Communication, enables audio, video and text communication between users using web browsers \footnote{\url{http://www.webrtc.org/}}.
  \item[Server-Sent Events]
  - a stream of events generated by the server, to which a client subscribes. Communication is impossible in the other direction\footnote{\url{https://developer.mozilla.org/en-US/docs/Server-sent_events/Using_server-sent_events}}.
\end{description}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Benchmarking}\label{Benchmarking}

\epigraph{"If you're not keeping score, you're just practicing."}{--- Vince Lombardi, American Footbal Player}

As this projects concerns benchmarking different architecture decompositions of The System Under Test \ref{The System Under Test}, it is important to define and describe the benchmarking process.

\begin{description}
  \item[Benchmarking]
  - the term is most commonly defined as an execution of a set of operations against a given object / program, in order to determin it's relative performance. Usually, it consists of a set of standardized tests. The same term is often used to describe benchmarking programs themselves. It is crucial for researchers, tool developers and users \cite{BenRM}.
\end{description}

As James Bornholt describes in his article \cite{BornBen}, computer science research is among the most non-deterministic. A huge risk of omitted-variable bias exists. This is due to the fact that computer systems have a tremendous number of dynamic parts (both in hardware and software). It is virtually impossible for researches and other experts to have complete knowledge of every bit of the system they are working with. Controlling all the involved variables is insurmountable.

Furthermore, many of the system parts factors are non-deterministic; e.g. networks and multi-threading. Because of that, researchers need to be very careful when attempting to draw statistically meaningful conclusions from their research.

Benchmarking a distributed system is even more complex \cite{BornBen}. If it is hosted in a cloud environment, the control over the hardware is in the hands of the cloud provider, not the researcher. This is situation I am in performing this research.

Bornholt \cite{BornBen} lists some of the many environmental factors that can have influence on the performance of a computer system. The most striking example is the \textit{linking order}, which can significantly bias the results of a benchmark. As UNIX systems are highly customizable, it is important to keep the values of environmental variables as consistent as possible across benchmarking sessions.

Author also states a need for the scientific community to unify and stardize the tools for benchmarking systems. A lot of researchers rebuild their infrastructures and build their scripts anew on every project. This is where I hope to contribute to the scientific and open source communities with releasing all the possible parts of the tooling build for the purpose of this project (decribed in details later \ref{Experiment deliverables}).

For a benchmark to be depandable, results need to be reproducible \cite{BornBen}. That means the same results can be obtained reruning the benchmark later on a machine with the same hardware and the same software versions. Reproducibility of experimental results requires measurements to be reliable. A measurement can be called reliable, if the method guarantees accuracy (small systematic and random measurement error, i.e., no bias or “volatile” effects, resp.) and sufficient precision.

Beyer et al \cite{BenRM} list well-established benchmarks for specific tasks:
\begin{description}
  \item[SPECi]
  - The Standard Performance Evaluation Corporation\footnote{\url{https://www.spec.org/}}. They provide benchmarks for CPUs, Graphic Cards, Java Client/Servers, Mail Servers and others.
  \item[TPC]
  - Transaction Processing Performance Council\footnote{\url{http://www.tpc.org/}}. They provide benchmarks for Transaction Processing (OLTP), Decision Support, Virtualization and Big Data.
  \item[nlrpBENCH]
  - Natural Language Requirements Processing\footnote{\url{http://nlrp.ipd.kit.edu/}}. They provide benchmarks for Requirements Engineering.
\end{description}

In case of the system being benchmarked in this research \ref{The System Under Test}, it is hard to reuse any of these since the performance depends on a custom set of operations spreading across multiple system layers.

Authors of "Benchmarking cloud serving systems with YCSB" \cite{Ycsb} point out that it is important to have random distributions for benchmarking loads - \textit{uniform}, \textit{zipfian}, \textit{latest}, \textit{multinomial}, etc.

From my own experience, I have drawn that against a complex system like our utilizing a wide stack of technologies, benchamrking tool should do a few "dry runs" in order to give caches a chance to fill up, internal tables to get populated or cloud comopotents to be warmed up (e.g. Elastic Load Balancers in case of our stack \ref{Cloud architecture setup}).

One also has to be aware of what the potential congestions are. For instance, without properly tuning the operating system's TCP stack, one has to wait a few minutes between load testing runs to let all the TCP sockets return to a usable state (that is exit the TIME\_WAIT state, unless the operating system is tuned to reuse such sockets). Details of the process of tuning are desciber in detail later \ref{Kernel tuning}. In the process of testing, I have also realized our API Server takes a substantial amount of time to handle disconnectis by huge number of clients (test suite disconnects all of them at the same time). That means there needs to be a non-zero pause between load test suite reruns.

\section{Benchmarking Crimes}

Gernot Heiser prepared a list \cite{GerHeiBench} of \textit{benchmarking crimes} which should be avoided when attempting to benchmark a system.

\subsubsection{Selective benchmarking}
\begin{enumerate}
  \item Not covering the full evaluation space.
  \item Not evaluating potential performance degradation.
    \begin{enumerate}
      \item \texttt{Progressive Criterion} - performance actually does improve significantly in area of interest.
      \item \texttt{Conservative Criterion} - performance does not significantly degrade elsewhere.
    \end{enumerate}
    For a benchmark to be reliable, both criterions need to be demonstrated. I trust the models that have been selected to reason about system properties \ref{Selected Meters and Metrics} are living up that task.

    As an economist Mielton Friedman uded to put it, \textit{there's no such thing as free lunch}\footnote{\url{http://www.amazon.com/Theres-Such-Thing-Free-Lunch/dp/087548297X}}.

    In this context, we can iterpret it in the following way. Techniques improving performance in some capacity usually entail an additional cost (extra bookkeeping, caching). As Heiser writes: "This is really at the heart of systems: it's all about picking the right trade-offs" \cite{GerHeiBench}. In our case the potential improved performance comes at a cost of increased data transfers between data centers (and therefore costs) and simply increased complexity of certain system layers.
  \item Subsetting a benchmark without strong justification. A warning sign that one might sin in this way:
    \begin{itemize}
      \item "we picked a representative subset", "typical results are shown" - reads as "we cherry picked the data to fit our expected results".
    \end{itemize}
    If benchmarking is performed on a subset of a system, a strong justification needs to be as to why a given subset of funcionalities / components was selected. I am benchmarking a real-life system use case.
  \item Selective data set hiding deficiencies. Luckily, more and more statistical tools are being developed to detect that \footnote{\url{http://dataskeptic.com/epnotes/ep55_detecting-cheating-in-chess.php}}.
\end{enumerate}

\subsubsection{Using the same dataset for calibration and validation}
The way to avoid this is to calibrate the system first (using calibration workload). Then, one should use evaluation workload to show how accurate the model is. Both workloads need to be disjoint.

\subsubsection{Providing o indication of significance of data}
An example of this fallacy would be providing raw averages, without any indication of variance. At least standard deviations must be quoted. If in doubt, Heiser recommends using student's t-test to check significance \cite{GerHeiBench}.

\subsubsection{Benchmarking of simplified simulated system}
I am performing the tests on the full system deployed in the exact same environment as the production system.

\subsubsection{Using inappropriate and misleading benchmarks}
The tests are being performed on different versions of the proprietary system. The business goal of the host organization of the research is to find the best architecture - the benchmarking proccess is supposed to help with this. There is no incentive to be dishonest.

\subsubsection{Unfair benchmarking of competitors}
The aforementioned applies.

\subsubsection{Providing relative numbers only}
Heiser suggests this reads as "I am covering that the results are really bad or irrelevant" \cite{GerHeiBench}. In the case of this research, some results need to stary relative since this is what the selected scalability measurement meters \ref{Selected Meters and Metrics} require. Absolutes might not make sense in this context.

\subsubsection{Providing no proper baseline}
Often the state-of-the-art solution can be used as a baseline. I believe the solution we picked as a baseline (the simplest architecture decomposition) is a good one. The research leading up to the test did not yield any data to prove that statement wrong.

\subsubsection{Using arithmetic mean for averaging across benchmark scores}
Wallace and Fleming \cite{Fleming} point out that this is an incorrect way to approaching averaging.
The proper way to do this (i.e. arrive at a single figure of merit) is to use the \textit{geometric mean of the normalised scores.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The System Under Test}\label{The System Under Test}

A system under test can be best described as a \textit{real-time, dynamic, parimutuel prediction market}. It is a platform with a publicly exposed RESTful and WebSocket APIs \footnote{\url{http://instamrkt.github.io/slate/}}, through which system clients can connect their websites and mobile applications to access funcionality. The scope of this project concerns the WebSocket API.

The core functionality of the platform is to allow users to make predictions on (near) future events. Users can also ask their own questions regarding these events and send them out globally or to their friends. The events can literally be anything - users' imagination is the limit. A simple use case of the system would be a live football match or an e-game match broadcast on a streaming service like \textit{twitch.tv\footnote{\url{http://www.twitch.tv/}}}. Questions can also be automatically generated based on an incoming stream of events regarding the e-game/sports match. The stream can consist of an "admin" inputting the events manually through the system's admin interface, or, if a game/match has an automatic stream of data available, it can also be connected to the platform.

What is important for this research, is that questions (and predictions users make on them) are organized as parimutuel pools. That means that every prediction in the pool affects the future distribution of winnings. Pennock describes it in details in his work on dynamic parimutuel markets \cite{Pari}.

In order to keep the users informed on what the current distribution is (and thus what are the multipliers for each of the available options), the updated distribution needs to be broadcast as quickly as possible to all interested users (and reach them around the same time). In a sense, every user is therefore both a consumer and a producer of data. In case when users are globally distributed this provides a set of data-distribution and latency related challenges on which I focus in this research.

\section{Sample Use Case}

As mentioned above, one of the simplest use cases of the platform is a live football match. A client, who has connected his application to the platform can now offer users real time questions and placing predictions on them.

A case illustrating the need for the research well would be \textit{UEFA Champions League}\footnote{\url{http://www.uefa.com/uefachampionsleague/}} final, for instance between \textit{Real Madrid} and \textit{FC Barcelona}. These events generate viewership in the range of hundreds of millions \footnote{\url{http://www.uefa.com/uefachampionsleague/news/newsid=2111684.html}}.

Let's assume we have 10 million users connected to the platform throughout the game. Most of them come from Spain and the rest of Europe (65\%). Since the time-zone suits football-loving South Americans well, a lot of users are connected from there too (20\%). The platform also hosts some North American fans (5\%), with the rest comprised of hard-core Asian (9\%) and Australian (1\%) fans.

The company providing a real-time data feed for the game have their servers located in a UK-based data center.

This raises a few interesting questions that are explored within this research:

\begin{enumerate}
  \item Where should WebSocket server instances, which distribute messages, be spun up for minimal user-platform latencies and optimal general system performance?\footnote{Optimal as defined in the Scalability Measurements chapter \ref{Scalability Measurements}.}
  \item How big should the instances be?
  \item What should be the geographic distribution of the instances? How should users be routed to them?
  \item What should be the data-layer architecture? Should the data be centralized or distributed? Distributing using replication, sharding, or some other clustering technique?
\end{enumerate}

\section{Users of the System}

The users have a possibility to connect to the platform from any mobile and desktop device that supports WebSocket communication protocol. That means they can use the platform during live games in stadiums (more and more of which provide good network connectivity), watching sports on TV or e-gaming streams from their couch or in bars and other gatherings with their friends.

That means the platform has to be capable of working efficiently not only on cable and strong wi-fi networks, but also on 4g and 3g networks to facilitate mobile usage. As the experts at AT\&T inform\footnote{\url{http://developer.att.com/application-resource-optimizer/docs/best-practices/multiple-simultaneous-tcp-connections}}, latency is more of an issue or wireless networks than on wired ones. This is because wireless network is more constrained in size and scope than a wired one. The wireless connections there need to be managed more carefully. Most usage is expected to come from mobile networks (which often drop), so reconnecting them quick to the right instance is of huge importance.

Due to the nature of the platform, the demand is extremely lumpy and comes in a few-hour-long spikes for the big events.

\section{Basic Architecture}

The most basic architecture, in its non-distributed form, of the main WebSocket API system components is depicted on the figure \ref{figure:architecture}.

The system consists of the following units:
\begin{description}
  \item[Event Server]
  - receives and processes the external game/match related stream of events. Notifies the WebSocket API server so that it can broadcast the updates to the users.
  \item[Resolver]
  - upon an arrival of a new event, collects all the outstanding pools related to it, resolves accordingly, calculates winnings, updates accounts. Notifies the WebSocket API server so that it can broadcast the updates to the users.
  \item[WebSocket API Server]
  - handles all end-user communication. Receives requests, sends out response and broadcasts game, prediction pool and all other updates. User's subscribe to game-specific channels in order to receive updates.
\end{description}

\begin{figure}[h]
\centering
\includegraphics[scale=0.15]{architecture}
\caption{Basic System Architecture}
\label{figure:architecture}
\end{figure}

\section{Technology Stack} \label{Technology Stack}

The platform is written mostly in Python \texttt{2.7.6}. Data storages used by the platform consist of Redis \texttt{2.8.10} as a caching solution and Mysql \texttt{2.8.6} as persitent storage. A sample client application is written in Ampersand.js\footnote{\url{http://ampersandjs.com/}}. Cloud stack used to which the application is deployed is fully described later \ref{Cloud architecture setup}.

\section{Web Server}

We use nginx\footnote{\url{http://nginx.org/}} as a web server solution, which handles encrypted communicaton and forwards the traffic to the backend servers. If there are multiple backend servers deployed on a single machine, nginx handles the load balancing between them (using a simple round-robin routing policy).

% \subsection{Database Technologies}

% \subsubsection{Mysql Cluster}
% write scalable, important for us, multi master, automatic / manual sharding
% distributed, shared-nothing data that provides very high, scalable performance for applications that largely use primary key access and have high concurrency, but it incurs a cost in network access when accessing data between a MySQL Server and tables distributed across Data Nodes
% In terms of CAP, MySQL Cluster will sacrifice availability to maintain consistency in the event of an unsurvivable network partition.
% at least 3 machines (otherwise split brain problem)
% data nodes (ndbd) - need memory, single-threaded
% sql nodes (mysqld) - need cpu, multithreaded
% Important:
% Will MySQL Cluster “out of the box” run an application faster than my existing MySQL database?
% The reality is probably not without some optimizations to the application and the database. However, by following the guidelines and best practices in the Guide to Optimizing Performance of the MySQL Cluster Database1 , significant performance gains can be realized, especially as you start to scale write operations while maintaining very high levels of availability and low latency.
% [http://openquery.com.au/files/mysql-cluster-intro-use.pdf]

\section{Unique Aspects of The System}

There are multiple unique aspects of the system that make this research interesting:

\begin{itemize}
  \item Critical data is communicated using the WebSocket protocol.
  \item Users receiving data shared locally should receive it at the same time as data shared globally (with as small a latency as possible). This creates a need for globally consistent and manageable latency between end user and the system.
  \item Connected users (who serve both as consumers and producers of data) and sources of data are geographically changing. Users geographical center of mass is changing for each peak of demand.
  \item Demand is extremely lumpy. This creates a need for being able to quickly the system up and down.
  \item New data is generated constantly by the users so caching the content for geographical distribution is difficult.
\end{itemize}

\section{Message Handling and Message Publishing}
HUGE PART OF THE TEST
INSERT CODE SNIPPETS ON PUBLISHING (HOW SELECTION IS DONE BY EACH API SERVER)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiment Outline and Design} \label{Experiment Outline and Design}

\section{Goal of the experiment}

The goal of the project is to design a scalability framework for a real-time persistent websocket distributed application (the system). The core researched topic is whether a systems awareness of clients geographical distribution can improve the system performance according to selected metrics, in comparison with traditional approach \ref{Baseline Architecture}.

The goal of the project is to see if the proposed architecture decompositions \ref{Proposed Architecture I}, \ref{Proposed Architecture II} can perform better (quantitatively, according to the selected meters) than the baseline architecture in serving geographically dispersed clients. Approaches used to scale simple http applications cannot always be translated to websocket applications since the communication protocol differs. Websockets put a different kind of strain on the server machines since these need to keep the connection opened on a port for a prolonged period of time rather than simply open, server and close (as is the case with http). Along the way, an answer needs to be found what level of decoupling provides best performance on each layer of a stateless persistent system. One of the properties of a system of that kind is that key value stores come under heavy load since this is where the state resides. A good solution for distributing (sharding / replicating) these also needs to be found. Same goes for persistent storage.

\section{Problem To Be Solved}

from the company perspective:
  - has to prepare the platform for scalability
  - no clear answers on what the best architecture decomposition is
  - no tools to measure and reason about different approaches 

\section{Question to be answered}
[TODO] same as in the introduction. You can be a bit more specific here now.

\section{Research Method}
[TODO] Controlled experiment - implementation described in detail in the Experiment Implementation Chapter \ref{Experiment Implementation}.

\section{Experiment Outline}

\subsection{Pre-experiment preparations}
- Preparing a distributed metric collection suite (Both for servers and clients)
- Preparing a distributed load testing suite
    - simluates client from different regions with a specified user and message load

\subsection{Load Testing}

Clients will come from all zones

[TODO] explain the whole process.

1. Remote execution start.
2. Remote execution stop.
3. Copying the result files locally.
4. Collecting metrics from these files.

\subsection{Users Distribution}
The authors suggest choosing randomly when generating load as to which operation to perform, on what data size etc. They suggest using different random distributions: uniform, zipfian, latest, multinomial\cite{Ycsb}. (as mentioned in bemncharking 2.2 section)

\subsection{Used Benchmarking Scenario}\label{Used Benchmarking Scenario}

The load is obviously the same for each testable scenario, we use 8 regions, x clients from each, connecting over 15 minutes in equal one second steps and staying connected for 90 minutes.

X users loaded over 15 minutes, x / 15*60 every second.
Clients stay on connected for 90 minutes and then disconnect.
they join a game and load prediction pools within (0,15)seconds connecting
each client places a prediction every (0,30) seconds
the prediction updates pool distribution and this is then broadcast to all users (in 1-second batches)

ADD NUMBER TO HOW MANY GAME AND HOW MANY PREDICTIONS

\subsubsection{Why is this benchmark representative?}

It's the expected load during football game (the system has not been run at this scale yet - that's partly why we are doing this project).

User's load is representative of that - user's connect over a 15 minute period before the game starts, then stay connected over 90 minute game load and then disconnect once the game is finished.

[TODO] - how long exactly is the whole thing?

\section{Compared Architectures}\label{Compared Architectures}

Here's just the diagram overview, implementation details can be found in the following chapter \ref{Compared Architectures Implementation Details}.

\subsection{Baseline Architecture}\label{Baseline Architecture}
ONE MASSIVE INSTANCE WITH WEBSOCKET SERVER + REDIS + MYSQL

ONLY DIAGRAM HERE THE DETAILS IN THE FOLLOWING CHAPTER \ref{Baseline Architecture Implementation Details}.

\subsection{Proposed Architecture I}\label{Proposed Architecture I}
Region-distributed API servers

\ref{Baseline Architecture Implementation Details}

\subsection{Proposed Architecture II}\label{Proposed Architecture II}
Region-distributed API servers + redis cluster + MySQL Read Replicas

\ref{Baseline Architecture Implementation Details}

\section{Results Evaluation}
Results are evaluated using literature - based scalability models \cite{Models}
models and meters were selected \ref{Selected Meters and Metrics} among them to be able to reason about factors most important to the host organization - client latencies, cloud resource utillizaation and operating cost. The tested architectures are compared using these quantifiable meters. LIST THESE METERS - CRUM, SPM, SLM AND THE DERIVATIVES.

\section{Experiment deliverables}\label{Experiment deliverables}
open sourced code for:
using scalability models
managing the cloud (built on boto)
remote execution of load test and metric collection

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiment Implementation} \label{Experiment Implementation}

\epigraph{"At scale, everything breaks."}{--- Urs Holzle, Google Fellow}

\section{Baseline Architecture Preparations}\label{Baseline Architecture Preparations}
to get meaningful results we had to do slight optimizatoins of our application code - that proved timely

code snippets of publishing code, message batching

identifying resource bottlenecks - CPU, both for clients generating load (single core node process) and servers. Split up into processes utilizing all the cores.

2 core threads - one for message handling, another one for pushing redis-published messages through the the end websockets (TCP is 1-1 communication, cannot perform a simulatenous broadcast onto all of them); we can run.

\subsection{Kernel tuning}\label{Kernel tuning}

\subsubsection{Measurements}
etsy, logster, graphite etc.

syncling clocks on machines usint /etc/ntpd (polled from amazon servers)

\subsubsection{Linux TCP stack}

http://serverfault.com/questions/48717/practical-maximum-open-file-descriptors-ulimit-n-for-a-high-volume-system
the number of client connections that a server can support has nothing to do with ports in this scenario, since the server is [typically] only listening for WS/WSS connections on one single port. I think what the other commenters meant to refer to were file descriptors. You can set the maximum number of file descriptors quite high, but then you have to watch out for socket buffer sizes adding up for each open TCP/IP socket.  MAKE SURE THIS IS CONSISTENT THROUGHOUT THE PAPER.
If the file descriptors are tcp sockets, etc, then you risk using up a large amount of memory for the socket buffers and other kernel objects; this memory is not going to be swappable.

[http://stackoverflow.com/questions/4852702/do-html-websockets-maintain-an-open-connection-for-each-client-does-this-scale/25340220\#25340220]
Each TCP connection in itself consumes very little in terms server resources. Often setting up the connection can be expensive but maintaining an idle connection it is almost free. The first limitation that is usually encountered is the maximum number of file descriptors (sockets consume file descriptors) that can be open simultaneously. This often defaults to 1024 but can easily be configured higher.

all lab notes from 08.05
all websocket updates
http://serverfault.com/questions/48717/practical-maximum-open-file-descriptors-ulimit-n-for-a-high-volume-system
Also, and very important, you may need to check if your application has a memory/file descriptor leak. Use lsof to see all it has open to see if they are valid or not. Don't try to change your system to work around applications bugs.

Although the causes for such symptoms can vary, there's one scenario that can cause a complete lock of systems handling a very large number of web requests per second without any hint of what's going on: TCP/IP port exhaustion. [https://www.outsystems.com/forums/discussion/6956/how-to-tune-the-tcp-ip-stack-for-high-volume-of-web-requests/]

describe close\_wait, time\_wait etc

only 65k per (tcp 16 bit) ip address, one can add virtual ips to machines and up the number this way
(recent lab notes)

increasing the number of clients
The solution is more quadruplets5. This can be done in several ways (in the order of difficulty to setup):
use more client ports by setting net.ipv4.ip\_local\_port\_range to a wider range,
use more server ports by asking the web server to listen to several additional ports (81, 82, 83, …),
use more client IP by configuring additional IP on the load balancer and use them in a round-robin fashion,
use more server IP by configuring additional IP on the web server6.

But there are ways to tune the TCP/IP stack to reduce the impact of this problem, allowing the system to take advantage of all resources at its disposal. Basically, we can tune several TCP/IP stack parameters, but in this context, there are 2 that really make the different:
The time that the ports is in "waiting" status since it was released from the application, and it's in fact released by the system for reuse. It's called the TIME WAIT
The maximum number of ephemeral ports that the system can use, from the total pool of 65535 available ports. Let's just call it RANGE EPHEMERAL PORTS
Reduce the TIME\_WAIT by setting the tcp\_fin\_timeout kernel value on /proc/sys/net/ipv4/tcp\_fin\_timeout, using the command echo 30 > /proc/sys/net/ipv4/tcp\_fin\_timeout to set it to 30 seconds.

Increase the range of ephemeral ports by setting ip\_local\_port\_range kernel value on /proc/sys/net/ipv4/ip\_local\_port\_range, using the command echo "32768 65535" > /proc/sys/net/ipv4/ip\_local\_port\_range, this will set the port range from 32768 to 65535.

A common misunderstanding is that a server cannot accept more than 65,536 (216) TCP sockets because TCP ports are 16-bit integer numbers.
First, the number of ports is limited to 65,536, but this limitation applies only to a single IP address. Supposing that we are limited by the number of ports to have more than 65,536 clients, then adding more IP addresses to the server machine (either by adding new network cards, or simply by using IP aliasing for the existing network card) would solve the problem (even if, for opening 12 million client would need 184 network cards or IP aliases on the server machine).
In fact, the misunderstanding comes from the fact that the server does not use its listening IP address and a different ephemeral port for each new socket to distinguish among the sockets, but it uses the same listening IP address and the same listening port for all sockets and it distinguishes among sockets by using the IP address and the ephemeral port of each client. Therefore, MigratoryData Server uses a single port to accept any number of clients and optionally it uses another few ports for JMX monitoring, HTTP monitoring, etc
https://mrotaru.wordpress.com/category/websockets/

max descriptors per process:
Because one cannot increase the maximum number of socket descriptors per process to a value larger than the current kernel maximum (fs.nr\_open) and because the kernel maximum defaults to 1048576 (10242), prior to running the ulimit command, we increased the kernel maximum accordingly as follows:
echo 20000500 > /proc/sys/fs/nr\_open
CHECK LAB NOTES RECENT TESTS, A LOT
https://mrotaru.wordpress.com/category/websockets/

The kernel value parameters aren't saved with these commands, and are reset to the default values on system reboot, thus make sure to place the commands on a system startup script such as /etc/rc.local.
https://www.outsystems.com/forums/discussion/6956/how-to-tune-the-tcp-ip-stack-for-high-volume-of-web-requests/

look for \_what can limit concurrent sockets?\_ in lab notes

“The number of connections wstest can open on a server is limited by the number of ephemeral ports on the machine on the outgoing interface / IP. Something like 64k at most. If you need to test the server with more connections, currently you will need to run multiple instances of wstest (on different machines).” from: http://autobahn.ws/testsuite/usage.html\#mode-massconnect

Understanding tcp sockets:
sockets are left in TIME\_WAIT on the server when the client suite is killed which makes sense
when client exits gracefully no sockets left in TIME\_WAIT / CLOSE\_WAIT by the server
on\_close() from tornado only kicks in on client close
.onclose in js only kicks in when server initiates through self.close()
when server terminates through self.close() sockets ends in TIME\_WAIT for a minute (also makes sense according to docs)
describe how you can influence that (reusing web sockets + delay time )

ulimit, nofile, fs.max\-file, net.ipv4.ip\_local\_port\_range = 32768 65535  to /etc/sysctl.conf for ip ranges on the clients

What we used to cross-test our tools and make sure we're collecting reasonable values.

Basic metrics
\begin{itemize}
  \item cpu, memory, disk usage (pidstat / CloudWatch)
  \item network i/o (wireshark, list others analyzed)
  \item latency, throughput, concurrent connections, messages dropped?
\end{itemize}

\subsection{Nginx Optimizations}
added round-robin routing for multiple backend instances

worker\_processes auto; (x for x cores)
worker\_connections 165535 (set to ulimit-n)
tcp\_nopush off ((nagle’s algorithm)
keepalive\_timeout 1000;
proxy\_read\_timeout 1000;
listen 443 backlog=16384 deferred;
worker\_rlimit\_nofile 30000;

\subsection{Mysql Optimizations}
Mysql would periodically regularly stall for a very long time.

innodb\_flush\_log\_at\_trx\_commit (1=>2)
http://stackoverflow.com/questions/3722510/why-does-an-insert-query-occasionally-take-so-long-to-complete
innodb\_flush\_method to 0 O\_DSYNC

Since on big machines we are running up to instance of the api serever process, we were maxing out on default mysql max number of connections; flipped it to 500; solved

\subsection{Redis Optimizations}
[1772] 24 Jul 10:09:09.009 \# Can't save in background: fork: Cannot allocate memory
[1772] 24 Jul 10:09:15.020 * 1 changes in 900 seconds. Saving...

ResponseError: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.

Explain overommiting here shortly (redis.io/topics/faq)

sysctl: vm.overcommit\_memory = 1

LAB NOTES FROM 24.07

\subsection{Cloud baseline preparations}

make sure to describe client simulation in details and issues that you battled with there - like cpu maxing, having to divide into multiple processes etc.

client machines reaching different resource limits at different loads, becuase of the increased publishing?

started with t1.micro - had to move to m1.small, because we were reaching memory limits on thest that run longer than ~3 minutes, kernel was silently killing the process and metrics would fail to get reported from the clients

Jul 24 11:36:28 ip-172-31-24-5 kernel: [39921901.888692] Out of memory: Kill process 2992 (node) score 673 or sacrifice hild"

super hard to estimate the necessary client resource, which we had to over-provision anyway - clients reaching hardware limits affects reported measurements and renders the test run useless.

run previously on smaller scale (to save costs)

Adjust the ports opened for cloud instances - redis publishing (communication going the other way round)

setting up custom cloudwatch metric collection - \ref{Custom Metric Collection}. what we thought would work didn't.

running as cronjob
had to dig into perl script which cached the instance id - so all server instances were reporting metrics for the first instance the scripts were deployed on. (verified from the EC2 instance using the command wget http://169.254.169.254/latest/meta-data/instance-id),
had to update our node scripts for collecting data pushed through and received on the sockets - the node library we use [REF] had a different internal representation so had to dig in to report data properly.

\subsubsection{Image Preparations}
Both servers and clients are spun up using AMIs (describe what they are [amazon source]).

Both needed to be meticulousy prepared to ensure smooth dpeloyment of the test suite and tested architecutres.

The client has been prepared with the right environment so that the test suite master can ssh into it (using node sequest) and run the test command [put it here].

The server has all the necessary services that need to be automatically started up:
- /etc/init.d starts redis (if applicable)
- /etc/init.d starts mysql (if applicable)
- /etc/init.d starts supervisord
    - supervisord conf has the processes that need to be started (with an autostart option) [PASTE sample api server config here]
- api server configurations has the correct host names for data stores - redis and mysql

\subsubsection{Distributing Resources Across Cloud Regions}

Aws stack is divided into Regions (e.g. Ireland, N. Virgina, Sao Paolo) and these into availability zones. All the service stack you set up for a region. No easy way to copy between regions - businesses built around that.

[TODO]:

copy code snippets for deploying client groups and orchestrating them 

and servers

\subsection{Exluding Frankfurt Region}

Had to be excluded, because the AMIs we use are not compatible with the region's infrastructure. We are using ParaVirtual images, whereas Frankfurt supports HVM [describe with sources].

\subsubsection{Observations}

splitting up redis and mysql onto separate instances causes CPUs to max out 100\% (handling the networks stack?) and data traffic to spike (obvsly)

backup by table from lab notes 21.07

\section{Load Testing Framework}\label{Load Testing Framework}
all the analyzed tools, list also in lab notes from may 11 [FOR EACH WHY WASN'T SATISFACTORY]
jmeter, thor (low extensibility which we needed), autobahn (max available connections but no further control),
gatling-websocket, http://www.opensourcetesting.org/performance.php
tsung - looked promising, highly scalable erlang, but hard to understand what was going on, community was not active enough, wsbench


\subsection{Benchmark Run Preparations}

Experiment Setup:
Cleanup Redis
Flushall + reboot instance
Cleanup Mysql
Drop im; reboot, source user creation
Reboot all servers to clean state


\section{Compared Architectures Implementation Details}\label{Compared Architectures Implementation Details}

selection - similar computing allocated resources [lab notes table]

instance selection limited by our image type, not all suported (our image paravirtual, some require newer hardware virtual machine HVM) [https://www.opswat.com/blog/aws-2015-why-you-need-switch-pv-hvm]

[TODO] describe selected instance types

we picked CPU intensive cause by far that's the first hardware limit we hit

\subsection{Baseline Architecture Implementation Details}\label{Baseline Architecture Implementation Details}
ONE MASSIVE INSTANCE WITH WEBSOCKET SERVER + REDIS + MYSQL
NO NETWORK OVERHEAD
~12/13 SERVER PROCESSES

c3.8xlarge instance, 32 CORES, 60GB, 140 GB SSD Storage with 4000 IOPS

\subsection{Proposed Architecture I Implementation Details}\label{Proposed Architecture I Implementation Details}
Region-distributed API servers
1 API-server instance per one of 8 regions (m3.xlarge, 4cores, 15gb RAM, 2x40SSD)
2 processes per instance

1 big redis instance (m2.4xlarge, 8 cores, 86.4 GB mem, 2 x 840 GB storage)
1 big mysql RDS instance (r3.4xlarge, 16 cores, 122 GB mem)

\subsection{Proposed Architecture II Implementation Details}\label{Proposed Architecture II Implementation Details}
Region-distributed API servers + redis cluster + MySQL Read Replicas

1 API-server instance per one of 8 regions (m3.2xlarge, 4cores, 30gb RAM, 2x40SSD)
2 processes per instance

Redis cluster - each node sitting on the same node as API server.

Mysql Read Replica nodes in each region - db.r3.large (2 cores, 15 GB)
Writes still go to the master in main region.

\section{Cloud architecture setup} \label{Cloud architecture setup}

selected aws

multiple availability zones very important in a shared cloud environment - often because another customer is getting hit by a DDoS.\cite{GroBuy}

In Amazon’s environment, Grozev and Buyya suggest using Elasticache service \cite{GroBuy}. We cannot use this, also described in \ref{Cloud architecture setup}.

[TODO] insert diagram
Route53 -> Elb -> Nginx -> Api Server instance

\subsection{Route53}
  dns mapped to inscance ips / load balancer dns names
  lbr, geo, weighed rr
  their internal heuristics take network conditions from past weeks [VERIFY], rather amibiguous
  For each end user’s location, Route 53 will return the most specific Geo DNS record that includes that location. In other words, for a given end user’s location, Route 53 will first return a state record; if no state record is found, Route 53 will return a country record; if no country record is found, Route 53 will return a continent record; and finally, if no continent record is found, Route 53 will return the global record.

  health checks - can be used custom-defined, but for us we can use load balancing health checks (will only route if healthy incstances behind it) [http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html]

  http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Tutorials.html
\subsection{Autoscaling}
  instances behind each groups. automatically scalable, connected to external redis and mysql instances.
  only within one region, multiple availability zones
  http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/GettingStartedTutorial.html
  http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/how-as-works.html\#arch-AutoScalingMultiAZ
  Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group. Auto Scaling does this by attempting to launch new instances in the Availability Zone with the fewest instances.
  alarm - object that watches over a single metric (e.g. avg cpu use of ec2 instances in auto scaling group over specified time period) (OK, ALARM, INSSUFICIENT\_DATA), can trigger scaling up / down policy on an autoscaling group
  policy variants (change ExactCapacity, ChangeInCapacity, PercentChangeInCapacity)
  recommendation one policy for scaling out, another policy for scaling in
  http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/as-scale-based-on-demand.html
  http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/AlarmThatSendsEmail.html
  http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/US\_AlarmAtThresholdEC2.html
  \subsubsection{Alarms}
  avg / min / max / sum / sample count (for whole group)
  of cpu / disk io / network io
  is >= / > / < / <= than x %
  for at least X consecutive periods of 5 min / 15 min / 1 hour / 6 hours
  \subsubsection{Scaling Policies}
  execute policy when alarm
  add / remove / set to   X   instances / % of group
  and then wait Y (cooldown period)

  \subsubsection{Launch configurations}

\subsection{Load balancing}
  elb uses dns name to server a pool of lb instances in the backend (you might exceed one instance connection limit), that's why they are not provided a static ip
  routing: request count-based for http(s), for others (tcp which is a workaround for websockets) they use tcp [http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/how-elb-works.html\#request-routing]
  The client uses DNS round robin to determine which IP address to use to send the request to the load balancer. No control over routing without load balancing groups.
  For example, if you have ten instances in Availability Zone us-west-2a and two instances in us-west-2b, the traffic is equally distributed between the two Availability Zones. As a result, the two instances in us-west-2b serve the same amount of traffic as the ten instances in us-west-2a. Instead, you should distribute your instances so that you have six instances in each Availability Zone.
  To distribute traffic evenly across all back-end instances, regardless of the Availability Zone, enable cross-zone load balancing on your load balancer. However, we still recommend that you maintain approximately equivalent numbers of instances in each Availability Zone for higher fault tolerance.
  [http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/how-elb-works.html\#request-routing]
  only ~64k connections possible per ELB instance TODO: VERIFY THAT
  stickiness only works for HTTP/HTTPS protocols
  Once you have a testing tool in place, you will need to define the growth in the load. We recommend that you increase the load at a rate of no more than 50 percent every five minutes. B.oth step patterns and linear patterns for load generation should work well with Elastic Load Balancing. If you are going to use a random load generator, then it is important that you set the ceiling for the spikes so that they do not go above the load that Elastic Load Balancing will handle until it scales (see Pre-Warming the ELB).
  [https://aws.amazon.com/articles/1636185810492479\#pre-warming]

\subsection{Data Layer}

Technologies we used described in \ref{Technology Stack}. We used Mysql and redis as a caching solution.

\subsubsection{Amazon Relational Database Service}
Does not support mysql cluster as of July 2015. Support coming but not released yet.
Offers mysql read replicas. With a single master. That obviously scales only reads
You can create a MySQL Read Replica in a different region than the source DB instance to improve your disaster recovery capabilities, scale read operations into a region closer to end users, or make it easier to migrate from a data center in one region to a data center in another region.

\subsubsection{Elasticache}
doesn't support crossregion (you cannot connect to an elasticache cluster from outisde of the region)

The above explain why, for a scaled out solution we had to roll with our custom setup. We were suggested using technologies like DynamoDB and Kinesis, but out of scope.

\section{System Scaling Delay}
Delays: up to 2 minutes to get metrics, up to ????? minutes for instance to be accessible to clients + ???? for DNS changes to propagate (TTL set to 60 seconds but TODO VERITYF)
Unfortunately there are a large number of (misbehaving) DNS servers out there that don’t properly obey TTLs on records and will still serve up stale records for an indefinite amount of time. [http://engineering.chartbeat.com/2014/01/02/part-1-lessons-learned-tuning-tcp-and-nginx-in-ec2/]

DNS PROTOCOL INHERENT LIMITATIONS PREVENTED US FROM TESTING HOW FAST AND DOWN ARCHITECTURE SCALES

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Scalability Measurements} \label{Scalability Measurements}
In this chapter I describe most relevant models and metrics from scientific literature and the subsets I decided to use in this research \ref{Selected Meters and Metrics}.

\section{Scalability Meters}
Models I find most useful were proposed by Pattabiraman et al \cite{Models}. As mentioned in Related Work section \ref{Related Work}, they divide the performance indicators into three groups and propose formal quantifiable meters for each of them:
\begin{enumerate}
  \item computing resource indicators (CPU, disk, memory, networks, etc.),
  \begin{itemize}
    \item Computing Resource Allocation Meter (CRAM),
    \item Computing Resource Utilization Meter (CRUM),
  \end{itemize}
  \item workload indicators (connected users, throughput and latency, etc.),
  \begin{itemize}
    \item System Load Meter (SLM),
  \end{itemize}
  \item performance indicators (processing speed, system reliability, availability)
  \begin{itemize}
    \item System Performance Meter (SPM).
  \end{itemize}
\end{enumerate}
They also propose model which allow to combine the above: System Capacity Meter (SCM), System Effective Capacity Meter (SEC), Effective System Scalability (ESS), and Effective Scalable Range (ESR).

\subsection{Scalability Meters}

The geometric interpretation of each of these meters is a polygon. The quantifiable value of the meter is the area of the polygon. Sample meter is plotted on the figure \ref{figure:polygon}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{angleradius}
\caption{Polygon's radius and central angle}
\label{figure:polygon1}
\end{figure}

The value of each metric plugged into the meter is represented by the radial distance between the center of the polygon and the vertex. Since all the metrics that serve as inputs to the meter have equal weights, the central angle (as shown of figure \ref{figure:polygon1}) between two consecutive radial distances is always going to be equal. For \textit{$n$} metrics in a meter, each central angle will have a value of \textit{$2\Pi/n$}.

The area of a polygon can be computed as a sum of areas of the triangles that constitute it. The sides of each of these triangles consist of the side of the polygon and 2 radii.

Area of a triangle can be computed using the formula:
\begin{center}
  $$A_t = 0.5 * a*b * \sin(\alpha_{ab}) $$
  where \textit{$a$} and \textit{$b$} are to sides of the triangle and $\alpha_{ab}$ is the angle between them.
\end{center}

In case of these meters, $\alpha_{ab}$ is always equal to $2\Pi/n$. The area of a triangle limited by two radii with values of two metrics $x_i$ and $x_{i+1}$ would look like this:
\begin{center}
  $$A_t = 0.5 * x_i*x_{i+1} * \sin(2\Pi/n) $$
\end{center}

To compute the area of the polygon one needs to sum all the areas of the triangles formed by radii, and thus the formula is the following:
\begin{center}
  \texttt{$$A_p = 0.5 * \sin(2\Pi/n) * \sum_{i=1}^{n}x_i*x_{i+1}$$}
  where \textit{$x_i$} is a value of an input metric and \textit{n} is the count of input metrics.
\end{center}

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{polygon}
\caption{Sample Meter Geometric Interpretation (SPM)}
\label{figure:polygon}
\end{figure}

\subsubsection{Computing Resource Allocation Meter (CRAM)}
\begin{center}
  $$CRAM(S,t) = 0.5 * \sin(2\Pi/n) * \sum_{i=1}^{n}a_i*a_{i+1}$$
\end{center}
  \texttt{$S$} - represents the measured system, \\*
  \texttt{$t$} - represents the time over which total resource allocation for the system is evaluated, \\*
  \texttt{$a_i$} - represents the values of different types of \textit{allocated} computing resources to S by a cloud (network traffic boundary, cache allocated, memory allocated, cpu allocated, etc.), \\*
  \texttt{$n$} - represents the count of different types of allocated computing resources to S by a cloud.

\subsubsection{Computing Resource Utilization Meter (CRUM)}
\begin{center}
  $$CRUM(S,t) = 0.5 * \sin(2\Pi/n) * \sum_{i=1}^{n}u_i*u_{i+1}$$
\end{center}
  \texttt{$S$} - represents the measured system, \\*
  \texttt{$t$} - represents the time over which total resource allocation for the system is evaluated, \\*
  \texttt{$u_i$} - represents the values of different types of computing resources \textit{utilized} by S in a cloud (network traffic boundary, cache allocated, memory allocated, cpu allocated, etc.), \\*
  \texttt{$n$} - represents the count of different types of computing resources utilized by S in a cloud.

\subsubsection{System Performance Meter (SPM)}
\begin{center}
  $$SPM(S,t) = 0.5 * \sin(2\Pi/n) * \sum_{i=1}^{n}p_i*p_{i+1}$$
\end{center}
  \texttt{$S$} - represents the measured system, \\*
  \texttt{$t$} - represents the time over which system performance is evaluated, \\*
  \texttt{$p_i$} - represents the values of different performance metrics of the S in a cloud  (throughput ratio, reliability, availability, utilization, response time, etc.), \\*
  \texttt{$n$} - represents the count of different performance metrics measured in S.

\subsubsection{System Load Meter (SLM)}
\begin{center}
  $$SCM(S,t) = 0.5 * \sin(2\Pi/3) * [CTL(t)*UAL(t)+UAL(t)*SDL(t)+CTL(t)*CDL(t)]$$
\end{center}
  \texttt{$S$} - represents the measured system, \\*
  \texttt{$t$} - represents the time over which load for the system is evaluated, \\*
  \texttt{$CTL(t)$} - represents system communication traffic load, \\*
  \texttt{$UAL(t)$} - represents system user access load, \\*
  \texttt{$SDL(t)$} - represents system data load. \\*

\subsubsection{System Capacity Meter (SCM)}
\begin{center}
  $$SCM(S,t) = 0.5 * \sin(2\Pi/3) * [SL(t)*SP(t)+SL(t)*AR(t)+SP(t)*AR(t)]$$
\end{center}
  \texttt{$S$} - represents the measured system, \\*
  \texttt{$t$} - represents the time over which system capacity is evaluated, \\*
  \texttt{$SL(t)$} - represents system load based on SLM, \\*
  \texttt{$SP(t)$} - represents system performance based on SPM, \\*
  \texttt{$AR(t)$} - represents system resource allocation based on CRAM. \\*

\subsubsection{System Effective Capacity Meter (SEC)}
\begin{center}
  $$SEC(S,t) = 0.5 * \sin(2\Pi/3) * [SL(t)*SP(t)+SL(t)*RU(t)+SP(t)*RU(t)]$$
\end{center}
  \texttt{$S$} - represents the measured system, \\*
  \texttt{$t$} - represents the time over which load for the system is evaluated, \\*
  \texttt{$SL(t)$} - represents system load based on SLM, \\*
  \texttt{$SP(t)$} - represents system performance based on SPM, \\*
  \texttt{$RU(t)$} - represents system resource utilization based on CRUM. \\*

Since all the meters above use the values sampled over a period of time as input, for each of the mentioned meters one can talk about \textit{min} and \textit{max} values. This is useful for the last two meters.

\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{sec}
\caption{Sample System Effective Capacity Meter (SEC)}
\label{figure:sec}
\end{figure}

\subsubsection{Effective Scalable Range (ESR)}
\begin{center}
  $$ESS = [SL(t) / SL_{min}] / [SEC(t) / SEC_{min}]$$
\end{center}
  \texttt{$t$} - represents the time over which the system is evaluated, \\*
  \texttt{$SL(t)$} - represents system load based on SLM, \\*
  \texttt{$SEC(t)$} - represents system effective capacity based on SEC. \\*

\subsubsection{Effective System Scalability (ESS)}
\begin{center}
  $$ESR = (SEC_{max} - SEC_{min})$$
\end{center}
  \texttt{$SEC$} - represents \textit{max} and \textit{min} system effective capacity based on SEC. \\*

Using the last two models (ESR and ESS), one can compare scalability of two systems/architectures in absolute quantifiable terms.

The meters I decided to use in this research are described below \ref{Selected Meters and Metrics}.

\subsection{Other Models}
There is much more work related to the measuring scalability of distributed systems. Srinivas and Janakiram in their work \cite{SriJan} mention similar metric evaluating scalability as a product of throughput and response time (or any value function) divided by the cost factor.

They propose a model considering scalability as a function of synchronization, consistency, availability, workload and faultload. It aims on identifying bottlenecks and hence improving the scalability. The authors also emphasize the fact of interconnectedness of synchronization, consistency and availability:
\begin{center}
  $$scalability = f(availability, synchronization, consistency ,workload, faultload)$$ \\*
\end{center}

Jogalekar and Woodside \cite{EvaScaDS} propose a strategy-based scalability metric based on productivity and cost effectiveness (a function of system's throughput and it’s Quality of Service). It separates evaluation of throughput or quantity of work from Quality of Service (which, according to the authors, can be any suitable expression). Productivity can be computed as follows:
\begin{center}
  $$F(k) = \lambda(k) * f(k) / C(k)$$ \\*
\end{center}
  \texttt{$F(k)$} - represents the productivity of system at scale k, \\*
  \texttt{$\lambda(k)$} - represents throughput in responses/s of the system at scale k, \\*
  \texttt{$f(k)$} - represents the average value of a responses of the system at scale k, calculated from its QoS, \\*
  \texttt{$f(k)$} - represents the running cost per second of the system at scale k. \\*

Based on that, one can compute scalability metric relating systems at two different scale factors as the ratio of their productivity figures:
\begin{center}
  $$\upsilon(k_1,k_2) = (F(k_1)/F(k_2)) $$\\*
\end{center}

\section{AWS Available metrics} \label{AWS Available metrics}

Since the System Under Test \ref{The System Under Test} is deployed to Amazon's Cloud \ref{Cloud architecture setup}, the metrics of the system need to be collected as a combination of metrics offered by Amazon's measurement service (CloudWatch\footnote{\url{https://aws.amazon.com/cloudwatch/}} and our custom collection process (described here \ref{Custom Metric Collection}).

Metrics from CloudWatch are collectible by default every 5 minutes. They offer a paid \textit{detailed mode}, with which one can collect metrics every minute. From what I have experienced, even in the detailed mode, the requested metrics show up in the CloudWatch API results with a delay as high as 2 minutes.

The CloudWatch API has its limitations - for instance, the allocation and utilization of memory for EC2 instances and processes is not available. This is something we are collecting in our custom metric collection process \ref{Custom Metric Collection}.

As Pattabhiraman et al. \cite{Models} warn, there exist additional costs which are not apparent from the start.
If one exceeds 10 metrics, 10 alarms, 1,000,000 million requests, 1000 SNS email notifications, 5GB of incoming data logs or 5GB of data archiving per month then additional costs accrue. This is included in the cost estimates.

CloudWatch Alarms \footnote{\url{http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch.html}} can be configured based on these metrics. They can be applied to trigger AutoScaling actions that scale the deployed system in or out automatically.

A very important limitation for the case of this research is that data cannot be aggregated automatically across AWS regions \footnote{\url{http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/cloudwatch_concepts.html}}.

For each metric collected for a given time window (1 or 5 minutes), CloudWatch offers the following statistics: min, max, average, sum and sample count (count (number) of data points used for the statistical calculation).

Metrics can be aggregated over different domains - instances, AutoScaling groups, Elastic Load Balancers (all instances behind it) or availability zones. Metrics for groups of instances differ from metrics for single instances.

\subsection{Instance Specific}
The following metrics can be collected on an EC2 instance level:
\begin{itemize}
  \item CPU utilization (\% and absolute),
  \item Network in/out (in bytes),
  \item Disk read/write (number of operations and bytes),
  \item Custom user-defined metrics,
  \item Instance status checks data.
\end{itemize}

\subsection{Cloud (group) Specific}
Collected from Elastic Load Balancer for all instances connected.

\subsection{Load Test Metric Collection}
http://stackoverflow.com/questions/3871886/ssl-and-load-balancing

elb uses dns name to server a pool of lb instances in the backend (you might exceed one instance connection limit)
in order for the full range of ELB machine IP addresses to be utilized, “make sure each [client] refreshes their DNS resolution results every few minutes.”
One test client equals one public IP address. ELB machines seem to route all traffic from a single IP address to the same back-end instance, so if you run more than one test client process behind a single public IP address, ELB regards these as a single client.
Use 12 test clients for every availability zone you have enabled on the ELB. These test clients do not need to be in different availability zones – they do not even need to be in EC2 (although it is quite attractive to use EC2 instances for test clients). If you have configured your ELB to balance among two availability zones then you should use 24 test clients.
Each test client should gradually ramp up its load over the course of a few hours. Each client can begin at (for example) one connection per second, and increase its rate of connections-per-second every X minutes until the load reaches your desired target after a few hours. - WE CANNOT DO THAT

[TODO] describe the process here - latency, handhskaing, all agreggarion, results snippet
explain the difference between how broadcast latency is aggregated vs request handling

Tools described here \ref{Load Testing Framework}

\section{Custom Metric Collection} \label{Custom Metric Collection}

locally:
pidstat
latencies by our custom client suite
for network we looked ad ntop, nethogs, nload, vnstat, wireshark but amazon provides this data.

ss -s
netstat -s

plugged into statsd and graphite for real time monitoring

THE ABOVE DOESN'T WORK ON AMAZON LINUX.

perl scripts!!!!
http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html
end up in cloudwatch


\subsection{Real Time Metric Insights} \label{Real Time Metric Insights}
statsd, graphite
photo of our dashboard

\section{Selected Meters and Metrics}\label{Selected Meters and Metrics}
Computing Resource Utilization Meter
Server CPU utilization, compiled over all server instances + RDS DB where applicable (Cloudwatch)
Memory utilization, compiled over all server instances (Cloudwatch Custom)
Disk utilization, compiled over all server instances (Cloudwatch)

System Performance Meter
Handshaking latencies (Our suite)
Request Handling latencies (Our suite)
Broadcast latencies (Our suite)
% Mysql Read Latency (Cloudwatch)
% Mysql Write Latency (Cloudwatch)
Dropped Connections (Our suite)
Total Network In (Our suite / cloudwatch)
Total Network Out (Our suite / cloudwatch) - include internal, billable transfer somehow

System Load Meter
Concurrent users  (Our suite)
Data sent + data received  (Our suite)
Handled requests count (Our suite)
Broadcast messages count (Our suite)
Redis Hits (info | keyspace\_hits)
Mysql queries select * from information\_schema.global\_status s1 where variable\_name = \"QUERIES\";
% Mysql Read IOPS
% Mysql Write IOPS

System Effective Capacity Meter
SLM + SPM + CRUM + 1 / (Computed Cost of running the architecture per unit of time)

That's an update as to the proposed model.

We need a valuable that's inversely proportional to the cost value (e.g. how long you can run on x \$)

Effective Scalable Range and Effective System Scalability
$SEC_max$ + $SEC_min$ + $SLM_max$ + $SLM_min$

The benchmark is run against each of the deployed architecture setups and the resulting metrics are used to populated the described meters.

\subsection{Excluded Metrics}\label{Excluded Metrics}

I was forced to exclude availability - not enough resources (money and time) to run the tests long enough to run into any availability issues, since Amazon SLAs offer 99.9999\% orders of availability for selected technologies.

Also forced to include Dynamic ScaleUp and Down. The way load test suite is setup is taht it hits a single Route53 url, which then distributes traffic between regions according to the specified policy. Dynamically adding / removing resources causes changes to that policy - that means global DNS records need to be updated. One can set TTL (time to live) which DNS records should respect and refresh their caches every TTL seconds. They don't and it's a known issue - I ran into this, had to wait even up to 30 minutes for 60s changes to propagate properly.

\section{Controlling The Dynamic Cloud Environment}
tl;dr - impossible.

But you can pick instances that are more stable and have better guarantees (rather than some weird spikes and collecting credits if you run under limit - that's what happens on t1.micro isntances).

We run repetetive tests to verify there is no huge influential differences in allocated resources.

One can also request dedicated rather than shared resources - outside of the scope of this project though, but offers finer control.

\section{Expected Answers}\label{Expected Answers}
I am going to use the \textit{Effective System Scalability} and \textit{Effective Scalable Range} meters to perform a quantifiable comparison of the deployed architectures \ref{Compared Architectures}.

I expect to see differences in various metrics. Using all the other selected meters, I will be able to reason about singled out factors of scalability (such as cost or message handling latency).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiment results}\label{Experiment results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}\label{Evaluation}

\section{Statistical Significance}
http://technology.stitchfix.com/blog/2015/05/26/significant-sample/
http://insightdatascience.com/blog/ab\_testing\_statistics.html
http://www.qubit.com/sites/default/files/pdf/mostwinningabtestresultsareillusory\_0.pdf
http://www.evanmiller.org/how-not-to-run-an-ab-test.html


\section{Threats to validity}
ec2 whacky, control over hardware released to amazon (can be dedicated machines)
shared environment, someone else might get ddosed or sth - you have no control over that
network is inherently non-deterministic, load tests needed to actually test availability

\section{Host Company Benefits}
- measerment suite for further improvements
- real time telemetry introduced into the system
- system performance objectively improved along the process, bottlenecks identified
- cloud management suite
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}\label{Conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Further Work}\label{Further Work}
dynamodb
improvement to the models so that weighs can be added to the scalability meteres according to preferences.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
`'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BELOW WE HAVE ORIGINAL TEMPLATE STUFF THAT NEEDS TO BE REMOVED
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Everything below that needs to be removed (except for bib), Front Matter}

The first thing is to connect the class by saying:

\begin{snippet}
\begin{verbatim}
\documentclass{uvamscse}
\end{verbatim}
\end{snippet}

\section{Title}

Specify the title of the thesis with \cmd{title} and \cmd{subtitle} commands:

\begin{snippet}
\begin{verbatim}
\title{MetaThesis}
\subtitle{A Thesis Template Leading by Example}
\end{verbatim}
\end{snippet}

Any thesis can survive without a \cmd{subtitle}, but the \cmd{title} is mandatory.

\section{Author}

Introduce yourself with \cmd{author} and \cmd{authemail}:

\begin{snippet}
\begin{verbatim}
\author{Vadim Zaytsev}
\authemail{vadim@grammarware.net}
\end{verbatim}
\end{snippet}

Again, \cmd{authemail} is not mandatory. If you need anything fancier, just put it inside \cmd{author}.

\begin{snippet}
\begin{verbatim}
\author{Vadim Zaytsev\footnote{Yes, that one.}}
\end{verbatim}
\end{snippet}

The footnote would be printed on the bottom of the title page, and will be
referred to by a symbol, not by a number as any footnotes within the main
document body.

\section{Date}

By default, the date inserted in your PDF is the day of the build, e.g., ``March 25, 2014''. If you want it to be formatted differently or be more vague or outright fake, use \cmd{date}:

\begin{snippet}
\begin{verbatim}
\date{Spring 2014}
\end{verbatim}
\end{snippet}

The argument is just a string, the format is unrestricted:

\begin{snippet}
\begin{verbatim}
\date{Tomorrow. Honestly.}
\end{verbatim}
\end{snippet}

\section{Host}

If your hosting organisation is not the UvA, specify it with \cmd{host}. The
logo on the bottom of the title page will still be the UvA one, because this
is the organisation guaranteeing your degree.

\begin{snippet}
\begin{verbatim}
\host{Grammarware, Inc., \url{http://grammarware.github.io}}
\end{verbatim}
\end{snippet}

NB: footnotes will not work, unless you know how to \cmd{protect} them.

\section{Cover picture}

If the first page of your thesis looks too blunt, add a picture to it:

\begin{snippet}
\begin{verbatim}
\coverpic{figures/terminal.png}
\end{verbatim}
\end{snippet}

You can even specify the picture's width as an optional argument:

\begin{snippet}
\begin{verbatim}
\coverpic[100pt]{figures/terminal.png}
\end{verbatim}
\end{snippet}

How these three options look, you can see from \autoref{fig:titles}.

\begin{figure}[t]
  \fbox{\includegraphics[width=.25\textwidth]{figures/title1.pdf}}
  \hfill
  \fbox{\includegraphics[width=.25\textwidth]{figures/title2.pdf}}
  \hfill
  \fbox{\includegraphics[width=.25\textwidth]{figures/title3.pdf}}
  \caption{A hypothetical thesis title page without a cover picture (on the left), with an overly large one (in the centre) and with a tiny pic (on the right).}
  \label{fig:titles}
\end{figure}

\section{Abstract}

A thesis is fine without an abstract, if you do not feel like writing it and
your supervisor does not feel like enforcing it. If you do want an abstract,
make it with the \cmd{abstract} command:

\begin{snippet}
\begin{verbatim}
\abstract{This is not a thesis.}
\end{verbatim}
\end{snippet}

The abstract is just like any other section of your thesis, so you can use any
\LaTeX\ tricks there. If you think that the name ``abstract'' is too abstract
for your abstract, you can still use \cmd{abstract} without being too
abstract:

\begin{snippet}
\begin{verbatim}
\abstract[Confession]{I am a cenosillicaphobiac.}
\end{verbatim}
\end{snippet}

Kent Beck~\cite{JohnsonBBCGW93} proposes to have four sentences in a good abstract:

\begin{enumerate}
  \item The first states the problem.
  \item The second states why the problem is a problem.
  \item The third is the startling sentence.
  \item The fourth states the implication of the startling sentence.
\end{enumerate}

In practice, each of these ``sentences'' can be longer than an actual
sentence, but it is in general a good rule of thumb to condense the summary of
your thesis into these four tiny messages. Do not write too much, make it
tweetable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Core Chapters}

The structure of your thesis is up to you and your supervisor. Whatever you
do, do not consider the guidelines below as dogmas.

\section{Classic structure}

\begin{description}
  \item[Problem statement and motivation.]
  You describe in detail what problem the research is addressing, and what is
the motivation to address this problem. There is a concise and objective
statement of the research questions, hypotheses and goals. It is made clear
why these questions and goals are important and relevant to the world outside
the university (assuming it exists). You can already split the main research
question into subquestions in this chapter. This section also describes an
analysis of the problem: where does it occur and how, how often, and what are
the consequences? An important part is also to scope the research: what
aspects are included and what aspects are deliberately left out, and why?
  \item[Research method.]
  Here you describe the methods used to answer the research questions. A good
structure of this section often follows the subquestions by providing a method
for each. The research method needs a thorough motivation grounded in theory
in order to be acceptable. As a part of the method, you can introduce a number
of hypotheses --- these will be tested by the research, using the methods
described here. An important part of this section is validation. How will you
evaluate and validate the outcomes of the research?
  \item[Background and context.]
  This chapter contains all the information needed to put the thesis into
context. It is common to use a revised version of your literature survey for
this purpose. It is important to explicitly refer from your text to sources
you have used, they will be listed in your bibliography. For example, you can
write ``A small number of programming languages account for most language
use~\cite{MeyerovichR2013}'', where the following entry would be included in
your bibliography:
\begin{quote}
\cite{MeyerovichR2013} Leo A. Meyerovich and Ariel S. Rabkin. Empirical Analysis of Programming Language Adoption. In \emph{Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages and Applications}, OOPSLA, pages 1--18. ACM, 2013. \doi{10.1145/2509136.2509515}.
\end{quote}
Have a look at \autoref{sec:biblio} to learn more about citation.
  \item[Research.]
  This chapter reports on the execution of the research method as described in
an earlier chapter. If the research has been divided into phases, they are
introduced, reported on and concluded individually. If needed, this chapter
could be split up to balance out the sizes of all chapters.
  \item[Results.]
  This chapter presents and clarifies the results obtained during the
  research. The focus should be on the factual results, not the interpretation
  or discussion. Tables and graphics should be used to increase the clarity of
  the results where applicable.
  \item[Analysis and conclusions.]
  This chapter contains the analysis and interpretation of the results. The
  research questions are answered as best as possible with the results that
  were obtained. The analysis also discussed parts of the questions that were
  left unanswered.

  An important topic is the validity of the results. What methods of
  validation were used? Could the results be generalised to other cases? What
  threats to validity can be identified? There is room here to discuss the
  results of related scientific literature here as well. How do the results
  obtained here relate to other work, and what consequences are there? Did
  your approach work better or worse? Did you learn anything new compared to
  the already existing body of knowledge? Finally, what could you say in
  hindsight on the research approach by followed? What could have done better?
  What lessons have been learned? What could other researchers use from your
  experience? A separate section should be devoted to ``future work'', i.e.,
  possible extension points of your work that you have identified. Even other
  researchers should be able to use those as a starting point.
\end{description}

\section{Reporting on replications}

Here are the guidelines to report on replicated studies~\cite{Carver10}:

\begin{description}
  \item[Information about the original study]~\\
    \begin{description}
    \item[Research question(s)] that were the basis for the design
    \item[Participants,] their number and any other relevant characteristics
    \item[Design] as a graphical or textual description of the experimental design
    \item[Artefacts,] the description of them and/or links to the artefacts used
    \item[Context variables] as any important details that affected the design of the study or interpretation of the
results
    \item[Summary of the results] in a brief overview of the major findings
    \end{description}
  %
  \item[Information about the replication]~\\
    \begin{description}
    \item[Motivation for conducting the replication] as a
description of why the replication was conducted:
to validate the results, to broaden the results by
changing the participant pool or the artifacts.
    \item[Level of interaction with original experimenters.]
The level of interaction between the original experimenters and the
replicators should be reported. This interaction could range from none (i.e.
simply read the  paper) to them being the same people. There is quite a lot of
discussion of the level of interaction allowed for the replication to be
``successful'', but this level should be reported even without  addressing
the controversy.
    \item[Changes to the original experiment.] Any changes made to the
design, participants, artifacts, procedures, data collected and/or analysis
techniques should be  discussed along with the motivation for the change.
    \end{description}
  \item[Comparison of results to original]~\\
    \begin{description}
    \item[Consistent results,] when replication results supported
results from the original study, and
    \item[Differences in results,] when results from the replication
did not coincide with the results from the original study.
Authors should also discuss how changes made to the
experimental design (see above) may have caused
these differences.
    \end{description}
    \item[Drawing conclusions across studies]
\end{description}

NB: this section contains portions of text repeated directly from Carver~\cite{Carver10} and
only slightly massaged. Do not do this for your thesis, write your own thoughts down.

\section{\LaTeX\ details}

\subsection{Environments}

A \LaTeX\ environment is something with opening and closing tags, which look
like \cmd{begin}\{\texttt{name}\} and \cmd{end}\{\texttt{name}\}. Some useful
environments to know:

\begin{center}
\begin{tabular}{ll}
  \texttt{itemize}      & bullet lists\\
  \texttt{enumerate}    & numbered lists\\
  \texttt{description}  & definition lists\\
  \hline
  \texttt{center}       & centered line elements\\
  \texttt{flushright}   & right aligned lines\\
  \texttt{flushleft}    & left aligned lines\\
  \hline
  \texttt{tabular}      & table\\
  \texttt{longtable}    & multi-page table (needs the \texttt{longtable} package)\\
  \texttt{sideways}     & rotates some text\\
  \texttt{quote}        & block quote\\
  \texttt{verbatim}     & unformatted text\\
  \texttt{minipage}     & compound box with elements inside\\
  \texttt{boxedminipage}& compound box with elements inside and a border around it\\
  \hline
  \texttt{table}        & floating table (needs to have \texttt{tabular} nested inside)\\
  \texttt{figure}       & floating figure\\
  \texttt{sourcecode}   & floating listing\\
  \hline
  \texttt{equation}     & mathematical equation\\
  \texttt{lstlisting}   & pretty-printed syntax highligted listing\\
  \texttt{multline}     & mathematical equation spanning over multiple lines\\
  \texttt{eqnarray}     & system of mathematical equations\\
  \texttt{gather}       & bundled mathematical equations\\
  \texttt{align}        & bundled and aligned mathematical equations\\
  \texttt{array}        & matrix\\
  \texttt{CD}           & commutative diagrams\\
\end{tabular}
\end{center}

\section{Listings}

\begin{sourcecode}
\begin{lstlisting}[language=prolog]
define(Ps1,G1,G2)
 :-
    usedNs(G1,Uses),
    ps2n(Ps1,N),
    require(
      member(N,Uses),
      'Nonterminal ~q must not be fresh.',
      [N]),
    new(Ps1,N,G1,G2),
    !.
\end{lstlisting}
\caption{Code in Prolog}
\end{sourcecode}

\begin{sourcecode}
\begin{lstlisting}[language=sdf]
module Syntax

imports Numbers
imports basic/Whitespace

exports
  sorts
    Program Function Expr Ops Name Newline

  context-free syntax
    Function+                          -> Program
    Name Name+ "=" Expr Newline+       -> Function
    Expr Ops Expr                      -> Expr      {left,prefer,cons(binary)}
    Name Expr+                         -> Expr      {avoid,cons(apply)}
    "if" Expr "then" Expr "else" Expr  -> Expr      {cons(ifThenElse)}
    "(" Expr ")"                       -> Expr      {bracket}
    Name                               -> Expr      {cons(argument)}
    Int                                -> Expr      {cons(literal)}
    "-"                                -> Ops       {cons(minus)}
    "+"                                -> Ops       {cons(plus)}
    "=="                               -> Ops       {cons(equal)}
\end{lstlisting}
\caption{Code in SDF}
\end{sourcecode}

\begin{sourcecode}
\begin{lstlisting}[language=Java]
import types.*;
import org.antlr.runtime.*;

public class TestEvaluator
    public static void main(String[] args) throws Exception {

        // Parse file to program
        ANTLRFileStream input = new ANTLRFileStream(args[0]);
        FLLexer lexer = new FLLexer(input);
        CommonTokenStream tokens = new CommonTokenStream(lexer);
        FLParser parser = new FLParser(tokens);
        Program program = parser.program();

        // Parse sample expression
        input = new ANTLRFileStream(args[1]);
        lexer = new FLLexer(input);
        tokens = new CommonTokenStream(lexer);
        parser = new FLParser(tokens);
        Expr expr = parser.expr();

        // Evaluate program
        Evaluator eval = new Evaluator(program);
        int expected = Integer.parseInt(args[2]);
\end{lstlisting}
\caption{Code in Java}
\end{sourcecode}

\begin{sourcecode}
\begin{lstlisting}[style=mono,language=Python]
#!/usr/local/bin/python
# wiki: BGF
import os
import sys
import slpsns
import elementtree.ElementTree as ET

# root::nonterminal* production*
class Grammar:
  def __init__(self):
    self.roots = []
    self.prods = []
  def parse(self,fname):
    self.roots = []
    self.prods = []
    self.xml = ET.parse(fname)
    for e in self.xml.findall('root'):
      self.roots.append(e.text)
    for e in self.xml.findall(slpsns.bgf_('production')):
      prod = Production()
      prod.parse(e)
      self.prods.append(prod)
\end{lstlisting}
\caption{Code in Python}
\end{sourcecode}

\chapter{Literature}\label{sec:biblio}

\textsc{Bib}TeX\ is a JSON-like format for bibliographic entries. Encode each
source once as a \textsc{Bib}\TeX\ entry, give it a name and refer to it from
any place in your thesis. The bibliography at the end of the thesis will be
compiled automatically from those entries that are referenced at least once,
it will also be automatically sorted and fancyfied (URLs, DOIs, etc).

DOI is a digital object identifier, it is uniquely and immutably assigned to
any paper published in a well-established journal or conference proceedings
and can be used to refer to it. When used in a browser, it resolves to a
publisher's website where paper can be obtained. Including DOIs in citations
is considered good practice and lets the readers of your thesis get to the
text of the paper in one click. Books do not have DOIs, only ISBNs; some
workshop proceedings and most unofficial publications do not have DOIs. If you
want to get a DOI assigned to your work such as a piece of code, upload it to
\href{http://www.figshare.com}{FigShare}.

Keys in key-value pairs within each \textsc{Bib}\TeX\ entry are never quoted,
values usually are, but can also be included within curly brackets or left as
is, which works fine for numbers (e.g., years). If you want to preserve the
value from any adjustments (e.g., no recapitalisation in titles), use curlies
\emph{and} quotes. Separate authors and editors by ``and'', which will
automatically be mapped to commas or left as ``and''s as necessary.

\section{Books}

\cite{GruneJacobs} is just as good as the Dragon Book, but newer and has an
awesome extended bibliography available for free.

\begin{snippet}
\begin{verbatim}
@book{GruneJacobs,
  author    = "D. Grune and C. J. H. Jacobs",
  title     = "{Parsing Techniques: A Practical Guide}",
  series    = "Monographs in Computer Science",
  edition   = 2,
  publisher = "Springer",
  url       = "http://www.cs.vu.nl/~dick/PT2Ed.html",
  year      = 2008,
}
\end{verbatim}
\end{snippet}

\section{Journal papers}

Not all TOSEM papers are hard to read~\cite{GrammarwareAgenda}.

\begin{snippet}
\begin{verbatim}
@article{GrammarwareAgenda,
  author      = "Paul Klint and Ralf L{\"a}mmel and Chris Verhoef",
  title       = "{Toward an Engineering Discipline for Grammarware}",
  journal     = "ACM Transactions on Software Engineering Methodology (TOSEM)",
  volume      = 14,
  number      = 3,
  year        = 2005,
  pages       = "331--380",
}
\end{verbatim}
\end{snippet}

\section{Conference papers}

There is no limit to how many grammars can be used in one paper, but the
current record stands at 569~\cite{Micropatterns2013}.

\begin{snippet}
\begin{verbatim}
@inproceedings{Micropatterns2013,
  author = "Vadim Zaytsev",
  title = "{Micropatterns in Grammars}",
  booktitle = "{Proceedings of the Sixth International Conference on Software Language Engineering
                (SLE 2013)}",
  year = 2013,
  editor = "Martin Erwig and Richard F. Paige and Eric Van Wyk",
  volume = "8225",
  series = "LNCS",
  pages = "117--136",
  address = "Switzerland",
  month = oct,
  publisher = "Springer International Publishing",
  doi = "10.1007/978-3-319-02654-1_7",
}
\end{verbatim}
\end{snippet}

\section{Theses}

The seventh PhD student of Paul Klint was Jan Rekers~\cite{Rekers92}.

\begin{snippet}
\begin{verbatim}
@phdthesis{Rekers92,
 author   = "J. Rekers",
 title    = "{Parser Generation for Interactive Environments}",
 school   = "University of Amsterdam",
 year     = 1992,
 url      = "http://homepages.cwi.nl/~paulk/dissertations/Rekers.pdf",
}
\end{verbatim}
\end{snippet}

There is also \texttt{mastersthesis} type with exactly the same structure for
referring to Master's theses.

\section{Technical reports}

The original seminal work introducing two-level grammars was never published
in any book or conference, but there is a technical report explaining
it~\cite{Wijngaarden65}. SMC, or \emph{Stichting Matematisch Centrum}, was the
old name of CWI fifty years ago.

\begin{snippet}
\begin{verbatim}
@techreport{Wijngaarden65,
        author      = "Adriaan van Wijngaarden",
        title       = "{Orthogonal Design and Description of a Formal Language}",
        month       = oct,
        year        = 1965,
        institution = "SMC",
        type        = "{MR 76}",
        url         = "http://www.fh-jena.de/~kleine/history/languages/VanWijngaarden-MR76.pdf",
}
\end{verbatim}
\end{snippet}

\section{Wikipedia}

You do not refer to Wikipedia from academic writing, it works the other way around.

\section{Anything else}

You can refer to pretty much anything (websites, blog posts, software) through
\texttt{misc} type of entry~\cite{ANTLR}:

\begin{snippet}
\begin{verbatim}
@misc{ANTLR,
 author       = "Terence Parr",
 title        = "{ANTLR---ANother Tool for Language Recognition}",
 howpublished = "Software",
 url          = "http://antlr.org",
 year         = "2008"
}
\end{verbatim}
\end{snippet}

{%\tiny
\bibliographystyle{alphaurl}
\bibliography{thesis}
}

\end{document}
