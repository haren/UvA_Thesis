\documentclass{uvamscse}

\usepackage{color}
\usepackage{url}


\usepackage{epigraph}
\setlength\epigraphwidth{8cm}
\setlength\epigraphrule{0pt}

\usepackage{etoolbox}
\makeatletter
\patchcmd{\epigraph}{\@epitext{#1}}{\itshape\@epitext{#1}}{}{}
\makeatother

\input{program-listings}
\newcommand{\cmd}[1]{\texttt{$\backslash$#1}}

\usepackage{graphicx}
\graphicspath{ {figures/} }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title page.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Geographically-aware scaling for real-time persistent websocket applications.}
% \coverpic[100pt]{figures/terminal.png}
\subtitle{Master's Project in Software Engineering}
\date{Summer 2015}

\author{Lukasz Harezlak}
\authemail{lukasz.harezlak@gmail.com}
\host{Instamrkt, \url{https://instamrkt.com}}

\abstract{
  This section summarises the content of the thesis for potential readers who do not have time to read it whole,
  or for those undecided whether to read it at all. Sum up the following aspects:

  \begin{itemize}
    \item relevance and motivation for the research
    \item research question(s) and a brief description of the research method
    \item results, contributions and conclusions
  \end{itemize}

Kent Beck~\cite{JohnsonBBCGW93} proposes to have four sentences in a good abstract:

  \begin{enumerate}
    \item The first states the problem.
    \item The second states why the problem is a problem.
    \item The third is the startling sentence.
    \item The fourth states the implication of the startling sentence.
  \end{enumerate}
}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Let the juicy stuff start.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
Software scalability.
Cloud scalability.
Why is it important
Importance of measuring in the clouds and testing optimal architectures.

\section{Initial Study}
What we found out researching.

\section{Problem Statement}
what's the best architecture that answers that best:
how to deliver data to geographically distributed users with minimal, consistent and manageable latency
how to react
scales up and down in response to demand changes (WEBSOCKETS - DISCONNECT SESSIONS? WHEN?  This provides some additional challenges when it comes to websockets, since in order to stop an instance one needs to make sure it does not serve any sessions. \cite{GroBuy})
Moreover, they do not consider the cost and degree of utilization of the employed resources within a data center. \cite{GroBuy}
according to the models based on the set of metrics described in detail later \ref{Selected metrics}
Also, there is an inherent limitation for scaling a websocket application - a number of TCP ports (and file descriptors available to a ws/wss connections) on a server instance. Hardware limitations are different in an http-based application.
\subsection{Research Questions}
\begin{enumerate}
  \item Does architecture with geographically aware scaling case can produce better results than the baseline architecture?
  \item What is the preferred architecture decomposition for a system with given characteristics - stateless, deployed in the cloud, dynamically scaled up and down, with persistent connections, clients distributed globally and uncacheable data generated in real time?
\end{enumerate}
\subsection{Solution Outline}
How our solution works in short.
\subsection{Research Method}
Software engineering is a relatively difficult field to investigate in terms of lack of clarity on how to do it. Most of the problems are of design, rather than pure scientific, nature. West Churchman coined a specific term for these kind of modern problems - wicked (since they are resistant to resolutions) [MMPROJ1]. Eastbrook et al claim it is often difficult to identify the true underlying nature of the research problem and thus the best method to research it [MMPROJ2]. In their work, they name and compare five most classes of research methods to select from: controlled experiments, case studies, survey research, ethnographies, action research. They help to select the method by first establishing the type of research question being asked - existence question (“Does X exist”?), description and classification question (“What is X like?”), and descriptive-comparative questions(“How does X offer from Y?”). The questions of this research are of the last type. The authors suggest pinpointing up-front what will be accepted as a valid answer to the research question [MMPROJ2]. The detailed description of each of the methods helps me to settle for the controlled experiment. It’s well-suited for testing a hypothesis where manipulating independent variables has an effect of dependent ones, which is exactly the case of me research. The manipulated variable is architecture decomposition, and the measured ones are determined by scalability measurement models described below.
\subsubsection{Research Difficulty}
The experiments will be performed in a shared cloud environment, which is inherently unpredictable and changing. Designing and executing a test yielding statistically relevant results where all the necessary variables are controlled (within reasonable boundaries) will be a challenge.
The focus of the project is on scaling a websocket application. This is a relatively new technology, thus finding proper scientific coverage is not trivial.
Some of the necessary development and data collection might be in relatively low-level technology.
The geo-location with reasonable accuracy of the clients might prove difficult too.
Overall, it’s a complex project requiring knowledge of both hardware (protocol), software (architecture) and consisting of multiple disciplines.
even linking order in the compiler can change the performance!!! [one of CRIMES]
\subsection{Hypothesis}
The correct geographical decomposition of application stack can lead to vastly improved performance in comparison with baseline architecure(TODO:S????).

\section{Contributions}
Case study on AWS.
Architecture decomposition analysis.
Deliverable piece of code - metrics framework and auto-scaling scripts.

\section{Related Work}
The big paper on measuring scalability in the clouds.
custom routing between regions. A lot on page 14/21 MMPROJ.
The big paper on deploying different scalability structures \cite{GroBuy}.
Software engineers need to design for the cloud, not only deploy in the cloud. This is even more important when using multiple data centers situated in different legislative domains; constructed with different hardware, network, and software components; and prone to different environmental risks.
Both Route 53 and ELB do not consider applications’ regulatory requirements when selecting a data center site. Moreover, they do not consider the cost and degree of utilization of the employed resources within a data center.
because they want custom metrics routing they
Selecting the cloud:
As a first step, the user authenticates to one of the entry points. At this point, the entry point has the user’s identity and geographical location (extracted from the IP address). As a second step, the entry point broadcasts the user’s identifier to the admission controllers of all data centers. We call this step matchmaking broadcast.
As a first step, the user authenticates to one of the entry points. At this point, the entry point has the user’s identity and geographical location (extracted from the IP address). As a second step, the entry point broadcasts the user’s identifier to the admission controllers of all data centers. We call this step matchmaking broadcast.
The admission controllers respond to the entry point whether the user’s data are present and if they are allowed (in terms of legislation and regulations) to serve the user. In the response, they also include information about costs within the data center.
Based on the admission controllers’ responses, the entry point selects the data center to serve the user and redirects him or her to the load balancer deployed within it. The entry point filters all clouds that have the user’s data and are eligible to serve him or her. If there is more than one such cloud, the entry point selects the most suitable with respect to network latency and pricing. If no cloud meets the data location and legislative requirements, the user is denied service.
After a data center is selected, the user is served by the AS and DB servers within the chosen cloud as prescribed by the standard three-tier architecture.
they do:
sticky load balancing
it is not beneficial to terminate a running VM ahead of its next billing time. It is better to keep it running until its billing time in order to reuse it if resources are needed again.
their objectives - cost minimization and latency minimization
they use front VMs and each cloud runs an ELB an Admission Controller
latency approximation
GeoLite db (ip->geo coordinates)
compute latency using PingER (To approximate the latency between a user and a cloud, we select the three pairs of PingER hosts that are closest to the user and the cloud, respectively, and define the latency as a weighted sum of the three latencies between the hosts in these three pairs)
CloudSim
Small mentions of the others.

\section{Outline}
Here we outline the structure of the thesis. A short paragraph on what happens in each chapter.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}

\section{Scalability}

Scalability seems to be a notion that everyone intuitively grasps, but has difficulties when it comes to clear explanations. In my literature study I came across a few definitions, which might help with that, of which three can be found below:

\begin{quote}
~\cite{Williams04} Scalability is a measure of an application system’s ability to, without modification, cost-effectively provide increased throughput, reduced response time and/or support more users when hardware resources are added.
\end{quote}

\begin{quote}
\cite{WeinstockOnSystem2006} Scalability is an ability of a system to handle increased workload (without adding resources).
\end{quote}\label{x}
In light of this definition we would talk about a scalability failure if one of the following occured:
\begin{itemize}
  \item Address space was exceeded,
  \item Memory was overloaded,
  \item Available network bandwith was exceeded,
  \item etc.
\end{itemize}

\begin{quote}
\cite{WeinstockOnSystem2006} Scalability is an ability of a system to handle increased workload by repeatedly applying a cost-effective strategy for extending a system’s capacity.
\end{quote}
According to this definition, one could determine system scalability failure if a given resource got overloaded or exhausted an adding capacity to this resource would not result in a proportional ability to handle additional demand, e.g.:
\begin{itemize}
  \item an additional processor will not contribute to meeting the higher demand if handing of that processor entails an overhead).
  \item a newly added server instance might not contribute to handling a higher user demand if slows down the routing process.
\end{itemize}

\subsection{Scalability Cost}
Scalability is generally desired in the software systems, yet, as all architectural software decisions come at a cost \cite{GerHeiBench}, one must be aware of the trade-offs usually associated with it. Weinstock and Goodenough \cite{WeinstockOnSystem2006} point these out:
\begin{itemize}
  \item performance and scalability (non-scalable system will often demonstrate degrading performance with increasing demand, but scalable systems require performance sacrifice on lower usage levels),
  \item cost and scalability (designing a system to be scalable up-front entails additional costs),
  \item operability and scalability (it is difficult for humans to operate large systems),
  \item usability and scalability (it may be possible to increase servable demand with limiting system's service scope - e.g. removing personalization and displaying generic, cacheable data),
  \item data consistency and scalability (higher scalability can be achieved if system allows for data inconsistencies).
\end{itemize}

\subsection{Need for Scalability}

As the globalization and internetization progresses, more and more systems are expected to be capable of serving millions of globally distributed users. A single server instance often cannot live up to that task and thus application needs to be divided and distributed in multiple smaller chunks. This division happens on different application layers, and different parts of the system have to communicate and synchronize with each other.

In case of many software systems (the system under test \ref{The System Under Test} included), user traffic, and with it the need for system services and resources, varies significantly. A need to be able to scale up and down dynamically in response to traffic arises from this.

Huge parts of the internet are shifting towards real-time. This trend is giving rise to new technologies for exchanging messages between clients and servers in the client-server architecture. Traditionally, client would send a request to a server and receive a response. This is hugely inefficient when there is a need for continuous bidirectional exchange of messages. As an improvement, new mechanisms for server-client communication have been introduced recently \ref{Client-Server Communication Improvements} to enhance the process and reduce overhead. One of them - websockets - are a huge next step on this path, but introduce new challenges. One of them is scalability of applications which make use of this technology.

\subsection{Existing Approaches}
There exist multiple scalability vectors for web applications. Different decompositions of application stack can be applied to achieve the goal of scalability. A few traditional approaches of tackling an issue like that exist already \cite{Akamai}:
\begin{description}
  \item[Scaling up.]
  Increasing volume of system allocated resources. As internet is unreliable and so called "middle-mile bottlenecks" exist,
  a web application end-user latency and throughput experience is not fully deterministic. Traffic levels fluctuate tremendously, so the need to provision for peak traffic levels means that expensive infrastructure will sit underutilized most of the time. It is easy to implement, but costly, even extremely when you start pushing at current hardware limits.\cite{Qvef}
  \item[Scaling out.]
  Scaling out horizontally - increasing the number of units of resources comprising the system. Cost of hardware can be reduced dramatically this way. In a web application, one can deploy multiple instances of servers. Different types of balancing can be applied to distribuet traffic among them: application layer balancing, business load balancing, and anticipating load. The overhead of parsing requests in the application layer is high thus limiting scalability compared to load balancing in the transport layer. Client state needs to be stored in a layer shared between the webservers.\cite{Qvef}
  \item[Content Delivery Networks.]
  These only handle static assets. Communication in our system \ref{The System Under Test} happens mainly over websockets, which are not supported by CDNs. CDNs can be divided into Big Data Center CDNs and Highly Distributed CDNs, which put application data within end-user ISPs.\cite{Akamai}
  \item[Peer to peer networks.]
  An architecture different from client - server, where users communicate with each other directly. It handles adding and removing nodes to and from the network dynamically very well.
\end{description}

\subsection{Cloud Scalability}
Cloud computing has become virtually ubiquitous. All biggest internet services are either deployed to a cloud, or run their proprietary cloud systems. A lot of companies running proprietary clouds also make them available for hosting to the external clients. Netflix\footnote{\url{https://aws.amazon.com/solutions/case-studies/netflix/}} and Spotify\footnote{\url{https://aws.amazon.com/solutions/case-studies/spotify/}} (both deployed to Amazon's AWS) are examples of the first approach, with Microsoft(Azure)\footnote{\url{https://azure.microsoft.com/}}, Google(Google Cloud Platform)\footnote{\url{https://cloud.google.com/}} and Amazon(Amazon Web Services)\footnote{\url{https://aws.amazon.com/}} being the example of the second.
General cloud capabilities and the cloud stack I am working with for the scope of this project is discribed in a dedicated chapter \ref{Cloud architecture setup}.

One of the biggest advantages of deploying one's architecture to the cloud that aforementioned companies offer is the ability to dynamically scale up and in response to application traffic changes.

As Grozev and Buyya \cite{GroBuy} put it, to fully facilitate cloud capabilities, software engineers need to design for the cloud, not only to deploy to it.

\subsection{Data-layer Scalability}

Data layer scalability is an important part of system scalability. In a distributed system, multiple system agents share the data. There exist multiple strategies for operating on a shared, distributed data layer.

Data layer is often a performance bottleneck because of requirements for transactional access and atomicity - it is hard to scale out when system uses a relational data store \cite{GroBuy}.

\begin{description}
  \item[CAP Theorem]
  put forward by Eric Brewer \cite{Cap} states that it is impossible for a distributed system to provide all of the following guarantees:
  \begin{itemize}
    \item consistency,
    \item availability,
    \item partition tolerance.
  \end{itemize}
  Therefore, a trade-off needs to be made, depending on stakeholder priorities, which of the three to give up.
  \item[ACID]
   stands for Atomicity, Consistency, Isolation, Durability and is a set of properties guaranteeing reliable processing of database transactions. It has been a guideline in designing multiple database systems. The term was originaly coined by Haerder and Reuter in 1983 \cite{ACID}.
  \item[BASE]
   stands for Basically Available, Soft State, Eventually Consistent \cite{EveCon}. It’s a complement of ACID. Author claims we lack precise metrics to measure BASE aspects and that’s why every system implements eventual consistency differently.

  Most importantly for this research, the author \cite{EveCon} explains \textit{MySQL Cluster} - it performs much like an ACID database but with the performance benefits of a cluster. Aside from that, \textit{MySQL Replication} can be put to use. It can be configured at multiple topologies, not only the basic master-slave; consistency differs per configuration.
\end{description}

Many solutions and strategies for dealing with database scalability have emerged, including \textit{NoSQL} and \textit{NewSQL} databases, data replication and sharding \cite{Amza}.

Cooper et al touch on that subject in their work \cite{Ycsb}. They claim that in scaling out the database layer one should aim for elasticity (dynamically adding capacity to a running system) and high availability. These are hard to achieve using traditional database systems. They show that new protocols are being developed to address that issue, such as that: \textit{two-phase commit protocol} (provides atomicity for distributed transactions) and \textit{paxos}.


The authors \cite{Ycsb} also give an overview of different database systems, including \textit{PNUTS}, \textit{BigTable}, \textit{HBase}, \textit{Cassandra}, \textit{Sharded MySQL}, \textit{Azure}, \textit{CouchDB}, and \textit{SimpleDB}.


In their work \cite{Ycsb}, we can find enumeration of classic data-related scalability trade-offs:
\begin{itemize}
    \item read performance and write performance,
    \item latency and durability,
    \item synchronous and asynchronous replication,
    \item data partitioning (column and row-based storage).
  \end{itemize}


The technology stack I have been working with in the scope of this project consists of \textit{MySQL} as persistent storage and \textit{Redis} as a key-value cache. It is described in details in a dedicated chapter \ref{Technology Stack}.

\subsubsection{MySQL Scalability}

Mysql White paper \cite{MySQL} gives us a good overview of how scalability works in case of MySQL Cluster. Authors suggest starting the design of the scaling process by identifying which characteristic the application posseses: lots of write operations, real-time user experience, 24x7 user experience or agility and ease-of-use. The System Under Test \ref{The System Under Test} falls into the first and second categories.

They claim to support (among others) auto-sharding for write-scalability, active / active geographic replication and online scaling and schema upgrades.  Geographic replication offers distribution of clusters across remote data centers, which they claim helps reduce latency (which is extremely important case of The System Under Test \ref{The System Under Test}).

Authors show how MySQL Cluster is optimized for real-timeness:
\begin{itemize}
  \item data structures are optimized for in-memory access,
  \item persistence of updates runs in the background,
  \item all indexed columns are stored in memory.
\end{itemize}
According to the white paper \cite{MySQL}, MySQL Cluster is very well-suited for on-line, on-demand scalin.

MySQL can also be used to scale in unconventional ways. Ruflin et al, in Social-Data Storage-Systems \cite{SoDaSS}, mention that Twitter uses MySQL as a key value store.

They show how MySQL can be scaled horizontally by both sharding and data replication. They also indicate that the more structured the RDBMS data, the harder it is to scale horizontally. MySQL is optimized for writes, since only one record in one table is touched, whereas reads can prove expensive if they contain joints, especially spreading across multiple cluster nodes. Facebook and Twitter solved it by putting a cache on top of MySQL \cite{SoDaSS}.

\subsubsection{Redis Scalability}
Single Redis installation is said to be of limited scalability, because of the fact that for good performance the whole data set should fit into memory \cite{SoDaSS}. Redis offers cluster\footnote{\url{http://redis.io/topics/cluster-spec}} and replication\footnote{\url{http://redis.io/topics/replication}} solutions.

In a Redis Cluster, data is automatically sharded (not replicated) among multiple nodes. One has control over sharding; it is possible to ensure that certain data points end up on the same (or on a given) node. Redis Cluster does not guarantee strong consistency - under certain conditions the Cluster can lose writes that were acknowledged to the client. Support for synchronous writes exists, through the command \textit{WAIT}, which highly (but not entirely) decreases the likelihood of lost writes. Nevertheless, using it is discouraged unless absolutely necessary.

Redis can also be scaled for reads using a simple replication in a single master - multiple slaves topology.

\subsection{Websocket Scalability}

An introduction into push-base communication over the internet can be found in Agarwal’s work - Toward a Push-Scalable Global Internet \cite{PushScale}. The key message in the article is that push message delivery on the World Wide Web is not scalable for servers, intermediate network elements, and battery-operated mobile device clients. And yet, most of modern day websites have highly dynamic content updated even up to multiple times a minute (very much so for The System Under Test \ref{The System Under Test}).

Most of internet communications happens over HTTP Running over TCP. Real-time message delivery requires an always-on connection from the server to the client. HTTP proxies have limited memory and tcp ports, are shared among multiple users. Servers need to be provisioned in order to maintain active tcp connections from large populations of user clients. These all provide challenges that need to be dealt with in scalable applications \cite{PushScale}.

On a more positive note, Cassetti and Luz \cite{WebsApi} claim that overhead introduced by the websocket protocol and websocket API is rather small as compared to other communication methods. Furthermore, one data intensive applications can achieve superior bandwidth and performance when using websockets.

My own research has proven preparing websocket-based communication scalability to be difficult and tedious. Most of the cloud scalability stack that is available and that I have been working with was designed and built to support http-based -- and not websocket-based -- communication. On top of that, one needs to tweak the host operating system kernel in multiple ways to increase the number of concurrent websocket connections the system can maintain. Details of the tuning process are described later \ref{Kernel tuning}.

The Websocket Protocol is described in details in a dedicated section \ref{Websocket Protocol}.

\subsection{Measuring Scalability}
Measuring scalabliity proves a challenge, since there is no single physical quantity or unit which the community has accepted as a scalability measure.

Measuring and researching in a cloud environment is difficult, because, by definition, the researcher has no control over the hardware that his system is running on. Clouds are also inherently non-deterministic. Nevertheless, as Sobel et al. describe in their work \cite{Sobel}, even in cloud computing environments, where researchers have little control over network topology or hardware platform, understanding the performance bottlenecks and scalability limitations imposed by the offered infrastructure is valuable.

As this is a crucial topic to my research, this topic is explored in detail in a dedicated chapter \ref{Scalability Measurements}.

\section{Geographical Distribution}

With the rise of global internet services, single applications have to server users who are distributed around the whole globe. With the distribution of the users, comes distribution of the data. Multiple solutions exist that, through data distribution, aim at improving the \textit{Quality of Service}.

The topic is nicely introduced by Tom Leighton in his article \textit{Improving performance of the internet} \cite{Akamai}. He provides a few arguments why it is important to keep data as close to end users as possible. Apart from obvious latency benefits, by doing this, one reduces the chances of suffering from a big \textit{middle mile} provider outage.
He introduces a scale to reason about internet locality.

\begin{table}[h]
\begin{center}
\begin{tabular}{llll}
  \texttt{Scale Name}       & \texttt{Range}        & \texttt{Latency Range}  & \texttt{Typical Packet Loss}\\
  \hline
  local                     & less than 100 miles   & 1ms                     & 0.6\% \\
  regional                  & 500 - 1000 miles      & 15ms                    & 0.7\% \\
  cross-continent           & 3000 miles            & 50ms                    & 1.0\% \\
  multi-continent           & 6000 miles            & 100ms                   & 1.4\% \\
\end{tabular}
\end{center}
\caption{Internet Locality}
\label{table:internetLocality}
\end{table}
As the table \ref{table:internetLocality} shows, the longer data must travel through the middle mile, the more it is subject to congestion, packet loss, and poor performance \cite{Akamai}. This is why companies try to locate servers close to end users (on a scale that cannot be reproduced in this research - requires finer control over infrastructure). The lowest granular scale I am able to work with within this research is \textit{regional}. Leighton writes that big websites have at least two geographically dispersed mirror locations to improve performance, reliability and scalability.

In his work, he includes a set of guidelines to consider when performing geographical scalability:
\begin{itemize}
  \item reduce transport layer overhead (I am achieving this with usage of the websocket protocol),
  \item prefetch embedded content,
  \item assemble pages at the edge (even on an end client machine),
  \item offload computations to the edge,
  \item ensure significant redundancy in all systems to facilitate failover,
  \item use software logic to provide message reliability.
\end{itemize}

Ed Howorka in his interesting paper \textit{Colocation beats the speed of light} \cite{EdHoColoc}  about trading system geographical distributions focuses on the best placement of servers for traders trading on multiple exchanges. His paper demonstrates that traders gain nothing by positioning their computer at the midpoint between two financial exchanges. He claims that  every algorithm on a central machine talking to surrounding servers (users in case of the System Under Test \ref{The System Under Test}) can be replaced by colocated servers (located in geographical proximity to users). Furthermore, he implies and sets out to prove that server colocation provides a better solution for high-speed applications (as opposed to using a big, centralized server located in the middle).


\section{WebSocket Protocol}\label{Websocket Protocol}

WebSockets is an independent, TCP-based communication protocol. The protocol was standardized by the Internet Engineering Task Force (IEFT) in 2011\footnote{\url{https://tools.ietf.org/html/rfc6455}} and has been gaining popularity ever since. All major web browsers and mobile operating system support it network\footnote{\url{http://caniuse.com/\#feat=websockets}}.

\subsection{Technical details}

Every WebSocket communication has to start with an opening handshake. WebSocket us a protocol related to HTTP, in a sense that its handshake is interpreted by HTTP servers as an \textit{Upgrade request}. Websocket also operates on the same ports as HTTP (80 and 443), so as not to get the communication blocked by all the internet's intermediaries configured e.g. to allow exclusively HTTP traffic.

A sample handshake included in the client's request is depicted below. It is a simple \texttt{GET} request. A set of random bytes - \texttt{Sec-WebSocket-Key} - needs to be included.

\begin{sourcecode}
\begin{lstlisting}[style=mono]
GET HTTP/1.1
Upgrade: websocket
Connection: Upgrade
Host: echo.websocket.org
Origin: http://www.websocket.org
Sec-WebSocket-Key: i9ri`AfOgSsKwUlmLjIkGA==
Sec-WebSocket-Version: 13
Sec-WebSocket-Protocol: chat
\end{lstlisting}
\caption{Websocket Upgrade Client Request}
\end{sourcecode}

The server taes the \texttt{Sec-WebSocket-Key}, appends a globally unique identifier (GUID) string, computes a \texttt{SHA1} hash from it and \texttt{Base64}-encodes it. The computed value is sent in the response header confirming the upgrade as \texttt{Sec-WebSocket-Accept}:

\begin{sourcecode}
\begin{lstlisting}[style=mono]
HTTP/1.1 101 Web Socket Protocol Handshake
Upgrade: WebSocket
Connection: Upgrade
Sec-WebSocket-Accept: Qz9Mp4/YtIjPccdpbvFEm17G8bs=
Sec-WebSocket-Protocol: chat
Access-Control-Allow-Origin: http://www.websocket.org
\end{lstlisting}
\caption{Websocket Upgrade Server Response}
\end{sourcecode}

That leads to opening a websocket (\textit{ws://}) communication channel between the client and the server. Secure websocket connections (\textit{wss://}) are also possible.

\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{ws}
\caption{WebSocket Protocol}
\end{figure}

Typically mentioned benefits of using WebSocket protocol include reduced overhead in comparison with HTTP communication. After the initial handshake, only application-specific data travels between client server, as opposed to HTTP where every request/response contains headers, cookies, content type, content length, user-agent, server id, date, last-modified etc.

WebSocket protocol has an another advantage over HTTP on a TCP protocol level (on which both are based) - TCP connection only needs to be opened once per WebSocket communication, whereas for HTTP it is opened for every request (which introduces overhead).

What is more, HTTP servers are often configured to persist in a log form the start and completion of every HTTP request. This entails additional CPU cycles and costly I/O operations. Unless an application is configured to log its proprietary data, this additional logging cost is much smaller in case of WebSockets (since there is only one initial request).

Configuring a server for high-scalability WebSocket communication is no easy task, as I have learned the hard way during this research. The whole process is decribed in details in a dedicated section \ref{Kernel tuning}.

\subsection{Usage and Origins}\label{Client-Server Communication Improvements}

Multiplayer online games and real-time applications with a lot of user generated content are types of application that benefit most from popularization of the WebSocket protocol.

One of the biggest advantages of websockets is that the server can easily send unsolicited messages to the client, rather than simply respond to client's requests (as is the case with HTTP protocol). Before websockets, other solutions were being developed to enable bi-directional client-server communication, yet none of them has gained such popularity and is considered as elegant.

Before websockets, the applications mentioned above had to resort to other, less efficient ways to facilitate bi-directional client-server communication.

% M. Franklin and S. Zdonik provide an insight into the history of push-based technologies. Their paper from 1998 [MMPROJ12] classifies communication mechanisms into aperiodic pull, periodic pull, aperiodic push and periodic push.
Agarwal presents us with an overview of protocols previously used for the same kind of communication \cite{PushScale}:
\begin{description}
  \item[AJAX]
  - asynchronous JavaScript and XML, a request/response model; when originally introduced, it was a huge improvement, since the page didn’t have to be refreshed anymore to get new data. Still used widely, but for a slightly different purpose than the WebSocket Protocol.
  \item[Long Polling]
  - consists of the client sending a single request and the server waits until it can provide the response (or times out), which does not create much traffic but uses a lot of server resources. Connections are artificially kept open and clients need to reconnect periodically.
  \item[Short Polling]
  - also called AJAX-based timer, consists of the client repeatedly (timer is used to regulate that) sending the same request to the server (polling), which creates a lot of useless traffic.
  \item[webRTC]
  - Web Real-Time Communication, enables audio, video and text communication between users using web browsers \footnote{\url{http://www.webrtc.org/}}.
  \item[Server-Sent Events]
  - a stream of events generated by the server, to which a client subscribes. Communication is impossible in the other direction\footnote{\url{https://developer.mozilla.org/en-US/docs/Server-sent_events/Using_server-sent_events}}.
\end{description}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Benchmarking}

\epigraph{"If you're not keeping score, you're just practicing."}{--- Vince Lombardi, American Footbal Player}

As this projects concerns benchmarking different architecture decompositions of The System Under Test \ref{The System Under Test}, it is important to define and describe the benchmarking process.

\begin{description}
  \item[Benchmarking]
  - the term is most commonly defined as an execution of a set of operations against a given object / program, in order to determin it's relative performance. Usually, it consists of a set of standardized tests. The same term is often used to describe benchmarking programs themselves. It is crucial for researchers, tool developers and users \cite{BenRM}.
\end{description}

As James Bornholt describes in his article \cite{BornBen}, computer science research is among the most non-deterministic. A huge risk of omitted-variable bias exists. This is due to the fact that computer systems have a tremendous number of dynamic parts (both in hardware and software). It is virtually impossible for researches and other experts to have complete knowledge of every bit of the system they are working with. Controlling all the involved variables is insurmountable.

Furthermore, many of the system parts factors are non-deterministic; e.g. networks and multi-threading. Because of that, researchers need to be very careful when attempting to draw statistically meaningful conclusions from their research.

Benchmarking a distributed system is even more complex \cite{BornBen}. If it is hosted in a cloud environment, the control over the hardware is in the hands of the cloud provider, not the researcher. This is situation I am in performing this research.

Bornholt \cite{BornBen} lists some of the many environmental factors that can have influence on the performance of a computer system. The most striking example is the \textit{linking order}, which can significantly bias the results of a benchmark. As UNIX systems are highly customizable, it is important to keep the values of environmental variables as consistent as possible across benchmarking sessions.

Author also states a need for the scientific community to unify and stardize the tools for benchmarking systems. A lot of researchers rebuild their infrastructures and build their scripts anew on every project. This is where I hope to contribute to the scientific and open source communities with releasing all the possible parts of the tooling build for the purpose of this project (decribed in details later \ref{Experiment deliverables}).

For a benchmark to be depandable, results need to be reproducible \cite{BornBen}. That means the same results can be obtained reruning the benchmark later on a machine with the same hardware and the same software versions. Reproducibility of experimental results requires measurements to be reliable. A measurement can be called reliable, if the method guarantees accuracy (small systematic and random measurement error, i.e., no bias or “volatile” effects, resp.) and sufficient precision.

Beyer et al \cite{BenRM} list well-established benchmarks for specific tasks:
\begin{description}
  \item[SPECi]
  - The Standard Performance Evaluation Corporation\footnote{\url{https://www.spec.org/}}. They provide benchmarks for CPUs, Graphic Cards, Java Client/Servers, Mail Servers and others.
  \item[TPC]
  - Transaction Processing Performance Council\footnote{\url{http://www.tpc.org/}}. They provide benchmarks for Transaction Processing (OLTP), Decision Support, Virtualization and Big Data.
  \item[nlrpBENCH]
  - Natural Language Requirements Processing\footnote{\url{http://nlrp.ipd.kit.edu/}}. They provide benchmarks for Requirements Engineering.
\end{description}

In case of the system being benchmarked in this research \ref{The System Under Test}, it is hard to reuse any of these since the performance depends on a custom set of operations spreading across multiple system layers.

Authors of "Benchmarking cloud serving systems with YCSB" \cite{Ycsb} point out that it is important to have random distributions for benchmarking loads - \textit{uniform}, \textit{zipfian}, \textit{latest}, \textit{multinomial}, etc.

From my own experience, I have drawn that against a complex system like our utilizing a wide stack of technologies, benchamrking tool should do a few "dry runs" in order to give caches a chance to fill up, internal tables to get populated or cloud comopotents to be warmed up (e.g. Elastic Load Balancers in case of our stack \ref{Cloud architecture setup}).

\section{Benchmarking Crimes}

Gernot Heiser prepared a list \cite{GerHeiBench} of \textit{benchmarking crimes} which should be avoided when attempting to benchmark a system.

\subsubsection{Selective benchmarking}
\begin{enumerate}
  \item Not covering the full evaluation space.
  \item Not evaluating potential performance degradation.
    \begin{enumerate}
      \item \texttt{Progressive Criterion} - performance actually does improve significantly in area of interest.
      \item \texttt{Conservative Criterion} - performance does not significantly degrade elsewhere.
    \end{enumerate}
    For a benchmark to be reliable, both criterions need to be demonstrated. I trust the models that have been selected to reason about system properties \ref{Selected models} are living up that task.

    As an economist Mielton Friedman uded to put it, \textit{there's no such thing as free lunch}\footnote{\url{http://www.amazon.com/Theres-Such-Thing-Free-Lunch/dp/087548297X}}.

    In this context, we can iterpret it in the following way. Techniques improving performance in some capacity usually entail an additional cost (extra bookkeeping, caching). As Heiser writes: "This is really at the heart of systems: it's all about picking the right trade-offs" \cite{GerHeiBench}. In our case the potential improved performance comes at a cost of increased data transfers between data centers (and therefore costs) and simply increased complexity of certain system layers.
  \item Subsetting a benchmark without strong justification. A warning sign that one might sin in this way:
    \begin{itemize}
      \item "we picked a representative subset", "typical results are shown" - reads as "we cherry picked the data to fit our expected results".
    \end{itemize}
    If benchmarking is performed on a subset of a system, a strong justification needs to be as to why a given subset of funcionalities / components was selected. I am benchmarking a real-life system use case.
  \item Selective data set hiding deficiencies. Luckily, more and more statistical tools are being developed to detect that \footnote{\url{http://dataskeptic.com/epnotes/ep55_detecting-cheating-in-chess.php}}.
\end{enumerate}

\subsubsection{Using the same dataset for calibration and validation}
The way to avoid this is to calibrate the system first (using calibration workload). Then, one should use evaluation workload to show how accurate the model is. Both workloads need to be disjoint.

\subsubsection{Providing o indication of significance of data}
An example of this fallacy would be providing raw averages, without any indication of variance. At least standard deviations must be quoted. If in doubt, Heiser recommends using student's t-test to check significance \cite{GerHeiBench}.

\subsubsection{Benchmarking of simplified simulated system}
I am performing the tests on the full system deployed in the exact same environment as the production system.

\subsubsection{Using inappropriate and misleading benchmarks}
The tests are being performed on different versions of the proprietary system. The business goal of the host organization of the research is to find the best architecture - the benchmarking proccess is supposed to help with this. There is no incentive to be dishonest.

\subsubsection{Unfair benchmarking of competitors}
The aforementioned applies.

\subsubsection{Providing relative numbers only}
Heiser suggests this reads as "I am covering that the results are really bad or irrelevant" \cite{GerHeiBench}. In the case of this research, some results need to stary relative since this is what the selected scalability measurement models \ref{Selected models} require. Absolutes might not make sense in this context.

\subsubsection{Providing no proper baseline}
Often the state-of-the-art solution can be used as a baseline. I believe the solution we picked as a baseline (the simplest architecture decomposition) is a good one. The research leading up to the test did not yield any data to prove that statement wrong.

\subsubsection{Using arithmetic mean for averaging across benchmark scores}
Wallace and Fleming \cite{Fleming} point out that this is an incorrect way to approaching averaging.
The proper way to do this (i.e. arrive at a single figure of merit) is to use the \textit{geometric mean of the normalised scores.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The System Under Test}\label{The System Under Test}

real time prediction market
parimutuel pools
directed contracts
sort of a stock market for real-time prediction on anything, e.g. live sport

\section{General Purpose of The System}
first applicatoin - live sports
low manageable latencies important

\subsection{Sample Use Case}
A good example e.g. can be predicting the outcomes of certain drives in the football match as they happen. A sample case: Manchester United - Liverpool live game, live data coming from UK servers, introduced into the system through a UK node, most users (who also generate data that needs to be distributed) in China (10k), India(10k), Australia(3k). Where should websocket servers which distribute messages spun up for minimal latencies for all clients? Is it faster to spin up db replicas on that nodes too?

\subsection{Users of the System}
in stadium
on couch
dekstop
tablet
mobile (The bandwidth of a wireless network is more physically constrained in size and scope than a wired network. With less overall bandwidth to work with, the number and size of connections that are using that bandwidth must be managed more carefully because they have a greater impact on performance. This is why latency is more of an issue or wireless networks than on wired ones.
http://developer.att.com/application-resource-optimizer/docs/best-practices/multiple-simultaneous-tcp-connections
)
Users of the platform are mostly on mobile networks (that often drop), so reconnecting them quick to the right (providing lowest latencies) instance is important.
lumpy demand - it comes in 2-hour-long spikes and then can go quiet for days

\section{Basic Architecture}

Diagram

\section{Technology Stack} \label{Technology Stack}
Python + redis + mysql + redis (cluster) all versions
cloud stack described here: \ref{Cloud architecture setup}

\subsection{Programming languages}

python

\subsection{Database Technologies}

\subsubsection{Mysql Cluster}
write scalable, important for us, multi master, automatic / manual sharding
distributed, shared-nothing data that provides very high, scalable performance for applications that largely use primary key access and have high concurrency, but it incurs a cost in network access when accessing data between a MySQL Server and tables distributed across Data Nodes
In terms of CAP, MySQL Cluster will sacrifice availability to maintain consistency in the event of an unsurvivable network partition.
at least 3 machines (otherwise split brain problem)
data nodes (ndbd) - need memory, single-threaded
sql nodes (mysqld) - need cpu, multithreaded
Important:
Will MySQL Cluster “out of the box” run an application faster than my existing MySQL database?
The reality is probably not without some optimizations to the application and the database. However, by following the guidelines and best practices in the Guide to Optimizing Performance of the MySQL Cluster Database1 , significant performance gains can be realized, especially as you start to scale write operations while maintaining very high levels of availability and low latency.
[http://openquery.com.au/files/mysql-cluster-intro-use.pdf]


\section{System Scaleup Delay}
Delays: up to 2 minutes to get metrics, up to ????? minutes for instance to be accessible to clients + ???? for DNS changes to propagate (TTL set to 60 seconds but TODO VERITYF)
Unfortunately there are a large number of (misbehaving) DNS servers out there that don’t properly obey TTLs on records and will still serve up stale records for an indefinite amount of time. [http://engineering.chartbeat.com/2014/01/02/part-1-lessons-learned-tuning-tcp-and-nginx-in-ec2/]

\section{Unique Aspects of The System}
critical data flows over websockets
Users receiving data shared locally should receive it at the same time as data shared globally (with as low a latency as possible). This creates a need for globally consistent and manageable latency between end user and the system.
Connected users and sources of data are geographically changing.
Users geographical center of mass is changing for each peak of demand.
Extremely lumpy demand (peaks lasting around 2 hours). This creates a need for being able to quickly scale up and scale down.
New data is generated every few seconds by the users so caching the content and distributing geographically is difficult.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiment Outline} \label{Experiment Outline}

\begin{enumerate}
  \item Baseline architecture on a local network.
  \item Baseline architecture deployed in the cloud.
  \item Improved architecture in the cloud.
\end{enumerate}

All of them measured with the same set of metrics with a prepared framework for gathering them.

Details below.

\section{Goal of the experiment}

The goal of the project is to design a scalability framework for a real-time persistent websocket distributed application (the system). The core researched topic will be whether a systems awareness of clients geographical distribution can improve the system performance according to selected metrics, in comparison with traditional approaches.

The goal of the project is to see if the proposed architecture decomposition can perform better (quantitatively, according to the selected metric model) than the baseline architecture in serving geographically dispersed clients. Approaches used to scale simple http applications cannot always be translated to websocket applications since the communication protocol differs. Websockets put a different kind of strain on the server machines since these need to keep the connection opened on a port for a prolonged period of time rather than simply open, server and close (as is the case with http). Along the way, an answer needs to be found what level of decoupling provides best performance on each layer of a stateless persistent system. One of the properties of a system of that kind is that key value stores come under heavy load since this is where the state resides. A good solution for distributing (sharding / replicating) these also needs to be found. Same goes for persistent storage.
TODO: REFERENCE CURRENT APPROACHES HERE.

\section{Load Testing Framework}\label{Load Testing Framework}
all the analyzed tools, list also in lab notes from may 11 [FOR EACH WHY WASN'T SATISFACTORY]
jmeter, thor (low extensibility which we needed), autobahn (max available connections but no further control),
gatling-websocket, http://www.opensourcetesting.org/performance.php
tsung - looked promising, highly scalable erlang, but hard to understand what was going on, community was not active enough, wsbench

\section{Kernel tuning}\label{Kernel tuning}

\subsection{Measurements}
etsy, logster, graphite etc.

\subsection{Linux TCP stack}


http://serverfault.com/questions/48717/practical-maximum-open-file-descriptors-ulimit-n-for-a-high-volume-system
the number of client connections that a server can support has nothing to do with ports in this scenario, since the server is [typically] only listening for WS/WSS connections on one single port. I think what the other commenters meant to refer to were file descriptors. You can set the maximum number of file descriptors quite high, but then you have to watch out for socket buffer sizes adding up for each open TCP/IP socket.  MAKE SURE THIS IS CONSISTENT THROUGHOUT THE PAPER.
If the file descriptors are tcp sockets, etc, then you risk using up a large amount of memory for the socket buffers and other kernel objects; this memory is not going to be swappable.

[http://stackoverflow.com/questions/4852702/do-html-websockets-maintain-an-open-connection-for-each-client-does-this-scale/25340220\#25340220]
Each TCP connection in itself consumes very little in terms server resources. Often setting up the connection can be expensive but maintaining an idle connection it is almost free. The first limitation that is usually encountered is the maximum number of file descriptors (sockets consume file descriptors) that can be open simultaneously. This often defaults to 1024 but can easily be configured higher.

all lab notes from 08.05
all websocket updates
http://serverfault.com/questions/48717/practical-maximum-open-file-descriptors-ulimit-n-for-a-high-volume-system
Also, and very important, you may need to check if your application has a memory/file descriptor leak. Use lsof to see all it has open to see if they are valid or not. Don't try to change your system to work around applications bugs.

Although the causes for such symptoms can vary, there's one scenario that can cause a complete lock of systems handling a very large number of web requests per second without any hint of what's going on: TCP/IP port exhaustion. [https://www.outsystems.com/forums/discussion/6956/how-to-tune-the-tcp-ip-stack-for-high-volume-of-web-requests/]

describe close\_wait, time\_wait etc

only 65k per (tcp 16 bit) ip address, one can add virtual ips to machines and up the number this way
(recent lab notes)

increasing the number of clients
The solution is more quadruplets5. This can be done in several ways (in the order of difficulty to setup):
use more client ports by setting net.ipv4.ip\_local\_port\_range to a wider range,
use more server ports by asking the web server to listen to several additional ports (81, 82, 83, …),
use more client IP by configuring additional IP on the load balancer and use them in a round-robin fashion,
use more server IP by configuring additional IP on the web server6.

But there are ways to tune the TCP/IP stack to reduce the impact of this problem, allowing the system to take advantage of all resources at its disposal. Basically, we can tune several TCP/IP stack parameters, but in this context, there are 2 that really make the different:
The time that the ports is in "waiting" status since it was released from the application, and it's in fact released by the system for reuse. It's called the TIME WAIT
The maximum number of ephemeral ports that the system can use, from the total pool of 65535 available ports. Let's just call it RANGE EPHEMERAL PORTS
Reduce the TIME\_WAIT by setting the tcp\_fin\_timeout kernel value on /proc/sys/net/ipv4/tcp\_fin\_timeout, using the command echo 30 > /proc/sys/net/ipv4/tcp\_fin\_timeout to set it to 30 seconds.

Increase the range of ephemeral ports by setting ip\_local\_port\_range kernel value on /proc/sys/net/ipv4/ip\_local\_port\_range, using the command echo "32768 65535" > /proc/sys/net/ipv4/ip\_local\_port\_range, this will set the port range from 32768 to 65535.

A common misunderstanding is that a server cannot accept more than 65,536 (216) TCP sockets because TCP ports are 16-bit integer numbers.
First, the number of ports is limited to 65,536, but this limitation applies only to a single IP address. Supposing that we are limited by the number of ports to have more than 65,536 clients, then adding more IP addresses to the server machine (either by adding new network cards, or simply by using IP aliasing for the existing network card) would solve the problem (even if, for opening 12 million client would need 184 network cards or IP aliases on the server machine).
In fact, the misunderstanding comes from the fact that the server does not use its listening IP address and a different ephemeral port for each new socket to distinguish among the sockets, but it uses the same listening IP address and the same listening port for all sockets and it distinguishes among sockets by using the IP address and the ephemeral port of each client. Therefore, MigratoryData Server uses a single port to accept any number of clients and optionally it uses another few ports for JMX monitoring, HTTP monitoring, etc
https://mrotaru.wordpress.com/category/websockets/

max descriptors per process:
Because one cannot increase the maximum number of socket descriptors per process to a value larger than the current kernel maximum (fs.nr\_open) and because the kernel maximum defaults to 1048576 (10242), prior to running the ulimit command, we increased the kernel maximum accordingly as follows:
echo 20000500 > /proc/sys/fs/nr\_open
CHECK LAB NOTES RECENT TESTS, A LOT
https://mrotaru.wordpress.com/category/websockets/

The kernel value parameters aren't saved with these commands, and are reset to the default values on system reboot, thus make sure to place the commands on a system startup script such as /etc/rc.local.
https://www.outsystems.com/forums/discussion/6956/how-to-tune-the-tcp-ip-stack-for-high-volume-of-web-requests/

look for \_what can limit concurrent sockets?\_ in lab notes

“The number of connections wstest can open on a server is limited by the number of ephemeral ports on the machine on the outgoing interface / IP. Something like 64k at most. If you need to test the server with more connections, currently you will need to run multiple instances of wstest (on different machines).” from: http://autobahn.ws/testsuite/usage.html\#mode-massconnect

Understanding tcp sockets:
sockets are left in TIME\_WAIT on the server when the client suite is killed which makes sense
when client exits gracefully no sockets left in TIME\_WAIT / CLOSE\_WAIT by the server
on\_close() from tornado only kicks in on client close
.onclose in js only kicks in when server initiates through self.close()
when server terminates through self.close() sockets ends in TIME\_WAIT for a minute (also makes sense according to docs)
describe how you can influence that (reusing web sockets + delay time )

ulimit, nofile, fs.max\-file, net.ipv4.ip\_local\_port\_range = 32768 65535  to /etc/sysctl.conf for ip ranges on the clients

\section{Baseline architecture on a local network}
Architecture diagrams.
Technical details - local server capabilities.
Load testing.
LOCAL NETWORK DESCRIPTION

\section{Cloud architecture setup} \label{Cloud architecture setup}

selected aws

multiple availability zones very important in a shared cloud environment - often because another customer is getting hit by a DDoS.\cite{GroBuy}

In Amazon’s environment, Grozev and Buyya suggest using Elasticache service \cite{GroBuy}. We cannot use this, also described in \ref{Cloud architecture setup}.

\subsection{Route53}
  dns mapped to inscance ips / load balancer dns names
  lbr, geo, weighed rr
  their internal heuristics take network conditions from past weeks [VERIFY], rather amibiguous
  For each end user’s location, Route 53 will return the most specific Geo DNS record that includes that location. In other words, for a given end user’s location, Route 53 will first return a state record; if no state record is found, Route 53 will return a country record; if no country record is found, Route 53 will return a continent record; and finally, if no continent record is found, Route 53 will return the global record.

  health checks - can be used custom-defined, but for us we can use load balancing health checks (will only route if healthy incstances behind it) [http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html]

  http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Tutorials.html
\subsection{Autoscaling}
  instances behind each groups. automatically scalable, connected to external redis and mysql instances.
  only within one region, multiple availability zones
  http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/GettingStartedTutorial.html
  http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/how-as-works.html\#arch-AutoScalingMultiAZ
  Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group. Auto Scaling does this by attempting to launch new instances in the Availability Zone with the fewest instances.
  alarm - object that watches over a single metric (e.g. avg cpu use of ec2 instances in auto scaling group over specified time period) (OK, ALARM, INSSUFICIENT\_DATA), can trigger scaling up / down policy on an autoscaling group
  policy variants (change ExactCapacity, ChangeInCapacity, PercentChangeInCapacity)
  recommendation one policy for scaling out, another policy for scaling in
  http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/as-scale-based-on-demand.html
  http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/AlarmThatSendsEmail.html
  http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/US\_AlarmAtThresholdEC2.html
  \subsubsection{Alarms}
  avg / min / max / sum / sample count (for whole group)
  of cpu / disk io / network io
  is >= / > / < / <= than x %
  for at least X consecutive periods of 5 min / 15 min / 1 hour / 6 hours
  \subsubsection{Scaling Policies}
  execute policy when alarm
  add / remove / set to   X   instances / % of group
  and then wait Y (cooldown period)

  \subsubsection{Launch configurations}

\section{Load balancing}
  elb uses dns name to server a pool of lb instances in the backend (you might exceed one instance connection limit), that's why they are not provided a static ip
  routing: request count-based for http(s), for others (tcp which is a workaround for websockets) they use tcp [http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/how-elb-works.html\#request-routing]
  The client uses DNS round robin to determine which IP address to use to send the request to the load balancer. No control over routing without load balancing groups.
  For example, if you have ten instances in Availability Zone us-west-2a and two instances in us-west-2b, the traffic is equally distributed between the two Availability Zones. As a result, the two instances in us-west-2b serve the same amount of traffic as the ten instances in us-west-2a. Instead, you should distribute your instances so that you have six instances in each Availability Zone.
  To distribute traffic evenly across all back-end instances, regardless of the Availability Zone, enable cross-zone load balancing on your load balancer. However, we still recommend that you maintain approximately equivalent numbers of instances in each Availability Zone for higher fault tolerance.
  [http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/how-elb-works.html\#request-routing]
  only ~64k connections possible per ELB instance TODO: VERIFY THAT
  stickiness only works for HTTP/HTTPS protocols
  Once you have a testing tool in place, you will need to define the growth in the load. We recommend that you increase the load at a rate of no more than 50 percent every five minutes. B.oth step patterns and linear patterns for load generation should work well with Elastic Load Balancing. If you are going to use a random load generator, then it is important that you set the ceiling for the spikes so that they do not go above the load that Elastic Load Balancing will handle until it scales (see Pre-Warming the ELB).
  [https://aws.amazon.com/articles/1636185810492479\#pre-warming]

\section{Data Layer}

Technologies we used described in \ref{Technology Stack}. We used Mysql and redis as a caching solution.

\subsection{Amazon Relational Database Service}
Does not support mysql cluster as of July 2015. Support coming but not released yet.
Offers mysql read replicas. With a single master. That obviously scales only reads
You can create a MySQL Read Replica in a different region than the source DB instance to improve your disaster recovery capabilities, scale read operations into a region closer to end users, or make it easier to migrate from a data center in one region to a data center in another region.

\subsection{Elasticache}
doesn't support crossregion (you cannot connect to an elasticache cluster from outisde of the region)

The above explain why, for a scaled out solution we had to roll with our custom setup. We were suggested using technologies like DynamoDB and Kinesis, but out of scope.

\subsection{Baseline architecture deployed in the cloud}

\subsection{Improved architecture deployed in the cloud}

\section{Experiment deliverables}\label{Experiment deliverables}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Scalability Measurements} \label{Scalability Measurements}
Here we describe the metrics we settled for.

Most of what is necessary for the purpose of this research has been covered by Pushkala Pattabiraman et al [MMPROJ3].
They realized that cloud computing and its measurement provides a new set of challenges when it comes to measuring performance testing, as opposed to measuring performance of  traditional software systems. They list some key points for measuring cloud applications, among them we can find: validating and ensuring the elasticity of scalability and evaluating utility service billings and pricing models. The latter is also important in my case since cost is one of the driving factors in assessing the scalability of my system. A question they raise regarding this is: How to use a transparent approach to monitor and evaluate the correctness of the utility bill based on a posted price model during system performance evaluation and scalability measurement?.
The authors divide the performance indicators into three groups:
computing resource (CPU, disk, memory, networks) - they can be helpful in establishing baseline architecture in my case,
workload indicators (connected users, throughput and latency),
performance indicators - processing speed, system reliability and scalability based on the given QoS standards.
For each they propose formal models with pluggable values and graphic representations (BELOW).
On top of that, the research contains a case study performed in the Amazon EC2 environment [MMPROJ3].
Cloud limitations need to be taken into account. One needs to be aware of hidden costs (e.g. autoscaling service is free on EC2, but it requires cloudwatch, which is not). The authors also advise to pay attention to inconsistencies in performance and scalability data [MMPROJ3].

many others:
There is much more work related to the general scalability of distributed systems. Srinivas and Janakiram in their work [MMPROJ5] mention a metric evaluating scalability as a product of throughput and response time (or any value function) divided by the cost factor. They propose another model considering scalability as a function of synchronization, consistency, availability, workload and faultload. It aims on identifying bottlenecks and hence improving the scalability. The authors also emphasize the fact of interconnectedness of synchronization, consistency and availability.
Jogalekar and Woodside [6] propose a strategy-based scalability metric based on cost effectiveness (a function of system's throughput and it’s quality of service). It separates evaluation of throughput or quantity of work from QoS (which, according to the authors, can be any suitable expression).[MMPROJ6]

PASA[MMPROJ17]

\section{Selected metrics}
latency (messaging + handhsaking) (our suite)
sustainable concurrent websocket connectoins (our suite)
infrastructure cost (per unit of time, supporting the same number of users) (manual calculation)
dropped connections (our suite)
cpu usage (cloudwatch)
memory usage (our custom server metrics, pidstat)
network in network out (cloudwatch)
iops reads + writes (bots number and byte values, cloudwatch)

to calculate manually:
percentage of available ports used?
how long to run on a x euros


availability (SLAs???????)
architecture reaction speed to changes in demand (scaling up and scaling down) (????)

CRAM METRICS
TODO: review

\subsection{EC2 Available metrics}
collectible every minute (in detailed mode), by default every 5 minute, paid additionaly - delay up to 2 minutes
the current EC2 cloud technology does not provide any CloudWatch API to monitor the allocation and utilization of memory for EC2 instances (we do it on our own)
limits exist (10 metric, 10 alarms, 1,000,000 million requests, 1000 SNS email notifications per month for free
no limits on custom metrics
up to 5gb of incoming data logs for free
up to 5gb of data archiving for free) but we don't expect to hit them.
small delay (up to 2 minutes, that's what we experienced - delays in autoscaling)

alarms can be configured based on this -  http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch.html
data not aggregated across regions [lab2]

\subsubsection{Instance Specific}
[Lab2 - http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/cloudwatch\_concepts.html]
custom possible
available statistics for each: min, max, sum, avg, sampleCount (count of data points but research that)
cpu utilization
network in / out (in bytes)
disk read / disk write (number of ops + bytes)
status checks

\subsubsection{Cloud (group) Specific}
Collected from Elastic Load Balancer for all instances connected.

\subsubsection{Custom Metric Collection}
pidstat
latencies by our custom client suite
for network we looked ad ntop, nethogs, nload, vnstat, wireshark but amazon provides this data.

\section{Selected models}\label{Selected models}
[MMPROJ3] With CRUMs, SPMs etc.
four different types of needs:
resource utilization and allocation measurement
performance measurement under allocated computing resources
scalability measurement in different cloud infrastructures
cost driven evaluation based on a pre-defined price model

additional values we want to capture to proposed in the model:
cost
time to scale up / down
What we will use to compare architectures:
SCM (allocated resources) or SEC (used)
(1/cost) as an additional metric to SEC? or amount of s you can run on a 100bucks

Results of all setups get plugged into SCM / SEC and then ESS.

\section{Baseline, Local network}
Lab notes from 11 of May go here:

Basic metrics
\begin{itemize}
  \item cpu, memory, disk usage (pidstat / CloudWatch)
  \item network i/o (wireshark, list others analyzed)
  \item latency, throughput, concurrent connections, messages dropped?
\end{itemize}

\subsection{Load Testing}
http://stackoverflow.com/questions/3871886/ssl-and-load-balancing

elb uses dns name to server a pool of lb instances in the backend (you might exceed one instance connection limit)
in order for the full range of ELB machine IP addresses to be utilized, “make sure each [client] refreshes their DNS resolution results every few minutes.”
One test client equals one public IP address. ELB machines seem to route all traffic from a single IP address to the same back-end instance, so if you run more than one test client process behind a single public IP address, ELB regards these as a single client.
Use 12 test clients for every availability zone you have enabled on the ELB. These test clients do not need to be in different availability zones – they do not even need to be in EC2 (although it is quite attractive to use EC2 instances for test clients). If you have configured your ELB to balance among two availability zones then you should use 24 test clients.
Each test client should gradually ramp up its load over the course of a few hours. Each client can begin at (for example) one connection per second, and increase its rate of connections-per-second every X minutes until the load reaches your desired target after a few hours. - WE CANNOT DO THAT

Tools described here \cite{Load Testing Framework}

\subsubsection{Users Distribution}
The authors suggest choosing randomly when generating load as to which operation to perform, on what data size etc. They suggest using different random distributions: uniform, zipfian, latest, multinomial\cite{Ycsb}. (as mentioned in bemncharking 2.2 section)

Another distribution is suggested by Grozev and Buyya \cite{GroBuy} - Poisson distribution with a constant mean.

\section{Baseline, Deployed in the cloud}

\section{Improved, Deployed in the cloud}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiment results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}

\section{Statistical Significance}
http://technology.stitchfix.com/blog/2015/05/26/significant-sample/

\section{Threats to validity}
ec2 whacky, control over hardware released to amazon (can be dedicated machines)
shared environment, someone else might get ddosed or sth - you have no control over that
network is inherently non-deterministic, load tests needed to actually test availability

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Further work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BELOW WE HAVE ORIGINAL TEMPLATE STUFF THAT NEEDS TO BE REMOVED
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Everything below that needs to be removed (except for bib), Front Matter}

The first thing is to connect the class by saying:

\begin{snippet}
\begin{verbatim}
\documentclass{uvamscse}
\end{verbatim}
\end{snippet}

\section{Title}

Specify the title of the thesis with \cmd{title} and \cmd{subtitle} commands:

\begin{snippet}
\begin{verbatim}
\title{MetaThesis}
\subtitle{A Thesis Template Leading by Example}
\end{verbatim}
\end{snippet}

Any thesis can survive without a \cmd{subtitle}, but the \cmd{title} is mandatory.

\section{Author}

Introduce yourself with \cmd{author} and \cmd{authemail}:

\begin{snippet}
\begin{verbatim}
\author{Vadim Zaytsev}
\authemail{vadim@grammarware.net}
\end{verbatim}
\end{snippet}

Again, \cmd{authemail} is not mandatory. If you need anything fancier, just put it inside \cmd{author}.

\begin{snippet}
\begin{verbatim}
\author{Vadim Zaytsev\footnote{Yes, that one.}}
\end{verbatim}
\end{snippet}

The footnote would be printed on the bottom of the title page, and will be
referred to by a symbol, not by a number as any footnotes within the main
document body.

\section{Date}

By default, the date inserted in your PDF is the day of the build, e.g., ``March 25, 2014''. If you want it to be formatted differently or be more vague or outright fake, use \cmd{date}:

\begin{snippet}
\begin{verbatim}
\date{Spring 2014}
\end{verbatim}
\end{snippet}

The argument is just a string, the format is unrestricted:

\begin{snippet}
\begin{verbatim}
\date{Tomorrow. Honestly.}
\end{verbatim}
\end{snippet}

\section{Host}

If your hosting organisation is not the UvA, specify it with \cmd{host}. The
logo on the bottom of the title page will still be the UvA one, because this
is the organisation guaranteeing your degree.

\begin{snippet}
\begin{verbatim}
\host{Grammarware, Inc., \url{http://grammarware.github.io}}
\end{verbatim}
\end{snippet}

NB: footnotes will not work, unless you know how to \cmd{protect} them.

\section{Cover picture}

If the first page of your thesis looks too blunt, add a picture to it:

\begin{snippet}
\begin{verbatim}
\coverpic{figures/terminal.png}
\end{verbatim}
\end{snippet}

You can even specify the picture's width as an optional argument:

\begin{snippet}
\begin{verbatim}
\coverpic[100pt]{figures/terminal.png}
\end{verbatim}
\end{snippet}

How these three options look, you can see from \autoref{fig:titles}.

\begin{figure}[t]
  \fbox{\includegraphics[width=.25\textwidth]{figures/title1.pdf}}
  \hfill
  \fbox{\includegraphics[width=.25\textwidth]{figures/title2.pdf}}
  \hfill
  \fbox{\includegraphics[width=.25\textwidth]{figures/title3.pdf}}
  \caption{A hypothetical thesis title page without a cover picture (on the left), with an overly large one (in the centre) and with a tiny pic (on the right).}
  \label{fig:titles}
\end{figure}

\section{Abstract}

A thesis is fine without an abstract, if you do not feel like writing it and
your supervisor does not feel like enforcing it. If you do want an abstract,
make it with the \cmd{abstract} command:

\begin{snippet}
\begin{verbatim}
\abstract{This is not a thesis.}
\end{verbatim}
\end{snippet}

The abstract is just like any other section of your thesis, so you can use any
\LaTeX\ tricks there. If you think that the name ``abstract'' is too abstract
for your abstract, you can still use \cmd{abstract} without being too
abstract:

\begin{snippet}
\begin{verbatim}
\abstract[Confession]{I am a cenosillicaphobiac.}
\end{verbatim}
\end{snippet}

Kent Beck~\cite{JohnsonBBCGW93} proposes to have four sentences in a good abstract:

\begin{enumerate}
  \item The first states the problem.
  \item The second states why the problem is a problem.
  \item The third is the startling sentence.
  \item The fourth states the implication of the startling sentence.
\end{enumerate}

In practice, each of these ``sentences'' can be longer than an actual
sentence, but it is in general a good rule of thumb to condense the summary of
your thesis into these four tiny messages. Do not write too much, make it
tweetable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Core Chapters}

The structure of your thesis is up to you and your supervisor. Whatever you
do, do not consider the guidelines below as dogmas.

\section{Classic structure}

\begin{description}
  \item[Problem statement and motivation.]
  You describe in detail what problem the research is addressing, and what is
the motivation to address this problem. There is a concise and objective
statement of the research questions, hypotheses and goals. It is made clear
why these questions and goals are important and relevant to the world outside
the university (assuming it exists). You can already split the main research
question into subquestions in this chapter. This section also describes an
analysis of the problem: where does it occur and how, how often, and what are
the consequences? An important part is also to scope the research: what
aspects are included and what aspects are deliberately left out, and why?
  \item[Research method.]
  Here you describe the methods used to answer the research questions. A good
structure of this section often follows the subquestions by providing a method
for each. The research method needs a thorough motivation grounded in theory
in order to be acceptable. As a part of the method, you can introduce a number
of hypotheses --- these will be tested by the research, using the methods
described here. An important part of this section is validation. How will you
evaluate and validate the outcomes of the research?
  \item[Background and context.]
  This chapter contains all the information needed to put the thesis into
context. It is common to use a revised version of your literature survey for
this purpose. It is important to explicitly refer from your text to sources
you have used, they will be listed in your bibliography. For example, you can
write ``A small number of programming languages account for most language
use~\cite{MeyerovichR2013}'', where the following entry would be included in
your bibliography:
\begin{quote}
\cite{MeyerovichR2013} Leo A. Meyerovich and Ariel S. Rabkin. Empirical Analysis of Programming Language Adoption. In \emph{Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages and Applications}, OOPSLA, pages 1--18. ACM, 2013. \doi{10.1145/2509136.2509515}.
\end{quote}
Have a look at \autoref{sec:biblio} to learn more about citation.
  \item[Research.]
  This chapter reports on the execution of the research method as described in
an earlier chapter. If the research has been divided into phases, they are
introduced, reported on and concluded individually. If needed, this chapter
could be split up to balance out the sizes of all chapters.
  \item[Results.]
  This chapter presents and clarifies the results obtained during the
  research. The focus should be on the factual results, not the interpretation
  or discussion. Tables and graphics should be used to increase the clarity of
  the results where applicable.
  \item[Analysis and conclusions.]
  This chapter contains the analysis and interpretation of the results. The
  research questions are answered as best as possible with the results that
  were obtained. The analysis also discussed parts of the questions that were
  left unanswered.

  An important topic is the validity of the results. What methods of
  validation were used? Could the results be generalised to other cases? What
  threats to validity can be identified? There is room here to discuss the
  results of related scientific literature here as well. How do the results
  obtained here relate to other work, and what consequences are there? Did
  your approach work better or worse? Did you learn anything new compared to
  the already existing body of knowledge? Finally, what could you say in
  hindsight on the research approach by followed? What could have done better?
  What lessons have been learned? What could other researchers use from your
  experience? A separate section should be devoted to ``future work'', i.e.,
  possible extension points of your work that you have identified. Even other
  researchers should be able to use those as a starting point.
\end{description}

\section{Reporting on replications}

Here are the guidelines to report on replicated studies~\cite{Carver10}:

\begin{description}
  \item[Information about the original study]~\\
    \begin{description}
    \item[Research question(s)] that were the basis for the design
    \item[Participants,] their number and any other relevant characteristics
    \item[Design] as a graphical or textual description of the experimental design
    \item[Artefacts,] the description of them and/or links to the artefacts used
    \item[Context variables] as any important details that affected the design of the study or interpretation of the
results
    \item[Summary of the results] in a brief overview of the major findings
    \end{description}
  %
  \item[Information about the replication]~\\
    \begin{description}
    \item[Motivation for conducting the replication] as a
description of why the replication was conducted:
to validate the results, to broaden the results by
changing the participant pool or the artifacts.
    \item[Level of interaction with original experimenters.]
The level of interaction between the original experimenters and the
replicators should be reported. This interaction could range from none (i.e.
simply read the  paper) to them being the same people. There is quite a lot of
discussion of the level of interaction allowed for the replication to be
``successful'', but this level should be reported even without  addressing
the controversy.
    \item[Changes to the original experiment.] Any changes made to the
design, participants, artifacts, procedures, data collected and/or analysis
techniques should be  discussed along with the motivation for the change.
    \end{description}
  \item[Comparison of results to original]~\\
    \begin{description}
    \item[Consistent results,] when replication results supported
results from the original study, and
    \item[Differences in results,] when results from the replication
did not coincide with the results from the original study.
Authors should also discuss how changes made to the
experimental design (see above) may have caused
these differences.
    \end{description}
    \item[Drawing conclusions across studies]
\end{description}

NB: this section contains portions of text repeated directly from Carver~\cite{Carver10} and
only slightly massaged. Do not do this for your thesis, write your own thoughts down.

\section{\LaTeX\ details}

\subsection{Environments}

A \LaTeX\ environment is something with opening and closing tags, which look
like \cmd{begin}\{\texttt{name}\} and \cmd{end}\{\texttt{name}\}. Some useful
environments to know:

\begin{center}
\begin{tabular}{ll}
  \texttt{itemize}      & bullet lists\\
  \texttt{enumerate}    & numbered lists\\
  \texttt{description}  & definition lists\\
  \hline
  \texttt{center}       & centered line elements\\
  \texttt{flushright}   & right aligned lines\\
  \texttt{flushleft}    & left aligned lines\\
  \hline
  \texttt{tabular}      & table\\
  \texttt{longtable}    & multi-page table (needs the \texttt{longtable} package)\\
  \texttt{sideways}     & rotates some text\\
  \texttt{quote}        & block quote\\
  \texttt{verbatim}     & unformatted text\\
  \texttt{minipage}     & compound box with elements inside\\
  \texttt{boxedminipage}& compound box with elements inside and a border around it\\
  \hline
  \texttt{table}        & floating table (needs to have \texttt{tabular} nested inside)\\
  \texttt{figure}       & floating figure\\
  \texttt{sourcecode}   & floating listing\\
  \hline
  \texttt{equation}     & mathematical equation\\
  \texttt{lstlisting}   & pretty-printed syntax highligted listing\\
  \texttt{multline}     & mathematical equation spanning over multiple lines\\
  \texttt{eqnarray}     & system of mathematical equations\\
  \texttt{gather}       & bundled mathematical equations\\
  \texttt{align}        & bundled and aligned mathematical equations\\
  \texttt{array}        & matrix\\
  \texttt{CD}           & commutative diagrams\\
\end{tabular}
\end{center}

\section{Listings}

\begin{sourcecode}
\begin{lstlisting}[language=prolog]
define(Ps1,G1,G2)
 :-
    usedNs(G1,Uses),
    ps2n(Ps1,N),
    require(
      member(N,Uses),
      'Nonterminal ~q must not be fresh.',
      [N]),
    new(Ps1,N,G1,G2),
    !.
\end{lstlisting}
\caption{Code in Prolog}
\end{sourcecode}

\begin{sourcecode}
\begin{lstlisting}[language=sdf]
module Syntax

imports Numbers
imports basic/Whitespace

exports
  sorts
    Program Function Expr Ops Name Newline

  context-free syntax
    Function+                          -> Program
    Name Name+ "=" Expr Newline+       -> Function
    Expr Ops Expr                      -> Expr      {left,prefer,cons(binary)}
    Name Expr+                         -> Expr      {avoid,cons(apply)}
    "if" Expr "then" Expr "else" Expr  -> Expr      {cons(ifThenElse)}
    "(" Expr ")"                       -> Expr      {bracket}
    Name                               -> Expr      {cons(argument)}
    Int                                -> Expr      {cons(literal)}
    "-"                                -> Ops       {cons(minus)}
    "+"                                -> Ops       {cons(plus)}
    "=="                               -> Ops       {cons(equal)}
\end{lstlisting}
\caption{Code in SDF}
\end{sourcecode}

\begin{sourcecode}
\begin{lstlisting}[language=Java]
import types.*;
import org.antlr.runtime.*;

public class TestEvaluator
    public static void main(String[] args) throws Exception {

        // Parse file to program
        ANTLRFileStream input = new ANTLRFileStream(args[0]);
        FLLexer lexer = new FLLexer(input);
        CommonTokenStream tokens = new CommonTokenStream(lexer);
        FLParser parser = new FLParser(tokens);
        Program program = parser.program();

        // Parse sample expression
        input = new ANTLRFileStream(args[1]);
        lexer = new FLLexer(input);
        tokens = new CommonTokenStream(lexer);
        parser = new FLParser(tokens);
        Expr expr = parser.expr();

        // Evaluate program
        Evaluator eval = new Evaluator(program);
        int expected = Integer.parseInt(args[2]);
\end{lstlisting}
\caption{Code in Java}
\end{sourcecode}

\begin{sourcecode}
\begin{lstlisting}[style=mono,language=Python]
#!/usr/local/bin/python
# wiki: BGF
import os
import sys
import slpsns
import elementtree.ElementTree as ET

# root::nonterminal* production*
class Grammar:
  def __init__(self):
    self.roots = []
    self.prods = []
  def parse(self,fname):
    self.roots = []
    self.prods = []
    self.xml = ET.parse(fname)
    for e in self.xml.findall('root'):
      self.roots.append(e.text)
    for e in self.xml.findall(slpsns.bgf_('production')):
      prod = Production()
      prod.parse(e)
      self.prods.append(prod)
\end{lstlisting}
\caption{Code in Python}
\end{sourcecode}

\chapter{Literature}\label{sec:biblio}

\textsc{Bib}TeX\ is a JSON-like format for bibliographic entries. Encode each
source once as a \textsc{Bib}\TeX\ entry, give it a name and refer to it from
any place in your thesis. The bibliography at the end of the thesis will be
compiled automatically from those entries that are referenced at least once,
it will also be automatically sorted and fancyfied (URLs, DOIs, etc).

DOI is a digital object identifier, it is uniquely and immutably assigned to
any paper published in a well-established journal or conference proceedings
and can be used to refer to it. When used in a browser, it resolves to a
publisher's website where paper can be obtained. Including DOIs in citations
is considered good practice and lets the readers of your thesis get to the
text of the paper in one click. Books do not have DOIs, only ISBNs; some
workshop proceedings and most unofficial publications do not have DOIs. If you
want to get a DOI assigned to your work such as a piece of code, upload it to
\href{http://www.figshare.com}{FigShare}.

Keys in key-value pairs within each \textsc{Bib}\TeX\ entry are never quoted,
values usually are, but can also be included within curly brackets or left as
is, which works fine for numbers (e.g., years). If you want to preserve the
value from any adjustments (e.g., no recapitalisation in titles), use curlies
\emph{and} quotes. Separate authors and editors by ``and'', which will
automatically be mapped to commas or left as ``and''s as necessary.

\section{Books}

\cite{GruneJacobs} is just as good as the Dragon Book, but newer and has an
awesome extended bibliography available for free.

\begin{snippet}
\begin{verbatim}
@book{GruneJacobs,
  author    = "D. Grune and C. J. H. Jacobs",
  title     = "{Parsing Techniques: A Practical Guide}",
  series    = "Monographs in Computer Science",
  edition   = 2,
  publisher = "Springer",
  url       = "http://www.cs.vu.nl/~dick/PT2Ed.html",
  year      = 2008,
}
\end{verbatim}
\end{snippet}

\section{Journal papers}

Not all TOSEM papers are hard to read~\cite{GrammarwareAgenda}.

\begin{snippet}
\begin{verbatim}
@article{GrammarwareAgenda,
  author      = "Paul Klint and Ralf L{\"a}mmel and Chris Verhoef",
  title       = "{Toward an Engineering Discipline for Grammarware}",
  journal     = "ACM Transactions on Software Engineering Methodology (TOSEM)",
  volume      = 14,
  number      = 3,
  year        = 2005,
  pages       = "331--380",
}
\end{verbatim}
\end{snippet}

\section{Conference papers}

There is no limit to how many grammars can be used in one paper, but the
current record stands at 569~\cite{Micropatterns2013}.

\begin{snippet}
\begin{verbatim}
@inproceedings{Micropatterns2013,
  author = "Vadim Zaytsev",
  title = "{Micropatterns in Grammars}",
  booktitle = "{Proceedings of the Sixth International Conference on Software Language Engineering
                (SLE 2013)}",
  year = 2013,
  editor = "Martin Erwig and Richard F. Paige and Eric Van Wyk",
  volume = "8225",
  series = "LNCS",
  pages = "117--136",
  address = "Switzerland",
  month = oct,
  publisher = "Springer International Publishing",
  doi = "10.1007/978-3-319-02654-1_7",
}
\end{verbatim}
\end{snippet}

\section{Theses}

The seventh PhD student of Paul Klint was Jan Rekers~\cite{Rekers92}.

\begin{snippet}
\begin{verbatim}
@phdthesis{Rekers92,
 author   = "J. Rekers",
 title    = "{Parser Generation for Interactive Environments}",
 school   = "University of Amsterdam",
 year     = 1992,
 url      = "http://homepages.cwi.nl/~paulk/dissertations/Rekers.pdf",
}
\end{verbatim}
\end{snippet}

There is also \texttt{mastersthesis} type with exactly the same structure for
referring to Master's theses.

\section{Technical reports}

The original seminal work introducing two-level grammars was never published
in any book or conference, but there is a technical report explaining
it~\cite{Wijngaarden65}. SMC, or \emph{Stichting Matematisch Centrum}, was the
old name of CWI fifty years ago.

\begin{snippet}
\begin{verbatim}
@techreport{Wijngaarden65,
        author      = "Adriaan van Wijngaarden",
        title       = "{Orthogonal Design and Description of a Formal Language}",
        month       = oct,
        year        = 1965,
        institution = "SMC",
        type        = "{MR 76}",
        url         = "http://www.fh-jena.de/~kleine/history/languages/VanWijngaarden-MR76.pdf",
}
\end{verbatim}
\end{snippet}

\section{Wikipedia}

You do not refer to Wikipedia from academic writing, it works the other way around.

\section{Anything else}

You can refer to pretty much anything (websites, blog posts, software) through
\texttt{misc} type of entry~\cite{ANTLR}:

\begin{snippet}
\begin{verbatim}
@misc{ANTLR,
 author       = "Terence Parr",
 title        = "{ANTLR---ANother Tool for Language Recognition}",
 howpublished = "Software",
 url          = "http://antlr.org",
 year         = "2008"
}
\end{verbatim}
\end{snippet}

{%\tiny
\bibliographystyle{alphaurl}
\bibliography{thesis}
}

\end{document}
